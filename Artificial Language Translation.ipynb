{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Goal\" data-toc-modified-id=\"Goal-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Goal</a></span></li><li><span><a href=\"#Overview\" data-toc-modified-id=\"Overview-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Overview</a></span></li><li><span><a href=\"#DATA\" data-toc-modified-id=\"DATA-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>DATA</a></span><ul class=\"toc-item\"><li><span><a href=\"#Building-Word-Vectors-and-Pairs-for-SQL\" data-toc-modified-id=\"Building-Word-Vectors-and-Pairs-for-SQL-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Building Word Vectors and Pairs for SQL</a></span><ul class=\"toc-item\"><li><span><a href=\"#Webscraping\" data-toc-modified-id=\"Webscraping-3.1.1\"><span class=\"toc-item-num\">3.1.1&nbsp;&nbsp;</span>Webscraping</a></span><ul class=\"toc-item\"><li><span><a href=\"#SQLITE\" data-toc-modified-id=\"SQLITE-3.1.1.1\"><span class=\"toc-item-num\">3.1.1.1&nbsp;&nbsp;</span>SQLITE</a></span><ul class=\"toc-item\"><li><span><a href=\"#Scrapping-Text-from-the-SQLite-Documentation\" data-toc-modified-id=\"Scrapping-Text-from-the-SQLite-Documentation-3.1.1.1.1\"><span class=\"toc-item-num\">3.1.1.1.1&nbsp;&nbsp;</span>Scrapping Text from the SQLite Documentation</a></span></li><li><span><a href=\"#[LATER]-Scrapping-Text-from-the-Stackoverflow-Questions-with-SQlite\" data-toc-modified-id=\"[LATER]-Scrapping-Text-from-the-Stackoverflow-Questions-with-SQlite-3.1.1.1.2\"><span class=\"toc-item-num\">3.1.1.1.2&nbsp;&nbsp;</span>[LATER] Scrapping Text from the Stackoverflow Questions with SQlite</a></span></li><li><span><a href=\"#[LATER]-Scraping-text-from-github\" data-toc-modified-id=\"[LATER]-Scraping-text-from-github-3.1.1.1.3\"><span class=\"toc-item-num\">3.1.1.1.3&nbsp;&nbsp;</span>[LATER] Scraping text from github</a></span></li><li><span><a href=\"#[LATER]-Merging-and-saving-the-Text\" data-toc-modified-id=\"[LATER]-Merging-and-saving-the-Text-3.1.1.1.4\"><span class=\"toc-item-num\">3.1.1.1.4&nbsp;&nbsp;</span>[LATER] Merging and saving the Text</a></span></li></ul></li><li><span><a href=\"#Mongo\" data-toc-modified-id=\"Mongo-3.1.1.2\"><span class=\"toc-item-num\">3.1.1.2&nbsp;&nbsp;</span>Mongo</a></span><ul class=\"toc-item\"><li><span><a href=\"#Scrapping-Text-from-the-Documentation\" data-toc-modified-id=\"Scrapping-Text-from-the-Documentation-3.1.1.2.1\"><span class=\"toc-item-num\">3.1.1.2.1&nbsp;&nbsp;</span>Scrapping Text from the Documentation</a></span></li></ul></li></ul></li><li><span><a href=\"#Building-a-Language-Model\" data-toc-modified-id=\"Building-a-Language-Model-3.1.2\"><span class=\"toc-item-num\">3.1.2&nbsp;&nbsp;</span>Building a Language Model</a></span></li><li><span><a href=\"#Training-a-Word-Vector\" data-toc-modified-id=\"Training-a-Word-Vector-3.1.3\"><span class=\"toc-item-num\">3.1.3&nbsp;&nbsp;</span>Training a Word Vector</a></span><ul class=\"toc-item\"><li><span><a href=\"#SQLite-Word-Vector\" data-toc-modified-id=\"SQLite-Word-Vector-3.1.3.1\"><span class=\"toc-item-num\">3.1.3.1&nbsp;&nbsp;</span>SQLite Word Vector</a></span></li><li><span><a href=\"#Mongo-Word-Vector\" data-toc-modified-id=\"Mongo-Word-Vector-3.1.3.2\"><span class=\"toc-item-num\">3.1.3.2&nbsp;&nbsp;</span>Mongo Word Vector</a></span></li></ul></li><li><span><a href=\"#Building-Language-Pairs\" data-toc-modified-id=\"Building-Language-Pairs-3.1.4\"><span class=\"toc-item-num\">3.1.4&nbsp;&nbsp;</span>Building Language Pairs</a></span><ul class=\"toc-item\"><li><span><a href=\"#SQlite\" data-toc-modified-id=\"SQlite-3.1.4.1\"><span class=\"toc-item-num\">3.1.4.1&nbsp;&nbsp;</span>SQlite</a></span><ul class=\"toc-item\"><li><span><a href=\"#SQlite-Inserts\" data-toc-modified-id=\"SQlite-Inserts-3.1.4.1.1\"><span class=\"toc-item-num\">3.1.4.1.1&nbsp;&nbsp;</span>SQlite Inserts</a></span></li><li><span><a href=\"#[LATER]-SQLite-Update\" data-toc-modified-id=\"[LATER]-SQLite-Update-3.1.4.1.2\"><span class=\"toc-item-num\">3.1.4.1.2&nbsp;&nbsp;</span>[LATER] SQLite Update</a></span></li><li><span><a href=\"#[LATER]-SQLite-Delete\" data-toc-modified-id=\"[LATER]-SQLite-Delete-3.1.4.1.3\"><span class=\"toc-item-num\">3.1.4.1.3&nbsp;&nbsp;</span>[LATER] SQLite Delete</a></span></li><li><span><a href=\"#[LATER]-SQLite-Select\" data-toc-modified-id=\"[LATER]-SQLite-Select-3.1.4.1.4\"><span class=\"toc-item-num\">3.1.4.1.4&nbsp;&nbsp;</span>[LATER] SQLite Select</a></span></li></ul></li><li><span><a href=\"#Mongo\" data-toc-modified-id=\"Mongo-3.1.4.2\"><span class=\"toc-item-num\">3.1.4.2&nbsp;&nbsp;</span>Mongo</a></span><ul class=\"toc-item\"><li><span><a href=\"#Mongo-Insert\" data-toc-modified-id=\"Mongo-Insert-3.1.4.2.1\"><span class=\"toc-item-num\">3.1.4.2.1&nbsp;&nbsp;</span>Mongo Insert</a></span></li><li><span><a href=\"#[LATER]-Mongo-Update\" data-toc-modified-id=\"[LATER]-Mongo-Update-3.1.4.2.2\"><span class=\"toc-item-num\">3.1.4.2.2&nbsp;&nbsp;</span>[LATER] Mongo Update</a></span></li><li><span><a href=\"#[LATER]-Mongo-Delete\" data-toc-modified-id=\"[LATER]-Mongo-Delete-3.1.4.2.3\"><span class=\"toc-item-num\">3.1.4.2.3&nbsp;&nbsp;</span>[LATER] Mongo Delete</a></span></li><li><span><a href=\"#[LATER]-Mongo-Select\" data-toc-modified-id=\"[LATER]-Mongo-Select-3.1.4.2.4\"><span class=\"toc-item-num\">3.1.4.2.4&nbsp;&nbsp;</span>[LATER] Mongo Select</a></span></li></ul></li><li><span><a href=\"#Zipping-the-pairs\" data-toc-modified-id=\"Zipping-the-pairs-3.1.4.3\"><span class=\"toc-item-num\">3.1.4.3&nbsp;&nbsp;</span>Zipping the pairs</a></span></li></ul></li></ul></li><li><span><a href=\"#[CURRENT]-Building-a-Language-Model\" data-toc-modified-id=\"[CURRENT]-Building-a-Language-Model-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>[CURRENT] Building a Language Model</a></span></li><li><span><a href=\"#[OPEN]-Pre-Processing\" data-toc-modified-id=\"[OPEN]-Pre-Processing-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>[OPEN] Pre-Processing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Tokenize\" data-toc-modified-id=\"Tokenize-3.3.1\"><span class=\"toc-item-num\">3.3.1&nbsp;&nbsp;</span>Tokenize</a></span></li><li><span><a href=\"#[TO-DO]-Load-Word-Vectors\" data-toc-modified-id=\"[TO-DO]-Load-Word-Vectors-3.3.2\"><span class=\"toc-item-num\">3.3.2&nbsp;&nbsp;</span>[TO DO] Load Word Vectors</a></span></li><li><span><a href=\"#[TO-DO]-Model\" data-toc-modified-id=\"[TO-DO]-Model-3.3.3\"><span class=\"toc-item-num\">3.3.3&nbsp;&nbsp;</span>[TO DO] Model</a></span></li></ul></li></ul></li><li><span><a href=\"#TODO-[FAST.AI-part]-ARCHITECTURE-&amp;-LOSS-FUNCTION\" data-toc-modified-id=\"TODO-[FAST.AI-part]-ARCHITECTURE-&amp;-LOSS-FUNCTION-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>TODO [FAST.AI part] ARCHITECTURE &amp; LOSS FUNCTION</a></span><ul class=\"toc-item\"><li><span><a href=\"#Initial-Model\" data-toc-modified-id=\"Initial-Model-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Initial Model</a></span></li><li><span><a href=\"#Loss-Function:-categorical-cross-entropy\" data-toc-modified-id=\"Loss-Function:-categorical-cross-entropy-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Loss Function: categorical cross entropy</a></span></li><li><span><a href=\"#Test-Initial-Model\" data-toc-modified-id=\"Test-Initial-Model-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Test Initial Model</a></span></li><li><span><a href=\"#Bidirectional\" data-toc-modified-id=\"Bidirectional-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Bidirectional</a></span></li><li><span><a href=\"#Teacher-Forcing\" data-toc-modified-id=\"Teacher-Forcing-4.5\"><span class=\"toc-item-num\">4.5&nbsp;&nbsp;</span>Teacher Forcing</a></span></li><li><span><a href=\"#Attentional-Model\" data-toc-modified-id=\"Attentional-Model-4.6\"><span class=\"toc-item-num\">4.6&nbsp;&nbsp;</span>Attentional Model</a></span></li><li><span><a href=\"#Test-Current-Model\" data-toc-modified-id=\"Test-Current-Model-4.7\"><span class=\"toc-item-num\">4.7&nbsp;&nbsp;</span>Test Current Model</a></span></li><li><span><a href=\"#Summarised\" data-toc-modified-id=\"Summarised-4.8\"><span class=\"toc-item-num\">4.8&nbsp;&nbsp;</span>Summarised</a></span></li></ul></li><li><span><a href=\"#Final-Test\" data-toc-modified-id=\"Final-Test-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Final Test</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial LanguageTranslation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook uses common seg2seg translations build to translate natural languages into each other to translate different computer languages with the example of 2 SQL dialects into each other. \n",
    "\n",
    "This book is built upon the API and examples of fast.ai, specifically\n",
    "\n",
    "- https://github.com/fastai/fastai/blob/master/courses/dl2/translate.ipynb\n",
    "- http://course.fast.ai/lessons/lesson11.html\n",
    "\n",
    "Find a more extensive discussion of motivation & usecases here: https://github.com/Benudek/ALP/blob/master/articles/ArticialLanguageTranslation.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "For a Recurrent Neural Network to translate artificial languages into each other, we need 3 things:\n",
    "     \n",
    "    Data: \n",
    "        - pairs of 2 SQL statements, here SQL_MYSQL SQL_MONGODB\n",
    "        - Word Vectors, build from Documentation and Github\n",
    "    \n",
    "    Architecture: \n",
    "        - Recurrent Neural Network with Attention Mechanism \n",
    "        - arbirary length output where the tokens in the inut do not necessarily correspond \n",
    "        to the tokens in the output    \n",
    "        - we pass the SQL_MYSQL through a RNN. Result is one hidden state, 'backbone' / 'encoder'. \n",
    "        This is just a vector (matrix with mini batch)\n",
    "\n",
    "    Loss Function: \n",
    "        - in our case, would state that  SQL_MYSQL X should have generated SQL_MONGODB x, \n",
    "        - we will need a 'yardstick' to see how far off we were and update the weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "While Artificial Languages are less complex than natural languages, there are a number of additional considerations  \n",
    "        \n",
    "Syntactially correct output: \n",
    "    - the output must be syntactically correct formed, one way to verify this could be to send any \n",
    "    output to send to a compiler.\n",
    "    - If there is a syntactic error, the lossu function should 'punish' \n",
    "    that accordingly\n",
    "\n",
    "Handling of values & variable:\n",
    "    - variable values should be immutable! 1 is 1 'hello' is 'hello'. While natural languages know this construction, \n",
    "    in Artificial Languages it is particularly common and important.  \n",
    "    - Anything which is not a keyword of the language is either an immutable variable value or refers to a table or \n",
    "    column of the source or target language. While the former should be treated as immutable the latter should get \n",
    "    translated to the target model\n",
    "    - How a trained RNN treats non keywords, remains to be tested (TO DO)\n",
    "\n",
    "Traing Word Vectors:\n",
    "    - One approach could be to take bits of SQL and train a word vector with it. Since a SQL \n",
    "    language is a very sparse language this approach is not promising (TO DO: Test)\n",
    "    - Instead, we will train word vectors in text that embedds the desired sql statements. We will scrap \n",
    "    documentation of the 2 sql dialects and we will pick github repos with general programming languages like java or \n",
    "    python that contain these sql dialects\n",
    "    - Such word vectors will create embeddings for many more words than we will use for the pure sql translation. \n",
    "    This will eventually not matter, since the input string will only contain words of the source sql dialect and \n",
    "    hence only activate sql words of the target sql\n",
    "\n",
    "Usage of a Language Model:\n",
    "    - Alternatively to a Word Vector, we could train a language Model and use this in translations. This is planned \n",
    "    for a follow up Release\n",
    "    - Usage of a Language Model would be specifically useful for pre-training models. Since all SQL dialects are \n",
    "    known it would be easy to pre-train Models. A specific application would then have a pretrained network for the \n",
    "    syntax of the language and in the later layers add e.g. specifics of source and target model (TO ELABORATE)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Word Vectors and Pairs for SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To Train the Vector, we will get text that embeds the 2 sql dialects, i.e. Documentation of the dialects and github repositories with general programming languages using these sql dialects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Webscraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some useful links:\n",
    "\n",
    "- https://www.crummy.com/software/BeautifulSoup/bs4/doc/ from stackexchange stackoverflow, github, sql docs\n",
    "\n",
    "- https://pygithub.readthedocs.io/en/latest/examples/MainClass.html#search-repositories-by-language\n",
    "\n",
    "- https://realpython.com/python-web-scraping-practical-introduction/\n",
    "\n",
    "- https://pygithub.readthedocs.io/en/latest/examples/Repository.html#get-all-of-the-contents-of-the-repository-recursively\n",
    "\n",
    "- https://docs.scrapy.org/en/latest/intro/overview.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## http://docs.python-requests.org/en/master/\n",
    "##!pip install requests "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "##!pip install BeautifulSoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "from requests.exceptions import RequestException\n",
    "from contextlib import closing\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO: scrap this better ! with e.g. scrapy or recurvice functions going down the page tree from a starting point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SQLITE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Scrapping Text from the SQLite Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO: do for insert, update and so on also"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://www.sqlite.org/lang.html start here and go down the tree\n",
    "## \"https://www.sqlite.org/lang_select.html\"\n",
    "url_sqlitedoc = \"https://www.sqlite.org/lang_insert.html\" \n",
    "html_sqlitedoc = urlopen(url_sqlitedoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup_sqlitedoc = []\n",
    "soup_sqlitedoc.append(BeautifulSoup(html_sqlitedoc, 'lxml'))\n",
    "for link in soup_sqlitedoc[0].findAll('a', attrs={'href': re.compile(\"^http://\")}):\n",
    "    soup_sqlitedoc.append(BeautifulSoup(urlopen(link.get('href')), 'lxml'))\n",
    "##type(soup_sqlitedoc)\n",
    "##for mem in soup_sqlitedoc:\n",
    "##    print(mem.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = open(\"sqlite_all.txt\", \"w\")\n",
    "## adjust this later, merge below\n",
    "##text_file = open(\"sqliteinsert_sqllitedoc.txt\", \"w\")\n",
    "for mem in soup_sqlitedoc:\n",
    "    ##print(mem.text)\n",
    "    text_file.write(mem.text)\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### [LATER] Scrapping Text from the Stackoverflow Questions with SQlite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_stackoverflow = \"https://stackoverflow.com/questions/tagged/sqlite\"\n",
    "html_stackoverflow = urlopen(url_stackoverflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup_stackoverflow = []\n",
    "soup_stackoverflow.append(BeautifulSoup(html_stackoverflow, 'lxml'))\n",
    "for link in soup_stackoverflow[0].findAll('link', attrs={'href': re.compile(\"^http://\")}):\n",
    "    soup_stackoverflow.append(BeautifulSoup(urlopen(link.get('href')), 'lxml'))\n",
    "##type(soup_sqlitedoc)\n",
    "##for mem in soup_stackoverflow:\n",
    "##    print(mem.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = open(\"sqliteselect_stackoverflow.txt\", \"w\")\n",
    "for mem in soup_stackoverflow:\n",
    "    text_file.write(mem.text)\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### [LATER] Scraping text from github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##!pip install PyGithub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from github import Github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('credentials.json') as f:\n",
    "    data = json.load(f)\n",
    "    username = data['username']\n",
    "    password = data['password']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Github(username, password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##repositories = g.search_repositories(query='language:python')\n",
    "##for repo in repositories:\n",
    "##    print(repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "textgithub_sqlitedoc = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "GithubException",
     "evalue": "403 {'message': 'This API returns blobs up to 1 MB in size. The requested blob is too large to fetch via the API, but you can use the Git Data API to request blobs up to 100 MB in size.', 'errors': [{'resource': 'Blob', 'field': 'data', 'code': 'too_large'}], 'documentation_url': 'https://developer.github.com/v3/repos/contents/#get-contents'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mGithubException\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-70d65ecbad4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_content\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"file\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;31m##if(file_content.type != \"NoneType\" and file_content.type != \"dir\"):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0mfile_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase64\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb64decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_content\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m                 \u001b[0;31m##file_data_string = file_data.decode(\"utf-8\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0;31m##if(\"Select\" in file_data_string): print(file_data_string)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/github/ContentFile.py\u001b[0m in \u001b[0;36mcontent\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \"\"\"\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_completeIfNotSet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_content\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/github/GithubObject.py\u001b[0m in \u001b[0;36m_completeIfNotSet\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_completeIfNotSet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotSet\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_completeIfNeeded\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_completeIfNeeded\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/github/GithubObject.py\u001b[0m in \u001b[0;36m_completeIfNeeded\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_completeIfNeeded\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__completed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/github/GithubObject.py\u001b[0m in \u001b[0;36m__complete\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    270\u001b[0m         headers, data = self._requester.requestJsonAndCheck(\n\u001b[1;32m    271\u001b[0m             \u001b[0;34m\"GET\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_url\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m         )\n\u001b[1;32m    274\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storeAndUseAttributes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/github/Requester.py\u001b[0m in \u001b[0;36mrequestJsonAndCheck\u001b[0;34m(self, verb, url, parameters, headers, input)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrequestJsonAndCheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequestJson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__customConnection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrequestMultipartAndCheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/github/Requester.py\u001b[0m in \u001b[0;36m__check\u001b[0;34m(self, status, responseHeaders, output)\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__structuredFromJson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m400\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__createException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponseHeaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponseHeaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mGithubException\u001b[0m: 403 {'message': 'This API returns blobs up to 1 MB in size. The requested blob is too large to fetch via the API, but you can use the Git Data API to request blobs up to 100 MB in size.', 'errors': [{'resource': 'Blob', 'field': 'data', 'code': 'too_large'}], 'documentation_url': 'https://developer.github.com/v3/repos/contents/#get-contents'}"
     ]
    }
   ],
   "source": [
    "from github import Github\n",
    "import getpass\n",
    "import base64\n",
    "## repo = g.get_repo(\"PyGithub/PyGithub\")\n",
    "##repo.get_topics()\n",
    "repositories = g.search_repositories(query='topic:sqlite')\n",
    "##repositories. (g.search_repositories(query='label:sqlite'))\n",
    "decoder = json.JSONDecoder()\n",
    "for repo in repositories:\n",
    "    ##print(repo)\n",
    "    contents = repo.get_contents(\"\")\n",
    "    while len(contents) > 1:\n",
    "        file_content = contents.pop(0)\n",
    "        if file_content.type == \"dir\":\n",
    "            contents.extend(repo.get_contents(file_content.path))\n",
    "        ## most files never contain sqlite\n",
    "        if (file_content.name.endswith(\"css\") or file_content.name.endswith(\"png\") or\n",
    "             file_content.name.endswith(\"yml\") or file_content.name.endswith(\"json\")) : continue\n",
    "        else:\n",
    "            ## only if we have those key words contained do we care\n",
    "            ##if (file_content.raw_data.values contains('SELECT')):  \n",
    "            ##    textgithub_sqlitedoc = textgithub_sqlitedoc + file_content.raw_data\n",
    "            ##print(file_content)\n",
    "            ##print(file_content.type)\n",
    "            if(file_content.type == \"file\"):\n",
    "            ##if(file_content.type != \"NoneType\" and file_content.type != \"dir\"):\n",
    "                file_data = base64.b64decode(file_content.content)\n",
    "                ##file_data_string = file_data.decode(\"utf-8\") \n",
    "                ##if(\"Select\" in file_data_string): print(file_data_string)\n",
    "                if(\"SELECT\" in file_content.content): print(file_data); print('#####')        \n",
    "            \n",
    "            ##file_out = open(file_content.name, \"w\")\n",
    "            ##file_out.write(file_data)\n",
    "            ##file_out.close()\n",
    "                   \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## repo = g.get_repo(\"PyGithub/PyGithub\")\n",
    "##repo.get_topics()\n",
    "##repositories = g.search_repositories(query='label:sqlite')\n",
    "##for repo in repositories:\n",
    "##    print(repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = open(\"sqliteselect_github.txt\", \"w\")\n",
    "text_file.write(textgithub_sqlitedoc)\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### [LATER] Merging and saving the Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'soup_stackoverflow' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-163-02de850c9bd3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mstopped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m## merge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoup_sqlitedoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoup_stackoverflow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mtext_sqlitedoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_sqlitedoc\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msoup_sqlitedoc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msoup_stackoverflow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mstopped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'soup_stackoverflow' is not defined"
     ]
    }
   ],
   "source": [
    "text_sqlitedoc = ''\n",
    "stopped = 0\n",
    "## merge \n",
    "for i in range(min(len(soup_sqlitedoc), len(soup_stackoverflow))):\n",
    "    text_sqlitedoc = text_sqlitedoc + soup_sqlitedoc[i].get_text() + soup_stackoverflow[i].get_text()\n",
    "    stopped = i\n",
    "## plug the rest    \n",
    "if len(soup_sqlitedoc) > len(soup_stackoverflow):\n",
    "    for i in range(len(soup_sqlitedoc)):\n",
    "        text_sqlitedoc = text_sqlitedoc + soup_sqlitedoc[stopped + i].get_text()\n",
    "if len(soup_sqlitedoc) < len(soup_stackoverflow):\n",
    "    for i in range(len(soup_sqlitedoc)):\n",
    "        text_sqlitedoc = text_sqlitedoc + soup_stackoverflow[stopped + i].get_text()  \n",
    "##TO DO: this text needs lots of cleaning and assessing, where relevant\n",
    "len(text_sqlitedoc)\n",
    "##text_sqlitedoc\n",
    "## now the github stuff\n",
    "text_sqlitedoc = text_sqlitedoc + textgithub_sqlitedoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "write() argument must be str, not _io.TextIOWrapper",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-166-594c198821e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtext_sqlitedoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sqliteselect_sqllitedoc.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtext_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sqliteselect_all.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtext_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_sqlitedoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtext_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: write() argument must be str, not _io.TextIOWrapper"
     ]
    }
   ],
   "source": [
    "##text_sqlitedoc = open(\"sqliteselect_sqllitedoc.txt\", \"w\")\n",
    "text_file = open(\"sqliteselect_all.txt\", \"w\")\n",
    "text_file.write(text_sqlitedoc)\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -ltr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##!more sqliteselect.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mongo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Scrapping Text from the Documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://www.sqlite.org/lang.html start here and go down the tree\n",
    "## \"https://www.sqlite.org/lang_select.html\"\n",
    "url_mongodoc = \"https://docs.mongodb.com/manual/crud/#create-operations\" \n",
    "html_mongodoc = urlopen(url_mongodoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "SQLite Query Language: INSERT\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Small. Fast. Reliable.Choose any three.\n",
      "\n",
      "\n",
      "\n",
      "Home\n",
      "Menu\n",
      "About\n",
      "Documentation\n",
      "Download\n",
      "License\n",
      "Support\n",
      "Purchase\n",
      "\n",
      "Search\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "About\n",
      "Documentation\n",
      "Download\n",
      "Support\n",
      "Purchase\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Search Documentation\n",
      "Search Changelog\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "function toggle_div(nm) {\n",
      "var w = document.getElementById(nm);\n",
      "if( w.style.display==\"block\" ){\n",
      "w.style.display = \"none\";\n",
      "}else{\n",
      "w.style.display = \"block\";\n",
      "}\n",
      "}\n",
      "function toggle_search() {\n",
      "var w = document.getElementById(\"searchmenu\");\n",
      "if( w.style.display==\"block\" ){\n",
      "w.style.display = \"none\";\n",
      "} else {\n",
      "w.style.display = \"block\";\n",
      "setTimeout(function(){\n",
      "document.getElementById(\"searchbox\").focus()\n",
      "}, 30);\n",
      "}\n",
      "}\n",
      "function div_off(nm){document.getElementById(nm).style.display=\"none\";}\n",
      "window.onbeforeunload = function(e){div_off(\"submenu\");}\n",
      "/* Disable the Search feature if we are not operating from CGI, since */\n",
      "/* Search is accomplished using CGI and will not work without it. */\n",
      "if( !location.origin.match || !location.origin.match(/http/) ){\n",
      "document.getElementById(\"search_menubutton\").style.display = \"none\";\n",
      "}\n",
      "/* Used by the Hide/Show button beside syntax diagrams, to toggle the */\n",
      "function hideorshow(btn,obj){\n",
      "var x = document.getElementById(obj);\n",
      "var b = document.getElementById(btn);\n",
      "if( x.style.display!='none' ){\n",
      "x.style.display = 'none';\n",
      "b.innerHTML='show';\n",
      "}else{\n",
      "x.style.display = '';\n",
      "b.innerHTML='hide';\n",
      "}\n",
      "return false;\n",
      "}\n",
      "\n",
      "SQL As Understood By SQLite[Top]INSERTinsert-stmt:\n",
      "hide\n",
      "\n",
      "\n",
      "expr:\n",
      "show\n",
      "\n",
      "\n",
      "filter:\n",
      "show\n",
      "\n",
      "\n",
      "\n",
      "literal-value:\n",
      "show\n",
      "\n",
      "\n",
      "\n",
      "raise-function:\n",
      "show\n",
      "\n",
      "\n",
      "\n",
      "type-name:\n",
      "show\n",
      "\n",
      "\n",
      "signed-number:\n",
      "show\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "window-defn:\n",
      "show\n",
      "\n",
      "\n",
      "frame-spec:\n",
      "show\n",
      "\n",
      "\n",
      "\n",
      "ordering-term:\n",
      "show\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "select-stmt:\n",
      "show\n",
      "\n",
      "\n",
      "common-table-expression:\n",
      "show\n",
      "\n",
      "\n",
      "\n",
      "compound-operator:\n",
      "show\n",
      "\n",
      "\n",
      "\n",
      "join-clause:\n",
      "show\n",
      "\n",
      "\n",
      "join-constraint:\n",
      "show\n",
      "\n",
      "\n",
      "\n",
      "join-operator:\n",
      "show\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ordering-term:\n",
      "show\n",
      "\n",
      "\n",
      "\n",
      "result-column:\n",
      "show\n",
      "\n",
      "\n",
      "\n",
      "table-or-subquery:\n",
      "show\n",
      "\n",
      "\n",
      "\n",
      "window-defn:\n",
      "show\n",
      "\n",
      "\n",
      "frame-spec:\n",
      "show\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "upsert-clause:\n",
      "show\n",
      "\n",
      "\n",
      "column-name-list:\n",
      "show\n",
      "\n",
      "\n",
      "\n",
      "indexed-column:\n",
      "show\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "with-clause:\n",
      "show\n",
      "\n",
      "\n",
      "cte-table-name:\n",
      "show\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The INSERT statement comes in three basic forms.  \n",
      "\n",
      "INSERT INTO table VALUES(...);\n",
      "The first form (with the \"VALUES\" keyword) creates one or more\n",
      "new rows in\n",
      "an existing table. If the column-name list after\n",
      "table-name is omitted then the number\n",
      "of values inserted into each row\n",
      "must be the same as the number of columns in the table. In this case\n",
      "the result of evaluating the left-most expression from each term of\n",
      "the VALUES list is inserted into the left-most column of each new row,\n",
      "and so forth for each subsequent expression. If a column-name\n",
      "list is specified, then the number of values in each term of the\n",
      "VALUE list must match the number of\n",
      "specified columns. Each of the named columns of the new row is populated\n",
      "with the results of evaluating the corresponding VALUES expression. Table\n",
      "columns that do not appear in the column list are populated with the \n",
      "default column value (specified as part of the CREATE TABLE statement), or\n",
      "with NULL if no default value is specified.\n",
      "\n",
      "INSERT INTO table SELECT ...;\n",
      "The second form of the INSERT statement contains a SELECT statement\n",
      "instead of a VALUES clause. A new entry is inserted into the table for each\n",
      "row of data returned by executing the SELECT statement. If a column-list is\n",
      "specified, the number of columns in the result of the SELECT must be the same\n",
      "as the number of items in the column-list. Otherwise, if no column-list is\n",
      "specified, the number of columns in the result of the SELECT must be the same\n",
      "as the number of columns in the table. Any SELECT statement, including\n",
      "compound SELECTs and SELECT statements with ORDER BY and/or LIMIT clauses, \n",
      "may be used in an INSERT statement of this form.\n",
      "\n",
      "To avoid a parsing ambiguity, the SELECT statement should always\n",
      "contain a WHERE clause, even if that clause is simply \"WHERE true\",\n",
      "if the upsert-clause is present.  Without the WHERE clause, the\n",
      "parser does not know if the token \"ON\" is part of a join constraint\n",
      "on the SELECT, or the beginning of the upsert-clause.\n",
      "\n",
      "INSERT INTO table DEFAULT VALUES;\n",
      "The third form of an INSERT statement is with DEFAULT VALUES.\n",
      "The INSERT ... DEFAULT VALUES statement inserts a single new row into the\n",
      "named table. Each column of the new row is populated with its \n",
      "default value, or with a NULL if no default value is specified \n",
      "as part of the column definition in the CREATE TABLE statement.\n",
      "The upsert-clause is not supported after DEFAULT VALUES.\n",
      "\n",
      "\n",
      "\n",
      "The initial \"INSERT\" keyword can be replaced by\n",
      "\"REPLACE\" or \"INSERT OR action\" to specify an alternative\n",
      "constraint conflict resolution algorithm to use during \n",
      "that one INSERT command.\n",
      "For compatibility with MySQL, the parser allows the use of the\n",
      "single keyword REPLACE as an \n",
      "alias for \"INSERT OR REPLACE\".\n",
      "\n",
      "The optional \"schema-name.\" prefix on the \n",
      "table-name\n",
      "is supported for top-level INSERT statements only.  The table name must be\n",
      "unqualified for INSERT statements that occur within CREATE TRIGGER statements.\n",
      "Similarly, the \"DEFAULT VALUES\" form of the INSERT statement is supported for\n",
      "top-level INSERT statements only and not for INSERT statements within\n",
      "triggers.\n",
      "\n",
      "The optional \"AS alias\" phrase provides an alternative\n",
      "name for the table into which content is being inserted.  The alias name\n",
      "can be used within WHERE and SET clauses of the UPSERT.  If there is no\n",
      "upsert-clause, then the alias is pointless, but also\n",
      "harmless.\n",
      "\n",
      "See the separate UPSERT documentation for the additional trailing\n",
      "syntax that can cause an INSERT to behave as an UPDATE if the INSERT would\n",
      "otherwise violate a uniqueness constraint.  The upsert clause is not\n",
      "allowed on an \"INSERT ... DEFAULT VALUES\".\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "soup_mongodoc = []\n",
    "soup_mongodoc.append(BeautifulSoup(html_mongodoc, 'lxml'))\n",
    "for link in soup_mongodoc[0].findAll('a', attrs={'href': re.compile(\"^http://\")}):\n",
    "    soup_mongodoc.append(BeautifulSoup(urlopen(link.get('href')), 'lxml'))\n",
    "##type(soup_sqlitedoc)\n",
    "print(len(soup_sqlitedoc))\n",
    "for mem in soup_sqlitedoc:\n",
    "    print(mem.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "##pd.write\n",
    "text_file = open(\"mongosql_all.txt\", \"w\")\n",
    "## adjust this later, merge below\n",
    "##text_file = open(\"sqliteinsert_sqllitedoc.txt\", \"w\")\n",
    "for mem in soup_mongodoc:\n",
    "    ##print(mem.text)\n",
    "    text_file.write(mem.text)\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('mongosql_all.csv', mode='w') as csv_file:\n",
    "    fieldnames = ['sqldialect', 'sentence']\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for mem in soup_mongodoc:\n",
    "        ##print(mem.text)\n",
    "        ##text_file.write(mem.text)\n",
    "        writer.writerow({'sqldialect': 'mongodb_sql doc', 'sentence': mem.text})\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building a Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.fast.ai/text.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "##?pd.read_clipboard\n",
    "##?pd.read_csv\n",
    "##?pd.read_excel\n",
    "##?pd.read_feather\n",
    "##?pd.read_fwf\n",
    "##?pd.read_gbq\n",
    "##?pd.read_hdf\n",
    "##?pd.read_html\n",
    "##?pd.read_json\n",
    "##?pd.read_msgpack\n",
    "##?pd.read_parquet\n",
    "##?pd.read_pickle\n",
    "##?pd.read_sas\n",
    "##?pd.read_sql\n",
    "##?pd.read_sql_query\n",
    "##?pd.read_stata\n",
    "##?pd.read_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training a Word Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pypi.org/project/word2vec/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting word2vec\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ce/51/5e2782b204015c8aef0ac830297c2f2735143ec90f592b9b3b909bb89757/word2vec-0.10.2.tar.gz (60kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 29.1MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /opt/anaconda3/lib/python3.6/site-packages (from word2vec) (1.11.0)\n",
      "Requirement already satisfied: cython in /opt/anaconda3/lib/python3.6/site-packages (from word2vec) (0.28.2)\n",
      "Requirement already satisfied: numpy>=1.9.2 in /opt/anaconda3/lib/python3.6/site-packages (from word2vec) (1.15.4)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.6/site-packages (from word2vec) (1.1.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.6/site-packages (from word2vec) (0.19.1)\n",
      "Building wheels for collected packages: word2vec\n",
      "  Running setup.py bdist_wheel for word2vec ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/jupyter/.cache/pip/wheels/ef/9f/06/aec42532c9c37e05f936d4d586b15cfdfc9f2ffb62bd7fed1c\n",
      "Successfully built word2vec\n",
      "Installing collected packages: word2vec\n",
      "Successfully installed word2vec-0.10.2\n"
     ]
    }
   ],
   "source": [
    "##!pip install cython\n",
    "##!pip install word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "import word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SQLite Word Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training using file sqlite_all.txt\n",
      "Vocab size: 36\n",
      "Words in train file: 664\n"
     ]
    }
   ],
   "source": [
    "## pick your file\n",
    "word2vec.word2vec('sqlite_all.txt', 'sqlite_all.bin', size=100, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## more sqlite_all.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "##word2vec.word2clusters('sqlite_all.txt', 'sqlite_all-clusters.txt', 100, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1052\r\n",
      "-rw-r--r-- 1 jupyter jupyter  58419 Dec  2 17:16 PairedSQLDataset.ipynb\r\n",
      "-rw-r--r-- 1 jupyter jupyter 361447 Dec  2 17:29 WordVectorsArtificial.ipynb\r\n",
      "-rw-r--r-- 1 jupyter jupyter     63 Dec  3 11:31 credentials.json\r\n",
      "-rw-r--r-- 1 jupyter jupyter 137418 Dec  3 11:35 sqliteselect.txt\r\n",
      "-rw-r--r-- 1 jupyter jupyter      0 Dec  3 11:59 sqliteselect_github.txt\r\n",
      "-rw-r--r-- 1 jupyter jupyter  39476 Dec  3 12:01 sqliteselect_stackoverflow.txt\r\n",
      "-rw-r--r-- 1 jupyter jupyter 116404 Dec  3 13:55 sqliteselect-phrases.txt\r\n",
      "-rw-r--r-- 1 jupyter jupyter   3342 Dec  3 13:58 sqliteselect-clusters.txt\r\n",
      "-rw-r--r-- 1 jupyter jupyter   8192 Dec  4 05:20 mydata.sqlite\r\n",
      "drwxr-xr-x 4 jupyter jupyter   4096 Dec  4 06:21 \u001b[0m\u001b[01;34mdata\u001b[0m/\r\n",
      "-rw-r--r-- 1 jupyter jupyter      0 Dec  4 10:00 sqliteselect_sqllitedoc.txt\r\n",
      "-rw-r--r-- 1 jupyter jupyter      0 Dec  4 10:02 sqliteselect_all.txt\r\n",
      "-rw-r--r-- 1 jupyter jupyter 179324 Dec  4 10:04 Artificial Language Translation.ipynb\r\n",
      "-rw-r--r-- 1 jupyter jupyter 151444 Dec  4 10:05 sqliteselect.bin\r\n"
     ]
    }
   ],
   "source": [
    "ls -ltr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['</s>', 'the', 'of', 'show', 'is', 'INSERT', '=', 'The', 'in', 'a', 'an', 'for', '}', 'if', 'statement',\n",
       "       'number', 'with', 'SELECT', 'not', 'be', 'as', 'table', 'new', 'each', 'columns', 'Search', 'and', 'or', 'list',\n",
       "       'into', 'row', 'must', 'column', 'that', 'statements', 'DEFAULT'], dtype='<U78')"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## import the word vec\n",
    "model_sqlite = word2vec.load('sqlite_all.bin')\n",
    "## and have a look as a numpy array\n",
    "model_sqlite.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36,)"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sqlite.vocab.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.143333,  0.158255, -0.137158, -0.117384, ..., -0.078551,  0.054569,  0.109554,  0.006934],\n",
       "       [ 0.073927, -0.179028,  0.155738,  0.003042, ..., -0.046278,  0.020136, -0.067982, -0.045481],\n",
       "       [-0.144618, -0.123692,  0.124851,  0.022317, ..., -0.051154, -0.032063, -0.067628,  0.09229 ],\n",
       "       [-0.161266,  0.094703,  0.045596,  0.096461, ..., -0.050246,  0.009316,  0.027453, -0.091457],\n",
       "       ...,\n",
       "       [-0.051065, -0.103672,  0.021186, -0.055101, ..., -0.000714,  0.0855  ,  0.182282, -0.015306],\n",
       "       [ 0.169361, -0.020955, -0.033527, -0.128954, ..., -0.043918,  0.086562,  0.005938,  0.081218],\n",
       "       [ 0.025859, -0.084878,  0.121857, -0.136162, ...,  0.040614, -0.076747, -0.155966, -0.149867],\n",
       "       [-0.05636 , -0.170235,  0.062747,  0.063618, ..., -0.034268,  0.026848, -0.064807,  0.135661]])"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sqlite.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## get vectors of individual words\n",
    "model_sqlite['INSERT'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.070444,  0.107697, -0.120542,  0.02696 ,  0.129128,  0.135426, -0.141813,  0.098574, -0.098237, -0.034946])"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sqlite['INSERT'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We can calculate the distance between two or more (all combinations) words.\n",
    "##model.distance(\"TABLE\", \"WHERE\", \"FROM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([27, 13, 23,  4,  0, 16, 34,  6, 14, 24]),\n",
       " array([0.275764, 0.210698, 0.170573, 0.165821, 0.15968 , 0.148367, 0.124971, 0.122451, 0.114953, 0.095202]))"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexes, metrics = model_sqlite.similar(\"INSERT\")\n",
    "indexes, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['or', 'if', 'each', 'is', '</s>', 'with', 'statements', '=', 'statement', 'columns'], dtype='<U78')"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## We can get the words for those indexes\n",
    "model_sqlite.vocab[indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## CLUSTERS\n",
    "##clusters_sqlite = word2vec.load_clusters('sqliteselect-clusters.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['</s>', '=', 'the', 'to', ..., 'put', 'being', 'Science', '['], dtype='<U30')"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## We can see the cluster number for individual words\n",
    "##clusters_sqlite.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['file', 'no', 'under'], dtype='<U30')"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## get all the words grouped on an specific cluster\n",
    "##clusters_sqlite.get_words_on_cluster(90).shape\n",
    "##clusters_sqlite.get_words_on_cluster(90)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mongo Word Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training using file mongosql_all.txt\n",
      "Vocab size: 65\n",
      "Words in train file: 1203\n"
     ]
    }
   ],
   "source": [
    "## pick your file\n",
    "word2vec.word2vec('mongosql_all.txt', 'mongosql_all.bin', size=100, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##word2vec.word2clusters('mongosql_all.txt', 'mongosql_all-clusters.txt', 100, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['</s>', 'a', 'MongoDB', 'to', ..., 'ChangelogCompatibility', 'Operations¶', 'provides', 'For'], dtype='<U78')"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## import the word vec\n",
    "model_mongo = word2vec.load('mongosql_all.bin')\n",
    "## and have a look as a numpy array\n",
    "model_mongo.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65,)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mongo.vocab.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.143333,  0.158255, -0.137158, -0.117384, ..., -0.078551,  0.054569,  0.109554,  0.006934],\n",
       "       [ 0.07836 , -0.170158,  0.149717,  0.014896, ..., -0.040637,  0.006677, -0.057856, -0.038477],\n",
       "       [-0.150827, -0.111103,  0.123228,  0.025531, ..., -0.045844, -0.045178, -0.057452,  0.101903],\n",
       "       [-0.161625,  0.093865,  0.048393,  0.092296, ..., -0.050627,  0.009648,  0.026504, -0.091045],\n",
       "       ...,\n",
       "       [-0.164451,  0.012601, -0.156944, -0.07509 , ..., -0.05807 ,  0.028517, -0.051903,  0.13784 ],\n",
       "       [-0.08999 , -0.158578,  0.075379,  0.056491, ...,  0.076199, -0.077039,  0.035485, -0.128359],\n",
       "       [-0.152186, -0.101329,  0.060832,  0.121144, ..., -0.157894, -0.003849,  0.00959 , -0.144871],\n",
       "       [ 0.129281,  0.020894,  0.175804,  0.025248, ...,  0.084016,  0.100523, -0.131071, -0.03258 ]])"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mongo.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get vectors of individual words\n",
    "## model_mongo['INSERT'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "## indexes, metrics = model_mongo.similar(\"INSERT\")\n",
    "## indexes, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Standalone', 'Cluster', 'an', 'Replica', '</s>', '0)', '3.2', 'for', 'of', 'Data'], dtype='<U78')"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## We can get the words for those indexes\n",
    "model_mongo.vocab[indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## CLUSTERS\n",
    "##clusters_mongodb = word2vec.load_clusters('mongodb-clusters.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We can see the cluster number for individual words\n",
    "##clusters_mongodb.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get all the words grouped on an specific cluster\n",
    "##clusters_mongodb.get_words_on_cluster(90).shape\n",
    "##clusters_mongodb.get_words_on_cluster(90)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pandas' has no attribute 'read'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-252-9e25e3c89821>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mongosql_all.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'pandas' has no attribute 'read'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building Language Pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SQlite "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##import numpy as np\n",
    "## import torch\n",
    "import pandas as pd\n",
    "from pandas import Series,DataFrame\n",
    "\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insertChar(mystring, position, chartoinsert ):\n",
    "    longi = len(mystring)\n",
    "    mystring   =  mystring[:position] + chartoinsert + mystring[position:] \n",
    "    return mystring "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## O Reilly: Python for Data Analysis, chapter 6.4. Interacting with Databases, p.189\n",
    "\n",
    "drop = \"\"\"DROP TABLE Customer;\"\"\"\n",
    "query = \"\"\"\n",
    " CREATE TABLE IF NOT EXISTS Customer (lastname VARCHAR(40), email VARCHAR(40), networth REAL);\"\"\"\n",
    "con = sqlite3.connect('mydata.sqlite')\n",
    "con.execute(drop) ## decomment during 1st run\n",
    "con.execute(query)\n",
    "con.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### SQlite Inserts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "amountinserts = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"INSERT INTO Customer VALUES ('average1','joe.average1@email.com',10000)\",\n",
       " \"INSERT INTO Customer VALUES ('Piet2','piet.yanneke2@email.nl',60000)\",\n",
       " \"INSERT INTO Customer VALUES ('Mustermann3','max.mustermann3@email.de',60000)\",\n",
       " \"INSERT INTO Customer VALUES ('average4','joe.average4@email.com',40000)\",\n",
       " \"INSERT INTO Customer VALUES ('Piet5','piet.yanneke5@email.nl',150000)\",\n",
       " \"INSERT INTO Customer VALUES ('Mustermann6','max.mustermann6@email.de',120000)\",\n",
       " \"INSERT INTO Customer VALUES ('average7','joe.average7@email.com',70000)\",\n",
       " \"INSERT INTO Customer VALUES ('Piet8','piet.yanneke8@email.nl',240000)\",\n",
       " \"INSERT INTO Customer VALUES ('Mustermann9','max.mustermann9@email.de',180000)\",\n",
       " \"INSERT INTO Customer VALUES ('average10','joe.average10@email.com',100000)\",\n",
       " \"INSERT INTO Customer VALUES ('Piet11','piet.yanneke11@email.nl',330000)\",\n",
       " \"INSERT INTO Customer VALUES ('Mustermann12','max.mustermann12@email.de',240000)\",\n",
       " \"INSERT INTO Customer VALUES ('average13','joe.average13@email.com',130000)\",\n",
       " \"INSERT INTO Customer VALUES ('Piet14','piet.yanneke14@email.nl',420000)\",\n",
       " \"INSERT INTO Customer VALUES ('Mustermann15','max.mustermann15@email.de',300000)\",\n",
       " \"INSERT INTO Customer VALUES ('average16','joe.average16@email.com',160000)\",\n",
       " \"INSERT INTO Customer VALUES ('Piet17','piet.yanneke17@email.nl',510000)\",\n",
       " \"INSERT INTO Customer VALUES ('Mustermann18','max.mustermann18@email.de',360000)\",\n",
       " \"INSERT INTO Customer VALUES ('average19','joe.average19@email.com',190000)\",\n",
       " \"INSERT INTO Customer VALUES ('Piet20','piet.yanneke20@email.nl',600000)\",\n",
       " \"INSERT INTO Customer VALUES ('Mustermann21','max.mustermann21@email.de',420000)\",\n",
       " \"INSERT INTO Customer VALUES ('average22','joe.average22@email.com',220000)\",\n",
       " \"INSERT INTO Customer VALUES ('Piet23','piet.yanneke23@email.nl',690000)\",\n",
       " \"INSERT INTO Customer VALUES ('Mustermann24','max.mustermann24@email.de',480000)\",\n",
       " \"INSERT INTO Customer VALUES ('average25','joe.average25@email.com',250000)\",\n",
       " \"INSERT INTO Customer VALUES ('Piet26','piet.yanneke26@email.nl',780000)\",\n",
       " \"INSERT INTO Customer VALUES ('Mustermann27','max.mustermann27@email.de',540000)\",\n",
       " \"INSERT INTO Customer VALUES ('average28','joe.average28@email.com',280000)\",\n",
       " \"INSERT INTO Customer VALUES ('Piet29','piet.yanneke29@email.nl',870000)\"]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data ={0 : ('Mustermann', 'max.mustermann@email.de', 20000),\n",
    "        1 : ('average', 'joe.average@email.com', 10000),\n",
    "        2 : ('Piet', 'piet.yanneke@email.nl', 30000)\n",
    "        }\n",
    "\n",
    "sqlliteinsertlist =[]\n",
    "i=1\n",
    "while i < amountinserts:\n",
    "    sqlliteinsert = \"INSERT INTO Customer VALUES (\"\n",
    "    sqlliteinsert = sqlliteinsert + \"\\'\" + data[i%len(data)][0] + str(i) + \"\\',\"  + \"\\'\" + insertChar(data[i%len(data)][1], data[i%len(data)][1].find('@'),str(i)) + \"\\',\" + str(i * data[i%len(data)][2]) + \")\" \n",
    "    con.execute(sqlliteinsert)\n",
    "    sqlliteinsertlist.append(sqlliteinsert)\n",
    "    i = i + 1\n",
    "\n",
    "sqlliteinsertlist    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('sqlite_insert.csv', mode='w') as csv_file:\n",
    "    fieldnames = ['sqldialect', 'DML Statement']\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for sqlliteinsert in sqlliteinsertlist:\n",
    "        ##print(mem.text)\n",
    "        ##text_file.write(mem.text)\n",
    "        writer.writerow({'sqldialect': 'sqllite_sql', 'DML Statement': sqlliteinsert})  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('average1', 'joe.average1@email.com', 10000.0),\n",
       " ('Piet2', 'piet.yanneke2@email.nl', 60000.0),\n",
       " ('Mustermann3', 'max.mustermann3@email.de', 60000.0),\n",
       " ('average4', 'joe.average4@email.com', 40000.0),\n",
       " ('Piet5', 'piet.yanneke5@email.nl', 150000.0),\n",
       " ('Mustermann6', 'max.mustermann6@email.de', 120000.0),\n",
       " ('average7', 'joe.average7@email.com', 70000.0),\n",
       " ('Piet8', 'piet.yanneke8@email.nl', 240000.0),\n",
       " ('Mustermann9', 'max.mustermann9@email.de', 180000.0),\n",
       " ('average10', 'joe.average10@email.com', 100000.0),\n",
       " ('Piet11', 'piet.yanneke11@email.nl', 330000.0),\n",
       " ('Mustermann12', 'max.mustermann12@email.de', 240000.0),\n",
       " ('average13', 'joe.average13@email.com', 130000.0),\n",
       " ('Piet14', 'piet.yanneke14@email.nl', 420000.0),\n",
       " ('Mustermann15', 'max.mustermann15@email.de', 300000.0),\n",
       " ('average16', 'joe.average16@email.com', 160000.0),\n",
       " ('Piet17', 'piet.yanneke17@email.nl', 510000.0),\n",
       " ('Mustermann18', 'max.mustermann18@email.de', 360000.0),\n",
       " ('average19', 'joe.average19@email.com', 190000.0),\n",
       " ('Piet20', 'piet.yanneke20@email.nl', 600000.0),\n",
       " ('Mustermann21', 'max.mustermann21@email.de', 420000.0),\n",
       " ('average22', 'joe.average22@email.com', 220000.0),\n",
       " ('Piet23', 'piet.yanneke23@email.nl', 690000.0),\n",
       " ('Mustermann24', 'max.mustermann24@email.de', 480000.0),\n",
       " ('average25', 'joe.average25@email.com', 250000.0),\n",
       " ('Piet26', 'piet.yanneke26@email.nl', 780000.0),\n",
       " ('Mustermann27', 'max.mustermann27@email.de', 540000.0),\n",
       " ('average28', 'joe.average28@email.com', 280000.0),\n",
       " ('Piet29', 'piet.yanneke29@email.nl', 870000.0)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cursor = con.execute('select lastname, email, networth from Customer')\n",
    "rows = cursor.fetchall()\n",
    "rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('lastname', None, None, None, None, None, None),\n",
       " ('email', None, None, None, None, None, None),\n",
       " ('networth', None, None, None, None, None, None))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cursor.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lastname</th>\n",
       "      <th>email</th>\n",
       "      <th>networth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>average1</td>\n",
       "      <td>joe.average1@email.com</td>\n",
       "      <td>10000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Piet2</td>\n",
       "      <td>piet.yanneke2@email.nl</td>\n",
       "      <td>60000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mustermann3</td>\n",
       "      <td>max.mustermann3@email.de</td>\n",
       "      <td>60000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>average4</td>\n",
       "      <td>joe.average4@email.com</td>\n",
       "      <td>40000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Piet5</td>\n",
       "      <td>piet.yanneke5@email.nl</td>\n",
       "      <td>150000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Mustermann6</td>\n",
       "      <td>max.mustermann6@email.de</td>\n",
       "      <td>120000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>average7</td>\n",
       "      <td>joe.average7@email.com</td>\n",
       "      <td>70000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Piet8</td>\n",
       "      <td>piet.yanneke8@email.nl</td>\n",
       "      <td>240000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Mustermann9</td>\n",
       "      <td>max.mustermann9@email.de</td>\n",
       "      <td>180000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>average10</td>\n",
       "      <td>joe.average10@email.com</td>\n",
       "      <td>100000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Piet11</td>\n",
       "      <td>piet.yanneke11@email.nl</td>\n",
       "      <td>330000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Mustermann12</td>\n",
       "      <td>max.mustermann12@email.de</td>\n",
       "      <td>240000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>average13</td>\n",
       "      <td>joe.average13@email.com</td>\n",
       "      <td>130000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Piet14</td>\n",
       "      <td>piet.yanneke14@email.nl</td>\n",
       "      <td>420000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Mustermann15</td>\n",
       "      <td>max.mustermann15@email.de</td>\n",
       "      <td>300000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>average16</td>\n",
       "      <td>joe.average16@email.com</td>\n",
       "      <td>160000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Piet17</td>\n",
       "      <td>piet.yanneke17@email.nl</td>\n",
       "      <td>510000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Mustermann18</td>\n",
       "      <td>max.mustermann18@email.de</td>\n",
       "      <td>360000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>average19</td>\n",
       "      <td>joe.average19@email.com</td>\n",
       "      <td>190000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Piet20</td>\n",
       "      <td>piet.yanneke20@email.nl</td>\n",
       "      <td>600000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Mustermann21</td>\n",
       "      <td>max.mustermann21@email.de</td>\n",
       "      <td>420000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>average22</td>\n",
       "      <td>joe.average22@email.com</td>\n",
       "      <td>220000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Piet23</td>\n",
       "      <td>piet.yanneke23@email.nl</td>\n",
       "      <td>690000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Mustermann24</td>\n",
       "      <td>max.mustermann24@email.de</td>\n",
       "      <td>480000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>average25</td>\n",
       "      <td>joe.average25@email.com</td>\n",
       "      <td>250000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Piet26</td>\n",
       "      <td>piet.yanneke26@email.nl</td>\n",
       "      <td>780000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Mustermann27</td>\n",
       "      <td>max.mustermann27@email.de</td>\n",
       "      <td>540000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>average28</td>\n",
       "      <td>joe.average28@email.com</td>\n",
       "      <td>280000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Piet29</td>\n",
       "      <td>piet.yanneke29@email.nl</td>\n",
       "      <td>870000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        lastname                      email  networth\n",
       "0       average1     joe.average1@email.com   10000.0\n",
       "1          Piet2     piet.yanneke2@email.nl   60000.0\n",
       "2    Mustermann3   max.mustermann3@email.de   60000.0\n",
       "3       average4     joe.average4@email.com   40000.0\n",
       "4          Piet5     piet.yanneke5@email.nl  150000.0\n",
       "5    Mustermann6   max.mustermann6@email.de  120000.0\n",
       "6       average7     joe.average7@email.com   70000.0\n",
       "7          Piet8     piet.yanneke8@email.nl  240000.0\n",
       "8    Mustermann9   max.mustermann9@email.de  180000.0\n",
       "9      average10    joe.average10@email.com  100000.0\n",
       "10        Piet11    piet.yanneke11@email.nl  330000.0\n",
       "11  Mustermann12  max.mustermann12@email.de  240000.0\n",
       "12     average13    joe.average13@email.com  130000.0\n",
       "13        Piet14    piet.yanneke14@email.nl  420000.0\n",
       "14  Mustermann15  max.mustermann15@email.de  300000.0\n",
       "15     average16    joe.average16@email.com  160000.0\n",
       "16        Piet17    piet.yanneke17@email.nl  510000.0\n",
       "17  Mustermann18  max.mustermann18@email.de  360000.0\n",
       "18     average19    joe.average19@email.com  190000.0\n",
       "19        Piet20    piet.yanneke20@email.nl  600000.0\n",
       "20  Mustermann21  max.mustermann21@email.de  420000.0\n",
       "21     average22    joe.average22@email.com  220000.0\n",
       "22        Piet23    piet.yanneke23@email.nl  690000.0\n",
       "23  Mustermann24  max.mustermann24@email.de  480000.0\n",
       "24     average25    joe.average25@email.com  250000.0\n",
       "25        Piet26    piet.yanneke26@email.nl  780000.0\n",
       "26  Mustermann27  max.mustermann27@email.de  540000.0\n",
       "27     average28    joe.average28@email.com  280000.0\n",
       "28        Piet29    piet.yanneke29@email.nl  870000.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## transfer data to pandas\n",
    "dfsqllitedata = pd.DataFrame(rows, columns=[x[0] for x in cursor.description])\n",
    "dfsqllitedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lastname</th>\n",
       "      <th>email</th>\n",
       "      <th>networth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>average1</td>\n",
       "      <td>joe.average1@email.com</td>\n",
       "      <td>10000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Piet2</td>\n",
       "      <td>piet.yanneke2@email.nl</td>\n",
       "      <td>60000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mustermann3</td>\n",
       "      <td>max.mustermann3@email.de</td>\n",
       "      <td>60000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>average4</td>\n",
       "      <td>joe.average4@email.com</td>\n",
       "      <td>40000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Piet5</td>\n",
       "      <td>piet.yanneke5@email.nl</td>\n",
       "      <td>150000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Mustermann6</td>\n",
       "      <td>max.mustermann6@email.de</td>\n",
       "      <td>120000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>average7</td>\n",
       "      <td>joe.average7@email.com</td>\n",
       "      <td>70000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Piet8</td>\n",
       "      <td>piet.yanneke8@email.nl</td>\n",
       "      <td>240000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Mustermann9</td>\n",
       "      <td>max.mustermann9@email.de</td>\n",
       "      <td>180000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>average10</td>\n",
       "      <td>joe.average10@email.com</td>\n",
       "      <td>100000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Piet11</td>\n",
       "      <td>piet.yanneke11@email.nl</td>\n",
       "      <td>330000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Mustermann12</td>\n",
       "      <td>max.mustermann12@email.de</td>\n",
       "      <td>240000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>average13</td>\n",
       "      <td>joe.average13@email.com</td>\n",
       "      <td>130000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Piet14</td>\n",
       "      <td>piet.yanneke14@email.nl</td>\n",
       "      <td>420000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Mustermann15</td>\n",
       "      <td>max.mustermann15@email.de</td>\n",
       "      <td>300000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>average16</td>\n",
       "      <td>joe.average16@email.com</td>\n",
       "      <td>160000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Piet17</td>\n",
       "      <td>piet.yanneke17@email.nl</td>\n",
       "      <td>510000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Mustermann18</td>\n",
       "      <td>max.mustermann18@email.de</td>\n",
       "      <td>360000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>average19</td>\n",
       "      <td>joe.average19@email.com</td>\n",
       "      <td>190000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Piet20</td>\n",
       "      <td>piet.yanneke20@email.nl</td>\n",
       "      <td>600000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Mustermann21</td>\n",
       "      <td>max.mustermann21@email.de</td>\n",
       "      <td>420000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>average22</td>\n",
       "      <td>joe.average22@email.com</td>\n",
       "      <td>220000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Piet23</td>\n",
       "      <td>piet.yanneke23@email.nl</td>\n",
       "      <td>690000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Mustermann24</td>\n",
       "      <td>max.mustermann24@email.de</td>\n",
       "      <td>480000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>average25</td>\n",
       "      <td>joe.average25@email.com</td>\n",
       "      <td>250000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Piet26</td>\n",
       "      <td>piet.yanneke26@email.nl</td>\n",
       "      <td>780000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Mustermann27</td>\n",
       "      <td>max.mustermann27@email.de</td>\n",
       "      <td>540000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>average28</td>\n",
       "      <td>joe.average28@email.com</td>\n",
       "      <td>280000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Piet29</td>\n",
       "      <td>piet.yanneke29@email.nl</td>\n",
       "      <td>870000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        lastname                      email  networth\n",
       "0       average1     joe.average1@email.com   10000.0\n",
       "1          Piet2     piet.yanneke2@email.nl   60000.0\n",
       "2    Mustermann3   max.mustermann3@email.de   60000.0\n",
       "3       average4     joe.average4@email.com   40000.0\n",
       "4          Piet5     piet.yanneke5@email.nl  150000.0\n",
       "5    Mustermann6   max.mustermann6@email.de  120000.0\n",
       "6       average7     joe.average7@email.com   70000.0\n",
       "7          Piet8     piet.yanneke8@email.nl  240000.0\n",
       "8    Mustermann9   max.mustermann9@email.de  180000.0\n",
       "9      average10    joe.average10@email.com  100000.0\n",
       "10        Piet11    piet.yanneke11@email.nl  330000.0\n",
       "11  Mustermann12  max.mustermann12@email.de  240000.0\n",
       "12     average13    joe.average13@email.com  130000.0\n",
       "13        Piet14    piet.yanneke14@email.nl  420000.0\n",
       "14  Mustermann15  max.mustermann15@email.de  300000.0\n",
       "15     average16    joe.average16@email.com  160000.0\n",
       "16        Piet17    piet.yanneke17@email.nl  510000.0\n",
       "17  Mustermann18  max.mustermann18@email.de  360000.0\n",
       "18     average19    joe.average19@email.com  190000.0\n",
       "19        Piet20    piet.yanneke20@email.nl  600000.0\n",
       "20  Mustermann21  max.mustermann21@email.de  420000.0\n",
       "21     average22    joe.average22@email.com  220000.0\n",
       "22        Piet23    piet.yanneke23@email.nl  690000.0\n",
       "23  Mustermann24  max.mustermann24@email.de  480000.0\n",
       "24     average25    joe.average25@email.com  250000.0\n",
       "25        Piet26    piet.yanneke26@email.nl  780000.0\n",
       "26  Mustermann27  max.mustermann27@email.de  540000.0\n",
       "27     average28    joe.average28@email.com  280000.0\n",
       "28        Piet29    piet.yanneke29@email.nl  870000.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## TO DO transfer sqllite statement to pandas\n",
    "dfsqllitestatements = pd.DataFrame(rows, columns=[x[0] for x in cursor.description])\n",
    "dfsqllitestatements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### [LATER] SQLite Update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  [LATER] SQLite Delete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  [LATER] SQLite Select"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mongo "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.w3schools.com/python/python_mongodb_getstarted.asp\n",
    "https://docs.mongodb.com/manual/tutorial/install-mongodb-on-os-x/#install-mongodb-community-edition\n",
    "\n",
    "See: https://docs.brew.sh/Homebrew-and-Python\n",
    "==> mongodb\n",
    "To have launchd start mongodb now and restart at login:\n",
    "  brew services start mongodb\n",
    "Or, if you don't want/need a background service you can just run:\n",
    "  mongod --config /usr/local/etc/mongod.conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting brew\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/19/75f6d42ca862c6b31e2da9864d94f59fe81978ac5d40c43937a1c17fd065/brew-0.1.4.zip (48kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 31.1MB/s ta 0:00:01\n",
      "\u001b[?25h    Complete output from command python setup.py egg_info:\n",
      "    Traceback (most recent call last):\n",
      "      File \"<string>\", line 1, in <module>\n",
      "      File \"/tmp/pip-install-yoi072fe/brew/setup.py\", line 22, in <module>\n",
      "        with open('requirements.txt') as fid:\n",
      "    FileNotFoundError: [Errno 2] No such file or directory: 'requirements.txt'\n",
      "    \n",
      "    ----------------------------------------\n",
      "\u001b[31mCommand \"python setup.py egg_info\" failed with error code 1 in /tmp/pip-install-yoi072fe/brew/\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "##!pip install pymongo\n",
    "##!pip install brew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: brew: not found\r\n"
     ]
    }
   ],
   "source": [
    "import pymongo\n",
    "\n",
    "## !brew services start mongodb\n",
    "## !brew services stop mongodb\n",
    "## !brew services restart mongodb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "## uname -a\n",
    "## https://docs.mongodb.com/manual/tutorial/install-mongodb-on-debian/\n",
    "\n",
    "!sudo service mongod start\n",
    "## !tail /var/log/mongodb/mongod.log | grep -m1 \"waiting for connections on port\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04T05:54:46.140+0000 I NETWORK  [initandlisten] waiting for connections on port 27017\r\n"
     ]
    }
   ],
   "source": [
    "## check\n",
    "!tail /var/log/mongodb/mongod.log | grep -m1 \"waiting for connections on port\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo service mongod stop\n",
    "##!rm /var/log/mongodb/mongod.log ## wont work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04T05:41:17.121+0000 I NETWORK  [initandlisten] waiting for connections on port 27017\r\n"
     ]
    }
   ],
   "source": [
    "!sudo service mongod restart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MongoDB waits until you have created a collection (table), \n",
    "## with at least one document (record) before it actually creates the database (and collection).\n",
    "\n",
    "myclient = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "\n",
    "mydb = myclient[\"mydatabase\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['admin', 'config', 'local', 'mydatabase']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dblist = myclient.list_database_names()\n",
    "dblist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The database exists.\n"
     ]
    }
   ],
   "source": [
    "if \"mydatabase\" in dblist:\n",
    "  print(\"The database exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['admin', 'config', 'local', 'mydatabase']\n"
     ]
    }
   ],
   "source": [
    "print(myclient.list_database_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "mycol = mydb[\"Clients\"]\n",
    "mycol.drop()\n",
    "mycol = mydb[\"Clients\"]\n",
    "print(mydb.list_collection_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "collist = mydb.list_collection_names()\n",
    "if \"Clients\" in collist:\n",
    "  print(\"The collection exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Mongo Insert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'average1', 'emailid': 'joe.average1@email.com'},\n",
       " {'name': 'Piet2', 'emailid': 'piet.yanneke2@email.nl'},\n",
       " {'name': 'Mustermann3', 'emailid': 'max.mustermann3@email.de'},\n",
       " {'name': 'average4', 'emailid': 'joe.average4@email.com'},\n",
       " {'name': 'Piet5', 'emailid': 'piet.yanneke5@email.nl'},\n",
       " {'name': 'Mustermann6', 'emailid': 'max.mustermann6@email.de'},\n",
       " {'name': 'average7', 'emailid': 'joe.average7@email.com'},\n",
       " {'name': 'Piet8', 'emailid': 'piet.yanneke8@email.nl'},\n",
       " {'name': 'Mustermann9', 'emailid': 'max.mustermann9@email.de'},\n",
       " {'name': 'average10', 'emailid': 'joe.average10@email.com'},\n",
       " {'name': 'Piet11', 'emailid': 'piet.yanneke11@email.nl'},\n",
       " {'name': 'Mustermann12', 'emailid': 'max.mustermann12@email.de'},\n",
       " {'name': 'average13', 'emailid': 'joe.average13@email.com'},\n",
       " {'name': 'Piet14', 'emailid': 'piet.yanneke14@email.nl'},\n",
       " {'name': 'Mustermann15', 'emailid': 'max.mustermann15@email.de'},\n",
       " {'name': 'average16', 'emailid': 'joe.average16@email.com'},\n",
       " {'name': 'Piet17', 'emailid': 'piet.yanneke17@email.nl'},\n",
       " {'name': 'Mustermann18', 'emailid': 'max.mustermann18@email.de'},\n",
       " {'name': 'average19', 'emailid': 'joe.average19@email.com'},\n",
       " {'name': 'Piet20', 'emailid': 'piet.yanneke20@email.nl'},\n",
       " {'name': 'Mustermann21', 'emailid': 'max.mustermann21@email.de'},\n",
       " {'name': 'average22', 'emailid': 'joe.average22@email.com'},\n",
       " {'name': 'Piet23', 'emailid': 'piet.yanneke23@email.nl'},\n",
       " {'name': 'Mustermann24', 'emailid': 'max.mustermann24@email.de'},\n",
       " {'name': 'average25', 'emailid': 'joe.average25@email.com'},\n",
       " {'name': 'Piet26', 'emailid': 'piet.yanneke26@email.nl'},\n",
       " {'name': 'Mustermann27', 'emailid': 'max.mustermann27@email.de'},\n",
       " {'name': 'average28', 'emailid': 'joe.average28@email.com'},\n",
       " {'name': 'Piet29', 'emailid': 'piet.yanneke29@email.nl'}]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##data ={0 : ('Mustermann', 'max.mustermann@email.de', 20000),\n",
    "##        1 : ('average', 'joe.average@email.com', 10000),\n",
    "##        2 : ('Piet', 'piet.yanneke@email.nl', 30000)\n",
    "##        }\n",
    "\n",
    "## Collection Clients, with slightly different column names and one less\n",
    "mylist = [{ \"name\": \"Mustermann\", \"emailid\": \"max.mustermann@email.de\"}\n",
    "            ,{ \"name\": \"average\", \"emailid\": \"joe.average@email.com\"}\n",
    "            , {\"name\": \"Piet\", \"emailid\": \"piet.yanneke@email.nl\"}\n",
    "         ]\n",
    "\n",
    "## d = data['response']['globalstats']['heist_success']['history']\n",
    "## result_dict = dict((i[\"date\"],i[\"total\"]) for i in d)\n",
    "\n",
    "## mongoinsertlist ={}\n",
    "mongoinsertlist =[]\n",
    "i=1\n",
    "mongoinserttest = mylist[i%len(mylist)][\"name\"]\n",
    "mongoinserttest = []\n",
    "## mongoinsertlist = \"[\"\n",
    "while i < amountinserts:\n",
    "    mongoinsertnamekey = \"name\"\n",
    "    mongoinsertnamevalue =  mylist[i%len(mylist)][\"name\"] + str(i)  ## '\\\"'  \n",
    "    \n",
    "    mongoinsertemailidkey = \"emailid\"\n",
    "    mongoinsertemaildvalue =  mylist[i%len(mylist)][\"emailid\"] ## '\\\"'  ## insertChar(data[i%len(data)][1], data[i%len(data)][1].find('@'),str(i)) + \"\\',\" + str(i * data[i%len(data)][2]) + \")\"\n",
    "    mongoinsertemaildvalue = insertChar(mongoinsertemaildvalue, mongoinsertemaildvalue.find('@'),str(i))\n",
    "    \n",
    "    client = { mongoinsertnamekey : mongoinsertnamevalue , mongoinsertemailidkey : mongoinsertemaildvalue }\n",
    "    ##mycol.insert_one(mongoinsert)\n",
    "    mongoinsertlist.append(client)\n",
    "    i = i + 1\n",
    "    \n",
    "mongoinsertlist\n",
    "##mylist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('mongodb_insert.csv', mode='w') as csv_file:\n",
    "    fieldnames = ['sqldialect', 'DML Statement']\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for mongoinsert in mongoinsertlist:\n",
    "        ##print(mem.text)\n",
    "        ##text_file.write(mem.text)\n",
    "        writer.writerow({'sqldialect': 'mongodb_sql', 'DML Statement': mongoinsert})  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##insertChar??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "## mongoinsert1 = { \"name\": \"Mustermann\", \"emailid\": \"max.mustermann@email.de\"}\n",
    "## mongoinsert2 = { \"name\": \"average\", \"emailid\": \"joe.average@email.com\"}\n",
    "## mongoinsert3 = {\"name\": \"Piet\", \"emailid\": \"piet.yanneke@email.nl\"}\n",
    "\n",
    "## x1 = mycol.insert_one(mongoinsert1)\n",
    "## x2 = mycol.insert_one(mongoinsert2)\n",
    "## x3 = mycol.insert_one(mongoinsert3)\n",
    "\n",
    "## print(x1.inserted_id)\n",
    "## print(x2.inserted_id)\n",
    "## print(x3.inserted_id)\n",
    "\n",
    "mycol.drop()\n",
    "x = mycol.insert_many(mongoinsertlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO DO: doesnt work yet, but no problem. We want to zip the sql statements not the data\n",
    "## transfer data to pandas\n",
    "##dfmongodata = pd.DataFrame(rowsmongo)\n",
    "##dfmongodata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  [LATER] Mongo Update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  [LATER] Mongo Delete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  [LATER] Mongo Select"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Zipping the pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target format, same as for natural languages. Only the dot might not mark a sentence end, even though we could just add it.\n",
    "\n",
    "http://www.manythings.org/anki/\n",
    "\n",
    "    I'm ill.\tIk ben ziek.\n",
    "    I'm sad.\tIk ben bedroefd.\n",
    "    It's me!\tIk ben het.\n",
    "    It's me.\tIk ben het.\n",
    "    Join us.\tKom met ons mee.\n",
    "    Join us.\tDoe maar mee.\n",
    "    Join us.\tKom maar meedoen.\n",
    "    Join us.\tSluit je aan.\n",
    "    Join us.\tSluit je bij ons aan.\n",
    "    Keep it.\tHoud het.\n",
    "    See you!\tTot weerziens!\n",
    "    Shut up!\tZwijg!\n",
    "    Stop it.\tHou daar toch mee op.\n",
    "    \n",
    "We do this with Panda .... we take the respective Insert, and Select statemeents from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sqlite</th>\n",
       "      <th>mongosql</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INSERT INTO Customer VALUES ('average1','joe.a...</td>\n",
       "      <td>{'name': 'average1', 'emailid': 'joe.average1@...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>INSERT INTO Customer VALUES ('Piet2','piet.yan...</td>\n",
       "      <td>{'name': 'Piet2', 'emailid': 'piet.yanneke2@em...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INSERT INTO Customer VALUES ('Mustermann3','ma...</td>\n",
       "      <td>{'name': 'Mustermann3', 'emailid': 'max.muster...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INSERT INTO Customer VALUES ('average4','joe.a...</td>\n",
       "      <td>{'name': 'average4', 'emailid': 'joe.average4@...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>INSERT INTO Customer VALUES ('Piet5','piet.yan...</td>\n",
       "      <td>{'name': 'Piet5', 'emailid': 'piet.yanneke5@em...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>INSERT INTO Customer VALUES ('Mustermann6','ma...</td>\n",
       "      <td>{'name': 'Mustermann6', 'emailid': 'max.muster...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>INSERT INTO Customer VALUES ('average7','joe.a...</td>\n",
       "      <td>{'name': 'average7', 'emailid': 'joe.average7@...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>INSERT INTO Customer VALUES ('Piet8','piet.yan...</td>\n",
       "      <td>{'name': 'Piet8', 'emailid': 'piet.yanneke8@em...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>INSERT INTO Customer VALUES ('Mustermann9','ma...</td>\n",
       "      <td>{'name': 'Mustermann9', 'emailid': 'max.muster...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>INSERT INTO Customer VALUES ('average10','joe....</td>\n",
       "      <td>{'name': 'average10', 'emailid': 'joe.average1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>INSERT INTO Customer VALUES ('Piet11','piet.ya...</td>\n",
       "      <td>{'name': 'Piet11', 'emailid': 'piet.yanneke11@...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>INSERT INTO Customer VALUES ('Mustermann12','m...</td>\n",
       "      <td>{'name': 'Mustermann12', 'emailid': 'max.muste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>INSERT INTO Customer VALUES ('average13','joe....</td>\n",
       "      <td>{'name': 'average13', 'emailid': 'joe.average1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>INSERT INTO Customer VALUES ('Piet14','piet.ya...</td>\n",
       "      <td>{'name': 'Piet14', 'emailid': 'piet.yanneke14@...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>INSERT INTO Customer VALUES ('Mustermann15','m...</td>\n",
       "      <td>{'name': 'Mustermann15', 'emailid': 'max.muste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>INSERT INTO Customer VALUES ('average16','joe....</td>\n",
       "      <td>{'name': 'average16', 'emailid': 'joe.average1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>INSERT INTO Customer VALUES ('Piet17','piet.ya...</td>\n",
       "      <td>{'name': 'Piet17', 'emailid': 'piet.yanneke17@...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>INSERT INTO Customer VALUES ('Mustermann18','m...</td>\n",
       "      <td>{'name': 'Mustermann18', 'emailid': 'max.muste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>INSERT INTO Customer VALUES ('average19','joe....</td>\n",
       "      <td>{'name': 'average19', 'emailid': 'joe.average1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>INSERT INTO Customer VALUES ('Piet20','piet.ya...</td>\n",
       "      <td>{'name': 'Piet20', 'emailid': 'piet.yanneke20@...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>INSERT INTO Customer VALUES ('Mustermann21','m...</td>\n",
       "      <td>{'name': 'Mustermann21', 'emailid': 'max.muste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>INSERT INTO Customer VALUES ('average22','joe....</td>\n",
       "      <td>{'name': 'average22', 'emailid': 'joe.average2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>INSERT INTO Customer VALUES ('Piet23','piet.ya...</td>\n",
       "      <td>{'name': 'Piet23', 'emailid': 'piet.yanneke23@...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>INSERT INTO Customer VALUES ('Mustermann24','m...</td>\n",
       "      <td>{'name': 'Mustermann24', 'emailid': 'max.muste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>INSERT INTO Customer VALUES ('average25','joe....</td>\n",
       "      <td>{'name': 'average25', 'emailid': 'joe.average2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>INSERT INTO Customer VALUES ('Piet26','piet.ya...</td>\n",
       "      <td>{'name': 'Piet26', 'emailid': 'piet.yanneke26@...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>INSERT INTO Customer VALUES ('Mustermann27','m...</td>\n",
       "      <td>{'name': 'Mustermann27', 'emailid': 'max.muste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>INSERT INTO Customer VALUES ('average28','joe....</td>\n",
       "      <td>{'name': 'average28', 'emailid': 'joe.average2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>INSERT INTO Customer VALUES ('Piet29','piet.ya...</td>\n",
       "      <td>{'name': 'Piet29', 'emailid': 'piet.yanneke29@...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sqlite  \\\n",
       "0   INSERT INTO Customer VALUES ('average1','joe.a...   \n",
       "1   INSERT INTO Customer VALUES ('Piet2','piet.yan...   \n",
       "2   INSERT INTO Customer VALUES ('Mustermann3','ma...   \n",
       "3   INSERT INTO Customer VALUES ('average4','joe.a...   \n",
       "4   INSERT INTO Customer VALUES ('Piet5','piet.yan...   \n",
       "5   INSERT INTO Customer VALUES ('Mustermann6','ma...   \n",
       "6   INSERT INTO Customer VALUES ('average7','joe.a...   \n",
       "7   INSERT INTO Customer VALUES ('Piet8','piet.yan...   \n",
       "8   INSERT INTO Customer VALUES ('Mustermann9','ma...   \n",
       "9   INSERT INTO Customer VALUES ('average10','joe....   \n",
       "10  INSERT INTO Customer VALUES ('Piet11','piet.ya...   \n",
       "11  INSERT INTO Customer VALUES ('Mustermann12','m...   \n",
       "12  INSERT INTO Customer VALUES ('average13','joe....   \n",
       "13  INSERT INTO Customer VALUES ('Piet14','piet.ya...   \n",
       "14  INSERT INTO Customer VALUES ('Mustermann15','m...   \n",
       "15  INSERT INTO Customer VALUES ('average16','joe....   \n",
       "16  INSERT INTO Customer VALUES ('Piet17','piet.ya...   \n",
       "17  INSERT INTO Customer VALUES ('Mustermann18','m...   \n",
       "18  INSERT INTO Customer VALUES ('average19','joe....   \n",
       "19  INSERT INTO Customer VALUES ('Piet20','piet.ya...   \n",
       "20  INSERT INTO Customer VALUES ('Mustermann21','m...   \n",
       "21  INSERT INTO Customer VALUES ('average22','joe....   \n",
       "22  INSERT INTO Customer VALUES ('Piet23','piet.ya...   \n",
       "23  INSERT INTO Customer VALUES ('Mustermann24','m...   \n",
       "24  INSERT INTO Customer VALUES ('average25','joe....   \n",
       "25  INSERT INTO Customer VALUES ('Piet26','piet.ya...   \n",
       "26  INSERT INTO Customer VALUES ('Mustermann27','m...   \n",
       "27  INSERT INTO Customer VALUES ('average28','joe....   \n",
       "28  INSERT INTO Customer VALUES ('Piet29','piet.ya...   \n",
       "\n",
       "                                             mongosql  \n",
       "0   {'name': 'average1', 'emailid': 'joe.average1@...  \n",
       "1   {'name': 'Piet2', 'emailid': 'piet.yanneke2@em...  \n",
       "2   {'name': 'Mustermann3', 'emailid': 'max.muster...  \n",
       "3   {'name': 'average4', 'emailid': 'joe.average4@...  \n",
       "4   {'name': 'Piet5', 'emailid': 'piet.yanneke5@em...  \n",
       "5   {'name': 'Mustermann6', 'emailid': 'max.muster...  \n",
       "6   {'name': 'average7', 'emailid': 'joe.average7@...  \n",
       "7   {'name': 'Piet8', 'emailid': 'piet.yanneke8@em...  \n",
       "8   {'name': 'Mustermann9', 'emailid': 'max.muster...  \n",
       "9   {'name': 'average10', 'emailid': 'joe.average1...  \n",
       "10  {'name': 'Piet11', 'emailid': 'piet.yanneke11@...  \n",
       "11  {'name': 'Mustermann12', 'emailid': 'max.muste...  \n",
       "12  {'name': 'average13', 'emailid': 'joe.average1...  \n",
       "13  {'name': 'Piet14', 'emailid': 'piet.yanneke14@...  \n",
       "14  {'name': 'Mustermann15', 'emailid': 'max.muste...  \n",
       "15  {'name': 'average16', 'emailid': 'joe.average1...  \n",
       "16  {'name': 'Piet17', 'emailid': 'piet.yanneke17@...  \n",
       "17  {'name': 'Mustermann18', 'emailid': 'max.muste...  \n",
       "18  {'name': 'average19', 'emailid': 'joe.average1...  \n",
       "19  {'name': 'Piet20', 'emailid': 'piet.yanneke20@...  \n",
       "20  {'name': 'Mustermann21', 'emailid': 'max.muste...  \n",
       "21  {'name': 'average22', 'emailid': 'joe.average2...  \n",
       "22  {'name': 'Piet23', 'emailid': 'piet.yanneke23@...  \n",
       "23  {'name': 'Mustermann24', 'emailid': 'max.muste...  \n",
       "24  {'name': 'average25', 'emailid': 'joe.average2...  \n",
       "25  {'name': 'Piet26', 'emailid': 'piet.yanneke26@...  \n",
       "26  {'name': 'Mustermann27', 'emailid': 'max.muste...  \n",
       "27  {'name': 'average28', 'emailid': 'joe.average2...  \n",
       "28  {'name': 'Piet29', 'emailid': 'piet.yanneke29@...  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## zip a dataframe of \n",
    "\n",
    "\n",
    "##mongoinsertlist = [mongoinsert1, mongoinsert1, mongoinsert3]\n",
    "## sqlliteinsert1 mongoinsert1\n",
    "## sqlliteinsert2 mongoinsert2\n",
    "## sqlliteinsert3 mongoinsert3\n",
    "\n",
    "## sqlliteinsert1 : \"INSERT INTO Customer VALUES ('Mustermann', 'max.mustermann@email.de', 20000)\"\n",
    "## mongoinsert1 : {'name': 'Mustermann', 'emailid': 'max.mustermann@email.de', '_id': ObjectId('5bd5aad637b8896cc56ea929')}\n",
    "\n",
    "sql_tuples = list(zip(sqlliteinsertlist,mongoinsertlist ))\n",
    "pd.DataFrame(sql_tuples, columns=['sqlite','mongosql'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [CURRENT] Building a Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UMLFIT for wikipedia corpuse: https://github.com/n-waves/ulmfit-multilingual/tree/master/ulmfit\n",
    "\n",
    "some for github and python?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##?pd.read_clipboard\n",
    "##?pd.read_csv\n",
    "##?pd.read_excel\n",
    "##?pd.read_feather\n",
    "##?pd.read_fwf\n",
    "##?pd.read_gbq\n",
    "##?pd.read_hdf\n",
    "##?pd.read_html\n",
    "##?pd.read_json\n",
    "##?pd.read_msgpack\n",
    "##?pd.read_parquet\n",
    "##?pd.read_pickle\n",
    "##?pd.read_sas\n",
    "##?pd.read_sql\n",
    "##?pd.read_sql_query\n",
    "##?pd.read_stata\n",
    "##?pd.read_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.fast.ai/text.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.text import *\n",
    "\n",
    "df_mongo = pd.read_csv('mongodb_insert.csv')\n",
    "df_sqlite = pd.read_csv('sqlite_insert.csv')\n",
    "\n",
    "df_mix = df_mongo.append(df_sqlite)\n",
    "df_mix\n",
    "df_mix.to_csv('sql_mix.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Language model data\n",
    "path = './'\n",
    "data_lm = TextLMDataBunch.from_csv(path, 'sql_mix.csv')\n",
    "# Classifier model data\n",
    "data_clas = TextClasDataBunch.from_csv(path, 'sql_mix.csv', vocab=data_lm.train_ds.vocab, bs=8)\n",
    "## data_lm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This does all the necessary preprocessing behing the scene. For the classifier, we also pass the vocabulary (mapping from ids to words) that we want to use: this is to ensure that data_clas will use the same dictionary as data_lm.\n",
    "\n",
    "Since this step can be a bit time-consuming, it's best to save the result with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lm.save()\n",
    "data_clas.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to save some time we store this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lm = TextLMDataBunch.load(path)\n",
    "data_clas = TextClasDataBunch.load(path, bs=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the data_lm object we created earlier to fine-tune a pretrained language model. \n",
    "\n",
    "fast.ai has an English model available that we can download. We can create a learner object that will directly create a model, download the pretrained weights and be ready for fine-tuning.\n",
    "\n",
    "[TO DO] Can we make a pre-trained model e.g. for python?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "        \t/* Turns off some styling */\n",
       "        \tprogress {\n",
       "\n",
       "            \t/* gets rid of default border in Firefox and Opera. */\n",
       "            \tborder: none;\n",
       "\n",
       "            \t/* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "            \tbackground-size: auto;\n",
       "            }\n",
       "\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='1', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/1 00:00<00:00]\n",
       "    </div>\n",
       "    \n",
       "<table style='width:300px; margin-bottom:10px'>\n",
       "  <tr>\n",
       "    <th>epoch</th>\n",
       "    <th>train_loss</th>\n",
       "    <th>valid_loss</th>\n",
       "    <th>accuracy</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "\n",
       "  </tr>\n",
       "</table>\n",
       "\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "        \t/* Turns off some styling */\n",
       "        \tprogress {\n",
       "\n",
       "            \t/* gets rid of default border in Firefox and Opera. */\n",
       "            \tborder: none;\n",
       "\n",
       "            \t/* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "            \tbackground-size: auto;\n",
       "            }\n",
       "\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='0', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100% [0/0]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/fastprogress/fastprogress.py:95: UserWarning: Your generator is empty.\n",
      "  warn(\"Your generator is empty.\")\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "__len__() should return >= 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-314-7b61575a202a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlearn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlanguage_model_learner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_lm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mURLs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWT103\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_mult\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_one_cycle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/fastai/train.py\u001b[0m in \u001b[0;36mfit_one_cycle\u001b[0;34m(learn, cyc_len, max_lr, moms, div_factor, pct_start, wd, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m     19\u001b[0m     callbacks.append(OneCycleScheduler(learn, max_lr, moms=moms, div_factor=div_factor,\n\u001b[1;32m     20\u001b[0m                                         pct_start=pct_start, **kwargs))\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcyc_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlr_find\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mLearner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_lr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_lr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_it\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_div\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, epochs, lr, wd, callbacks)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_fns\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         fit(epochs, self.model, self.loss_func, opt=self.opt, data=self.data, metrics=self.metrics,\n\u001b[0;32m--> 163\u001b[0;31m             callbacks=self.callbacks+callbacks)\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(epochs, model, loss_func, opt, data, callbacks, metrics)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mexception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(epochs, model, loss_func, opt, data, callbacks, metrics)\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'valid_dl'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid_dl\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid_ds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                 val_loss = validate(model, data.valid_dl, loss_func=loss_func,\n\u001b[0;32m---> 89\u001b[0;31m                                        cb_handler=cb_handler, pbar=pbar)\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(model, dl, loss_func, cb_handler, pbar, average, n_batch)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mval_losses\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnums\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpbar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpbar\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mval_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb_handler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/fastprogress/fastprogress.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, gen, total, display, leave, parent, auto_update)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mNBProgressBar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mProgressBar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauto_update\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogress\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhtml_progress_bar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtotal\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauto_update\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/fastai/basic_data.py\u001b[0m in \u001b[0;36m__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtfms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtfms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: __len__() should return >= 0"
     ]
    }
   ],
   "source": [
    "learn = language_model_learner(data_lm, pretrained_model=URLs.WT103, drop_mult=0.5)\n",
    "learn.fit_one_cycle(1, 1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like a computer vision model, we can then unfreeze the model and fine-tune it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "        \t/* Turns off some styling */\n",
       "        \tprogress {\n",
       "\n",
       "            \t/* gets rid of default border in Firefox and Opera. */\n",
       "            \tborder: none;\n",
       "\n",
       "            \t/* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "            \tbackground-size: auto;\n",
       "            }\n",
       "\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='1', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/1 00:00<00:00]\n",
       "    </div>\n",
       "    \n",
       "<table style='width:300px; margin-bottom:10px'>\n",
       "  <tr>\n",
       "    <th>epoch</th>\n",
       "    <th>train_loss</th>\n",
       "    <th>valid_loss</th>\n",
       "    <th>accuracy</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "\n",
       "  </tr>\n",
       "</table>\n",
       "\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "        \t/* Turns off some styling */\n",
       "        \tprogress {\n",
       "\n",
       "            \t/* gets rid of default border in Firefox and Opera. */\n",
       "            \tborder: none;\n",
       "\n",
       "            \t/* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "            \tbackground-size: auto;\n",
       "            }\n",
       "\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='0', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100% [0/0]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/fastprogress/fastprogress.py:95: UserWarning: Your generator is empty.\n",
      "  warn(\"Your generator is empty.\")\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "__len__() should return >= 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-305-d7c6ddf997a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munfreeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_one_cycle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/fastai/train.py\u001b[0m in \u001b[0;36mfit_one_cycle\u001b[0;34m(learn, cyc_len, max_lr, moms, div_factor, pct_start, wd, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m     19\u001b[0m     callbacks.append(OneCycleScheduler(learn, max_lr, moms=moms, div_factor=div_factor,\n\u001b[1;32m     20\u001b[0m                                         pct_start=pct_start, **kwargs))\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcyc_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlr_find\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mLearner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_lr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_lr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_it\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_div\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, epochs, lr, wd, callbacks)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_fns\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         fit(epochs, self.model, self.loss_func, opt=self.opt, data=self.data, metrics=self.metrics,\n\u001b[0;32m--> 163\u001b[0;31m             callbacks=self.callbacks+callbacks)\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(epochs, model, loss_func, opt, data, callbacks, metrics)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mexception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(epochs, model, loss_func, opt, data, callbacks, metrics)\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'valid_dl'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid_dl\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid_ds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                 val_loss = validate(model, data.valid_dl, loss_func=loss_func,\n\u001b[0;32m---> 89\u001b[0;31m                                        cb_handler=cb_handler, pbar=pbar)\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(model, dl, loss_func, cb_handler, pbar, average, n_batch)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mval_losses\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnums\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpbar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpbar\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mval_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb_handler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/fastprogress/fastprogress.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, gen, total, display, leave, parent, auto_update)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mNBProgressBar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mProgressBar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauto_update\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogress\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhtml_progress_bar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtotal\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauto_update\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/fastai/basic_data.py\u001b[0m in \u001b[0;36m__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtfms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtfms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: __len__() should return >= 0"
     ]
    }
   ],
   "source": [
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(1, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a review about 1 1 1 1 1 1 1 1 1 1'"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.predict(\"This is a review about\", n_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save_encoder('ft_enc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [OPEN] Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... whats next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check this, what does it and useful for our kinds of text ?\n",
    "## spacy? no, do we need this here?\n",
    "from fastai.text import *\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "## adjust this to 2 sql languages\n",
    "PATH = Path('data')\n",
    "##PATH.mkdir(exist_ok=True)\n",
    "PATH = PATH/'translatesql'\n",
    "PATH.mkdir(exist_ok=True)\n",
    "TMP_PATH = PATH/'tmp'\n",
    "TMP_PATH.mkdir(exist_ok=True)\n",
    "fname='sqls'\n",
    "sqlite_fname = PATH/f'{fname}.sqlite'\n",
    "mongosql_fname = PATH/f'{fname}.mongosql'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/translatesql\n"
     ]
    }
   ],
   "source": [
    "##?Path\n",
    "print(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(\"INSERT INTO Customer VALUES ('average1','joe.average1@email.com',10000)\",\n",
       "   {'name': 'average1',\n",
       "    'emailid': 'joe.average1@email.com',\n",
       "    '_id': ObjectId('5c061836769981077971672a')}),\n",
       "  (\"INSERT INTO Customer VALUES ('Piet2','piet.yanneke2@email.nl',60000)\",\n",
       "   {'name': 'Piet2',\n",
       "    'emailid': 'piet.yanneke2@email.nl',\n",
       "    '_id': ObjectId('5c061836769981077971672b')}),\n",
       "  (\"INSERT INTO Customer VALUES ('Mustermann3','max.mustermann3@email.de',60000)\",\n",
       "   {'name': 'Mustermann3',\n",
       "    'emailid': 'max.mustermann3@email.de',\n",
       "    '_id': ObjectId('5c061836769981077971672c')}),\n",
       "  (\"INSERT INTO Customer VALUES ('average4','joe.average4@email.com',40000)\",\n",
       "   {'name': 'average4',\n",
       "    'emailid': 'joe.average4@email.com',\n",
       "    '_id': ObjectId('5c061836769981077971672d')}),\n",
       "  (\"INSERT INTO Customer VALUES ('Piet5','piet.yanneke5@email.nl',150000)\",\n",
       "   {'name': 'Piet5',\n",
       "    'emailid': 'piet.yanneke5@email.nl',\n",
       "    '_id': ObjectId('5c061836769981077971672e')})],\n",
       " 29)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## just some examples\n",
    "##qs[:5], len(qs)\n",
    "sql_tuples[:5], len(sql_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'name': 'average1',\n",
       "  'emailid': 'joe.average1@email.com',\n",
       "  '_id': ObjectId('5c061836769981077971672a')},\n",
       " {'name': 'Piet2',\n",
       "  'emailid': 'piet.yanneke2@email.nl',\n",
       "  '_id': ObjectId('5c061836769981077971672b')},\n",
       " {'name': 'Mustermann3',\n",
       "  'emailid': 'max.mustermann3@email.de',\n",
       "  '_id': ObjectId('5c061836769981077971672c')},\n",
       " {'name': 'average4',\n",
       "  'emailid': 'joe.average4@email.com',\n",
       "  '_id': ObjectId('5c061836769981077971672d')},\n",
       " {'name': 'Piet5',\n",
       "  'emailid': 'piet.yanneke5@email.nl',\n",
       "  '_id': ObjectId('5c061836769981077971672e')},\n",
       " {'name': 'Mustermann6',\n",
       "  'emailid': 'max.mustermann6@email.de',\n",
       "  '_id': ObjectId('5c061836769981077971672f')},\n",
       " {'name': 'average7',\n",
       "  'emailid': 'joe.average7@email.com',\n",
       "  '_id': ObjectId('5c0618367699810779716730')},\n",
       " {'name': 'Piet8',\n",
       "  'emailid': 'piet.yanneke8@email.nl',\n",
       "  '_id': ObjectId('5c0618367699810779716731')},\n",
       " {'name': 'Mustermann9',\n",
       "  'emailid': 'max.mustermann9@email.de',\n",
       "  '_id': ObjectId('5c0618367699810779716732')},\n",
       " {'name': 'average10',\n",
       "  'emailid': 'joe.average10@email.com',\n",
       "  '_id': ObjectId('5c0618367699810779716733')},\n",
       " {'name': 'Piet11',\n",
       "  'emailid': 'piet.yanneke11@email.nl',\n",
       "  '_id': ObjectId('5c0618367699810779716734')},\n",
       " {'name': 'Mustermann12',\n",
       "  'emailid': 'max.mustermann12@email.de',\n",
       "  '_id': ObjectId('5c0618367699810779716735')},\n",
       " {'name': 'average13',\n",
       "  'emailid': 'joe.average13@email.com',\n",
       "  '_id': ObjectId('5c0618367699810779716736')},\n",
       " {'name': 'Piet14',\n",
       "  'emailid': 'piet.yanneke14@email.nl',\n",
       "  '_id': ObjectId('5c0618367699810779716737')},\n",
       " {'name': 'Mustermann15',\n",
       "  'emailid': 'max.mustermann15@email.de',\n",
       "  '_id': ObjectId('5c0618367699810779716738')},\n",
       " {'name': 'average16',\n",
       "  'emailid': 'joe.average16@email.com',\n",
       "  '_id': ObjectId('5c0618367699810779716739')},\n",
       " {'name': 'Piet17',\n",
       "  'emailid': 'piet.yanneke17@email.nl',\n",
       "  '_id': ObjectId('5c061836769981077971673a')},\n",
       " {'name': 'Mustermann18',\n",
       "  'emailid': 'max.mustermann18@email.de',\n",
       "  '_id': ObjectId('5c061836769981077971673b')},\n",
       " {'name': 'average19',\n",
       "  'emailid': 'joe.average19@email.com',\n",
       "  '_id': ObjectId('5c061836769981077971673c')},\n",
       " {'name': 'Piet20',\n",
       "  'emailid': 'piet.yanneke20@email.nl',\n",
       "  '_id': ObjectId('5c061836769981077971673d')},\n",
       " {'name': 'Mustermann21',\n",
       "  'emailid': 'max.mustermann21@email.de',\n",
       "  '_id': ObjectId('5c061836769981077971673e')},\n",
       " {'name': 'average22',\n",
       "  'emailid': 'joe.average22@email.com',\n",
       "  '_id': ObjectId('5c061836769981077971673f')},\n",
       " {'name': 'Piet23',\n",
       "  'emailid': 'piet.yanneke23@email.nl',\n",
       "  '_id': ObjectId('5c0618367699810779716740')},\n",
       " {'name': 'Mustermann24',\n",
       "  'emailid': 'max.mustermann24@email.de',\n",
       "  '_id': ObjectId('5c0618367699810779716741')},\n",
       " {'name': 'average25',\n",
       "  'emailid': 'joe.average25@email.com',\n",
       "  '_id': ObjectId('5c0618367699810779716742')},\n",
       " {'name': 'Piet26',\n",
       "  'emailid': 'piet.yanneke26@email.nl',\n",
       "  '_id': ObjectId('5c0618367699810779716743')},\n",
       " {'name': 'Mustermann27',\n",
       "  'emailid': 'max.mustermann27@email.de',\n",
       "  '_id': ObjectId('5c0618367699810779716744')},\n",
       " {'name': 'average28',\n",
       "  'emailid': 'joe.average28@email.com',\n",
       "  '_id': ObjectId('5c0618367699810779716745')},\n",
       " {'name': 'Piet29',\n",
       "  'emailid': 'piet.yanneke29@email.nl',\n",
       "  '_id': ObjectId('5c0618367699810779716746')})"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlite_sql_tuples,mongosql_sql_tuples = zip(*sql_tuples)\n",
    "##sqlite_sql_tuples\n",
    "mongosql_sql_tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "##?Tokenizer\n",
    "##https://docs.fast.ai/text.transform.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"xxup insert xxup into xxmaj customer xxup values ( ' average1','joe.average1@email.com',1 xxrep 4 0 ) xxup insert xxup into xxmaj customer xxup values ( ' xxmaj piet2','piet.yanneke2@email.nl',6 xxrep 4 0 ) xxup insert xxup into xxmaj customer xxup values ( ' xxmaj mustermann3','max.mustermann3@email.de',6 xxrep 4 0 ) xxup insert xxup into xxmaj customer xxup values ( ' average4','joe.average4@email.com',4 xxrep 4 0 ) xxup insert xxup into xxmaj customer xxup values ( ' xxmaj piet5','piet.yanneke5@email.nl',15 xxrep 4 0 ) xxup insert xxup into xxmaj customer xxup values ( ' xxmaj mustermann6','max.mustermann6@email.de',12 xxrep 4 0 ) xxup insert xxup into xxmaj customer xxup values ( ' average7','joe.average7@email.com',7 xxrep 4 0 ) xxup insert xxup into xxmaj customer xxup values ( ' xxmaj piet8','piet.yanneke8@email.nl',24 xxrep 4 0 ) xxup insert xxup into xxmaj customer xxup values ( ' xxmaj mustermann9','max.mustermann9@email.de',18 xxrep 4 0 ) xxup insert xxup into xxmaj customer xxup values ( ' average10','joe.average10@email.com',1 xxrep 5 0 ) xxup insert xxup into xxmaj customer xxup values ( ' xxmaj piet11','piet.yanneke11@email.nl',33 xxrep 4 0 ) xxup insert xxup into xxmaj customer xxup values ( ' xxmaj mustermann12','max.mustermann12@email.de',24 xxrep 4 0 ) xxup insert xxup into xxmaj customer xxup values ( ' average13','joe.average13@email.com',13 xxrep 4 0 ) xxup insert xxup into xxmaj customer xxup values ( ' xxmaj piet14','piet.yanneke14@email.nl',42 xxrep 4 0 ) xxup insert xxup into xxmaj customer xxup values ( ' xxmaj mustermann15','max.mustermann15@email.de',3 xxrep 5 0 ) xxup insert xxup into xxmaj customer xxup values ( ' average16','joe.average16@email.com',16 xxrep 4 0 ) xxup insert xxup into xxmaj customer xxup values ( ' xxmaj piet17','piet.yanneke17@email.nl',51 xxrep 4 0 ) xxup insert xxup into xxmaj customer xxup values ( ' xxmaj mustermann18','max.mustermann18@email.de',36 xxrep 4 0 ) xxup insert xxup into xxmaj customer xxup values ( ' average19','joe.average19@email.com',19 xxrep 4 0 ) xxup insert xxup into xxmaj customer xxup values ( ' xxmaj piet20','piet.yanneke20@email.nl',6 xxrep 5 0 ) xxup insert xxup into xxmaj customer xxup values ( ' xxmaj mustermann21','max.mustermann21@email.de',42 xxrep 4 0 ) xxup insert xxup into xxmaj customer xxup values ( ' average22','joe.average22@email.com',22 xxrep 4 0 ) xxup insert xxup into xxmaj customer xxup values ( ' xxmaj piet23','piet.yanneke23@email.nl',69 xxrep 4 0 ) xxup insert xxup into xxmaj customer xxup values ( ' xxmaj mustermann24','max.mustermann24@email.de',48 xxrep 4 0 ) xxup insert xxup into xxmaj customer xxup values ( ' average25','joe.average25@email.com',25 xxrep 4 0 ) xxup insert xxup into xxmaj customer xxup values ( ' xxmaj piet26','piet.yanneke26@email.nl',78 xxrep 4 0 ) xxup insert xxup into xxmaj customer xxup values ( ' xxmaj mustermann27','max.mustermann27@email.de',54 xxrep 4 0 ) xxup insert xxup into xxmaj customer xxup values ( ' average28','joe.average28@email.com',28 xxrep 4 0 ) xxup insert xxup into xxmaj customer xxup values ( ' xxmaj piet29','piet.yanneke29@email.nl',87 xxrep 4 0 )\""
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlite_sql_text = ''.join(sqlite_sql_tuples)\n",
    "tokenizer = Tokenizer()\n",
    "tok = SpacyTokenizer('en')\n",
    "sqlite_sql_tok = tokenizer.process_text(sqlite_sql_text, tok)\n",
    "' '.join(sqlite_sql_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{ ' name ' : ' average1 ' , ' emailid ' : ' joe.average1@email.com ' , ' _ i d ' : objectid('5c061836769981077971672a')},{'name ' : ' xxmaj piet2 ' , ' emailid ' : ' piet.yanneke2@email.nl ' , ' _ i d ' : objectid('5c061836769981077971672b')},{'name ' : ' xxmaj mustermann3 ' , ' emailid ' : ' max.mustermann3@email.de ' , ' _ i d ' : objectid('5c061836769981077971672c')},{'name ' : ' average4 ' , ' emailid ' : ' joe.average4@email.com ' , ' _ i d ' : objectid('5c061836769981077971672d')},{'name ' : ' xxmaj piet5 ' , ' emailid ' : ' piet.yanneke5@email.nl ' , ' _ i d ' : objectid('5c061836769981077971672e')},{'name ' : ' xxmaj mustermann6 ' , ' emailid ' : ' max.mustermann6@email.de ' , ' _ i d ' : objectid('5c061836769981077971672f')},{'name ' : ' average7 ' , ' emailid ' : ' joe.average7@email.com ' , ' _ i d ' : objectid('5c0618367699810779716730')},{'name ' : ' xxmaj piet8 ' , ' emailid ' : ' piet.yanneke8@email.nl ' , ' _ i d ' : objectid('5c0618367699810779716731')},{'name ' : ' xxmaj mustermann9 ' , ' emailid ' : ' max.mustermann9@email.de ' , ' _ i d ' : objectid('5c0618367699810779716732')},{'name ' : ' average10 ' , ' emailid ' : ' joe.average10@email.com ' , ' _ i d ' : objectid('5c0618367699810779716733')},{'name ' : ' xxmaj piet11 ' , ' emailid ' : ' piet.yanneke11@email.nl ' , ' _ i d ' : objectid('5c0618367699810779716734')},{'name ' : ' xxmaj mustermann12 ' , ' emailid ' : ' max.mustermann12@email.de ' , ' _ i d ' : objectid('5c0618367699810779716735')},{'name ' : ' average13 ' , ' emailid ' : ' joe.average13@email.com ' , ' _ i d ' : objectid('5c0618367699810779716736')},{'name ' : ' xxmaj piet14 ' , ' emailid ' : ' piet.yanneke14@email.nl ' , ' _ i d ' : objectid('5c0618367699810779716737')},{'name ' : ' xxmaj mustermann15 ' , ' emailid ' : ' max.mustermann15@email.de ' , ' _ i d ' : objectid('5c0618367699810779716738')},{'name ' : ' average16 ' , ' emailid ' : ' joe.average16@email.com ' , ' _ i d ' : objectid('5c0618367699810779716739')},{'name ' : ' xxmaj piet17 ' , ' emailid ' : ' piet.yanneke17@email.nl ' , ' _ i d ' : objectid('5c061836769981077971673a')},{'name ' : ' xxmaj mustermann18 ' , ' emailid ' : ' max.mustermann18@email.de ' , ' _ i d ' : objectid('5c061836769981077971673b')},{'name ' : ' average19 ' , ' emailid ' : ' joe.average19@email.com ' , ' _ i d ' : objectid('5c061836769981077971673c')},{'name ' : ' xxmaj piet20 ' , ' emailid ' : ' piet.yanneke20@email.nl ' , ' _ i d ' : objectid('5c061836769981077971673d')},{'name ' : ' xxmaj mustermann21 ' , ' emailid ' : ' max.mustermann21@email.de ' , ' _ i d ' : objectid('5c061836769981077971673e')},{'name ' : ' average22 ' , ' emailid ' : ' joe.average22@email.com ' , ' _ i d ' : objectid('5c061836769981077971673f')},{'name ' : ' xxmaj piet23 ' , ' emailid ' : ' piet.yanneke23@email.nl ' , ' _ i d ' : objectid('5c0618367699810779716740')},{'name ' : ' xxmaj mustermann24 ' , ' emailid ' : ' max.mustermann24@email.de ' , ' _ i d ' : objectid('5c0618367699810779716741')},{'name ' : ' average25 ' , ' emailid ' : ' joe.average25@email.com ' , ' _ i d ' : objectid('5c0618367699810779716742')},{'name ' : ' xxmaj piet26 ' , ' emailid ' : ' piet.yanneke26@email.nl ' , ' _ i d ' : objectid('5c0618367699810779716743')},{'name ' : ' xxmaj mustermann27 ' , ' emailid ' : ' max.mustermann27@email.de ' , ' _ i d ' : objectid('5c0618367699810779716744')},{'name ' : ' average28 ' , ' emailid ' : ' joe.average28@email.com ' , ' _ i d ' : objectid('5c0618367699810779716745')},{'name ' : ' xxmaj piet29 ' , ' emailid ' : ' piet.yanneke29@email.nl ' , ' _ i d ' : objectid('5c0618367699810779716746 ' ) }\""
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mongosql_sql_text = ','.join(str(v) for v in mongosql_sql_tuples)\n",
    "tokenizer = Tokenizer()\n",
    "mongosql_sql_tok = tokenizer.process_text(mongosql_sql_text, tok)\n",
    "tok = SpacyTokenizer('en')\n",
    "' '.join(mongosql_sql_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('xxup', '{')"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlite_sql_tok[0], mongosql_sql_tok[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6.0, 5.0)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "np.percentile([len(o) for o in sqlite_sql_tok], 90), np.percentile([len(o) for o in mongosql_sql_tok], 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_sqlite_sql_tok = np.array([len(o)<10 for o in sqlite_sql_tok])\n",
    "keep_mongosql_sql_tok = np.array([len(o)<10 for o in mongosql_sql_tok])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlite_sql_tok = np.array(sqlite_sql_tok)[keep_sqlite_sql_tok]\n",
    "mongosql_sql_tok = np.array(mongosql_sql_tok)[keep_mongosql_sql_tok]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(sqlite_sql_tok, (PATH/'sqlite_sql_tok.pkl').open('wb'))\n",
    "pickle.dump(mongosql_sql_tok, (PATH/'mongosql_sql_tok.pkl').open('wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlite_sql_tok = pickle.load((PATH/'sqlite_sql_tok.pkl').open('rb'))\n",
    "mongosql_sql_tok = pickle.load((PATH/'mongosql_sql_tok.pkl').open('rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- this might need to change, we might need to dictionaries to identify variables that are immutable\n",
    "- Also, we always have the same list of keywords, therefore we might do all this differnt (how)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "## can we use this instead: https://docs.fast.ai/text.transform.html\n",
    "import collections\n",
    "def toks2ids(tok,pre):\n",
    "    freq = collections.Counter(p for o in tok for p in o)\n",
    "    itos = [o for o,c in freq.most_common(40000)]\n",
    "    itos.insert(0, '_bos_')\n",
    "    itos.insert(1, '_pad_')\n",
    "    itos.insert(2, '_eos_')\n",
    "    itos.insert(3, '_unk')\n",
    "    stoi = collections.defaultdict(lambda: 3, {v:k for k,v in enumerate(itos)})\n",
    "    ids = np.array([([stoi[o] for o in p] + [2]) for p in tok])\n",
    "    np.save(TMP_PATH/f'{pre}_ids.npy', ids)\n",
    "    pickle.dump(itos, open(TMP_PATH/f'{pre}_itos.pkl', 'wb'))\n",
    "    return ids,itos,stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "## mapping from tokens to IDs\n",
    "sqlite_sql_ids,sqlite_sql_itos,sqlite_sql_stoi = toks2ids(sqlite_sql_tok,'sqlite')\n",
    "mongosql_sql_ids,mongosql_sql_itos,mongosql_sql_stoi = toks2ids(mongosql_sql_tok,'mongosql')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ids(pre):\n",
    "    ids = np.load(TMP_PATH/f'{pre}_ids.npy')\n",
    "    itos = pickle.load(open(TMP_PATH/f'{pre}_itos.pkl', 'rb'))\n",
    "    stoi = collections.defaultdict(lambda: 3, {v:k for k,v in enumerate(itos)})\n",
    "    return ids,itos,stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlite_sql_ids,sqlite_sql_itos,sqlite_sql_stoi = load_ids('sqlite')\n",
    "mongosql_sql_ids,mongosql_sql_itos,mongosql_sql_stoi = load_ids('mongosql')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['{', '_eos_'], 26, 35)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## just a test here\n",
    "[mongosql_sql_itos[o] for o in mongosql_sql_ids[0]], len(sqlite_sql_itos), len(mongosql_sql_itos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [TO DO] Load Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "##import fastai.text as ft\n",
    "from fastai import *\n",
    "from fastai.text import * "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the 2 wordvectors trained above:\n",
    "\n",
    "- sqlite_all.bin\n",
    "- mongosql_all.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-253-5788b8f1b79d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## en_vecs = ft.load_model(str((PATH/'wiki.en.bin')))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m## sqlite_vecs = word2vec.load('sqlite_all.bin')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msqlite_vecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlite_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'load_model' is not defined"
     ]
    }
   ],
   "source": [
    "## en_vecs = ft.load_model(str((PATH/'wiki.en.bin')))\n",
    "## sqlite_vecs = word2vec.load('sqlite_all.bin')\n",
    "sqlite_vecs = load_model(str((sqlite_all.bin)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "##fr_vecs = ft.load_model(str((PATH/'wiki.fr.bin')))\n",
    "mongo_vecs = word2vec.load('mongosql_all.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vecs(lang, ft_vecs):\n",
    "    vecd = {w:ft_vecs.get_word_vector(w) for w in ft_vecs.get_words()}\n",
    "    pickle.dump(vecd, open(PATH/f'wiki.{lang}.pkl','wb'))\n",
    "    return vecd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'WordVectors' object has no attribute 'get_words'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-234-7b99ba354d5e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msqlite_vecd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_vecs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sqlite'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msqlite_vecs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmongo_vecd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_vecs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mongo'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmongo_vecs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-233-6f59a973661a>\u001b[0m in \u001b[0;36mget_vecs\u001b[0;34m(lang, ft_vecs)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_vecs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mft_vecs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mvecd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mft_vecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_word_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mft_vecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvecd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34mf'wiki.{lang}.pkl'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvecd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'WordVectors' object has no attribute 'get_words'"
     ]
    }
   ],
   "source": [
    "sqlite_vecd = get_vecs('sqlite', sqlite_vecs)\n",
    "mongo_vecd = get_vecs('mongo', mongo_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_vecd = pickle.load(open(PATH/'wiki.en.pkl','rb'))\n",
    "fr_vecd = pickle.load(open(PATH/'wiki.fr.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_words = en_vecs.get_words(include_freq=True)\n",
    "ft_word_dict = {k:v for k,v in zip(*ft_words)}\n",
    "ft_words = sorted(ft_word_dict.keys(), key=lambda x: ft_word_dict[x])\n",
    "\n",
    "len(ft_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_en_vec = len(en_vecd[','])\n",
    "dim_fr_vec = len(fr_vecd[','])\n",
    "dim_en_vec,dim_fr_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_vecs = np.stack(list(en_vecd.values()))\n",
    "## mean and standard deviation\n",
    "en_vecs.mean(),en_vecs.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [TO DO] Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## purely for processing speed up, we make sure the length of the longest sequence gets truncated\n",
    "enlen_90 = int(np.percentile([len(o) for o in en_ids], 99))\n",
    "frlen_90 = int(np.percentile([len(o) for o in fr_ids], 97))\n",
    "enlen_90,frlen_90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_ids_tr = np.array([o[:enlen_90] for o in en_ids])\n",
    "fr_ids_tr = np.array([o[:frlen_90] for o in fr_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## training set\n",
    "## length & index for pytorch\n",
    "## convention: v variables, t tensors, a arrays\n",
    "class Seq2SeqDataset(Dataset):\n",
    "    def __init__(self, x, y): self.x,self.y = x,y\n",
    "    ## anything that is not yet a numpy array gets turned into it    \n",
    "    def __getitem__(self, idx): return A(self.x[idx], self.y[idx]) \n",
    "    def __len__(self): return len(self.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## easy way to get training & validation set\n",
    "np.random.seed(42)\n",
    "trn_keep = np.random.rand(len(en_ids_tr))>0.1 ## randomlist of bools to index into the set\n",
    "en_trn,fr_trn = en_ids_tr[trn_keep],fr_ids_tr[trn_keep]\n",
    "en_val,fr_val = en_ids_tr[~trn_keep],fr_ids_tr[~trn_keep]\n",
    "len(en_trn),len(en_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for english to french, just switch around. This is the training & validation set here\n",
    "trn_ds = Seq2SeqDataset(fr_trn,en_trn)\n",
    "val_ds = Seq2SeqDataset(fr_val,en_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  validation set: sort by length, training set: randomize the order of things, so similar things about similar spot\n",
    "trn_samp = SortishSampler(en_trn, key=lambda x: len(en_trn[x]), bs=bs)\n",
    "val_samp = SortSampler(en_val, key=lambda x: len(en_val[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## minute 45\n",
    "## why do we need to transpose the oriantation?\n",
    "## we did pre-work, no augmentation (?)\n",
    "## padding index\n",
    "## for classifier padding at the start, here pre-padding = false for encider\n",
    "trn_dl = DataLoader(trn_ds, bs, transpose=True, transpose_y=True, num_workers=1, \n",
    "                    pad_idx=1, pre_pad=False, sampler=trn_samp) ## uses fast.ai behind the scenes\n",
    "val_dl = DataLoader(val_ds, int(bs*1.6), transpose=True, transpose_y=True, num_workers=1, \n",
    "                    pad_idx=1, pre_pad=False, sampler=val_samp)## uses fast.ai behind the scenes\n",
    "\n",
    "## says: I ahve a training & validation set (optional test set): into one object with a path to store temporary stuff\n",
    "md = ModelData(PATH, trn_dl, val_dl)\n",
    "## after that you can create a learner and call fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(trn_dl)\n",
    "its = [next(it) for i in range(5)]\n",
    "[(len(x),len(y)) for x,y in its]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO [FAST.AI part] ARCHITECTURE & LOSS FUNCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Takes sequence of Tokens\n",
    "- RNN: inject into an encoder (backbone) to turn this into a representation\n",
    "- This RNN outputs the final hidden state, a vector per sentence\n",
    "- place this output into a Decoder RNN: this decoder can go through one word by word\n",
    "- Continue until 'it thinks' sentence is finished and give that back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## number of rows = vocabulary size, each word has a vector\n",
    "## how big? fast text says size 300 !\n",
    "def create_emb(vecs, itos, em_sz):\n",
    "    ## random embeddings: if we find it in fast test we replace with that finding\n",
    "    emb = nn.Embedding(len(itos), em_sz, padding_idx=1)\n",
    "    wgts = emb.weight.data ## pytorch weight attribute is a variable. Vars have data attributes, that is a tensor\n",
    "    miss = []\n",
    "    ## with weight tensor we can now go through our vocabulary\n",
    "    for i,w in enumerate(itos):\n",
    "        ## Hacky: the 3 is about aligning standard deviations\n",
    "        try: wgts[i] = torch.from_numpy(vecs[w]*3) ## replace random weights with pre-trained vectors. \n",
    "        except: miss.append(w) ## what is not isn fast text we keep track of it\n",
    "    print(len(miss),miss[5:10])\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Seq2SeqRNN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-245-6d55e7395cb4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdoc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSeq2SeqRNN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'Seq2SeqRNN' is not defined"
     ]
    }
   ],
   "source": [
    "Seq2SeqRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check enc vs dec here for encoder and decoder. emb embedding\n",
    "## out is output\n",
    "## GRU is similar to LSTM\n",
    "\n",
    "# Remember, we pass in an index. \n",
    "class Seq2SeqRNN(nn.Module):\n",
    "    def __init__(self, vecs_enc, itos_enc, em_sz_enc, vecs_dec, itos_dec, em_sz_dec, nh, out_sl, nl=2):\n",
    "        super().__init__()\n",
    "        self.nl,self.nh,self.out_sl = nl,nh,out_sl\n",
    "        ## create encode embedding\n",
    "        self.emb_enc = create_emb(vecs_enc, itos_enc, em_sz_enc)\n",
    "        ## add dropout\n",
    "        self.emb_enc_drop = nn.Dropout(0.15)\n",
    "        ## create the RNN: em_sz_enc = size of embedding, nh = our choice (56 for now), \n",
    "        ## num_layers: how many layers do we want, some dropout inside the RNN\n",
    "        self.gru_enc = nn.GRU(em_sz_enc, nh, num_layers=nl, dropout=0.25) ## standard pytorch, you could use LSTM too\n",
    "        ## some output to fit the decoder, so lets use a linear layer\n",
    "        self.out_enc = nn.Linear(nh, em_sz_dec, bias=False) ## nh: number of hidden into the decoder embedding size\n",
    "        \n",
    "        self.emb_dec = create_emb(vecs_dec, itos_dec, em_sz_dec)\n",
    "        self.gru_dec = nn.GRU(em_sz_dec, em_sz_dec, num_layers=nl, dropout=0.1) ## or take LSTM\n",
    "        self.out_drop = nn.Dropout(0.35)\n",
    "        self.out = nn.Linear(em_sz_dec, len(itos_dec))\n",
    "        self.out.weight.data = self.emb_dec.weight.data\n",
    "    ## forward pass\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        #####################\n",
    "        ## the most simple RNN: takes our inout and spits out a hidden vector that hopefull \n",
    "        ## will learn to contain all of the\n",
    "        ## about what the sentence says and how it says it\n",
    "        ## Only then we can hope it gives us a translation\n",
    "        sl,bs = inp.size()\n",
    "        ## initialise our hidden state to some zeros, vector of 0\n",
    "        h = self.initHidden(bs)\n",
    "        ## inout through embedding, dropout\n",
    "        emb = self.emb_enc_drop(self.emb_enc(inp))\n",
    "        ## pass 0 hidden state into our RMM\n",
    "        ## gives back final hidden state\n",
    "        ## AN RNN spits out 2 things: enc_out list of state every time step / state last timestep\n",
    "        enc_out, h = self.gru_enc(emb, h)\n",
    "        ## pass through linear layer\n",
    "        ## here we use state of the last timestep to create the input ( a vector) state for our decoder\n",
    "        h = self.out_enc(h)\n",
    "        ####################\n",
    "        \n",
    "        ##### ADDITIONAL to simple RNN\n",
    "        \n",
    "        ## dec_inp: represents the previous word that we translated\n",
    "        ## so tell me word 4, then you need the word 4 in a sentence. \n",
    "        ## therefore we feed that in into each time step\n",
    "        ## see function toks2ids, above\n",
    "            ## itos.insert(0, '_bos_') : this is the 1st token, beginning of stream\n",
    "            ## itos.insert(1, '_pad_')\n",
    "            ## itos.insert(2, '_eos_')\n",
    "            ## itos.insert(3, '_unk')\n",
    "        \n",
    "        dec_inp = V(torch.zeros(bs).long()) ## _bos_ for 1st run\n",
    "        res = []\n",
    "        ## for loop does the same as 4 steps inside pytorch, see above\n",
    "        for i in range(self.out_sl): ## output sequence length (see constructor) \n",
    "            ##= length of largest english sentence, because we translate into english (for this corpus)\n",
    "            \n",
    "            ## Normally: RNN gru_dec works in a whole sequence at a time, but we have a for loop\n",
    "            ## add leading unit access to the start unsqueeze. \n",
    "            ##So, we do not use the RNN really and could rewrite the thing with a linear layer\n",
    "            ## take input dec_inp and fit in the embedding emb_dec unsqueeze says: \n",
    "            ##treat this as a eequence of length 1\n",
    "            \n",
    "            ## 1. put through embedding\n",
    "            ## 1st run: what is the vector for beginning of stream token is\n",
    "            emb = self.emb_dec(dec_inp).unsqueeze(0) \n",
    "            \n",
    "            ## 2. put through RNN\n",
    "            ## 1st run: h is whatever came out of encoder. This figures out what the 1st word ir\n",
    "            outp, h = self.gru_dec(emb, h) \n",
    "            \n",
    "            ## 3. put through dropout 4. put through linear layer\n",
    "            ## 1st run 3: dropout\n",
    "            ## 1st run 4: linear layer in order to convert that into correct size for our decoder embedding matrix\n",
    "            outp = self.out(self.out_drop(outp[0]))\n",
    "            ## append that output to a list\n",
    "            \n",
    "            ## 5. Addpend to list of translated words\n",
    "            ## outp: is a tensor whose length is equual to the no. words in english vocabulary an\\\n",
    "            ## contains the probability that that word is the word\n",
    "            ## pdb.settrace\n",
    "            res.append(outp) \n",
    "            ## stack up list to a tensor and return it\n",
    "            \n",
    "            ## 6. takes the highes probability: check tensor for highes probability and give that index\n",
    "            \n",
    "            dec_inp = V(outp.data.max(1)[1]) ##1 is word index of largest things\n",
    "            ## dec_inp 1  is padding, so we are finished. Or largest sentence length\n",
    "            if (dec_inp==1).all(): break\n",
    "        ## we stack up these vector probabilities into a tensor, so we can feed this to a loss function\n",
    "        return torch.stack(res)\n",
    "    \n",
    "    def initHidden(self, bs): return V(torch.zeros(self.nl, bs, self.nh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc(Seq2SeqRNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function: categorical cross entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss Function is categorical cross entropy loss:\n",
    "- list of probabilities for each of our classes (class all words in our english vocab)\n",
    "- target: correct class, correct word at this location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## tweak no.2\n",
    "def seq2seq_loss(input, target):\n",
    "    sl,bs = target.size()\n",
    "    sl_in,bs_in,nc = input.size()\n",
    "    ## tweak no.1 : we might have stopped early , so sequence length could be smaller than target.\n",
    "    ##    so we add padding\n",
    "    ##  pytroch padding: \n",
    "    ##      rank 3 tensor (sequence length x batch size x no. words of vocab)\n",
    "    ##      6 tuple required: each pair padding before padding after 1:08:30\n",
    "    ## 1st dim. & 2nd dim no padding 3. dim no padding left as much as required on the right\n",
    "    if sl>sl_in: input = F.pad(input, (0,0,0,0,0,sl-sl_in))\n",
    "    input = input[:sl]\n",
    "    ## tweak 2cross entropy loss expects rank 2 tensor, we have 3 , so flatten out: -1 in view\n",
    "    return F.cross_entropy(input.view(-1,nc), target.view(-1))#, ignore_index=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_fn = partial(optim.Adam, betas=(0.8, 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## this gives some stuff that misses in that word vector (not relevant for us, just: these would be variables)\n",
    "## standard pytorch\n",
    "rnn = Seq2SeqRNN(fr_vecd, fr_itos, dim_fr_vec, en_vecd, en_itos, dim_en_vec, nh, enlen_90)\n",
    "## put to the GPU\n",
    "## SingleModel turns pytorch model into fast.ai Model min 1:09:40:\n",
    "## how to handle learnng rate groups (fast.ai concept)\n",
    "## we call RNN_Learner (not just Learner): RNN_Learner has cross entropy as default criteria\n",
    "## check save & load_encoder --> not really required\n",
    "learn = RNN_Learner(md, SingleModel(to_gpu(rnn)), opt_fn=opt_fn)\n",
    "## now we give our learner that loss function\n",
    "learn.crit = seq2seq_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()\n",
    "learn.sched.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=3e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(lr, 1, cycle_len=12, use_clr=(20,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('initial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('initial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Initial Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CONTINUE 1:11:10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(iter(val_dl)) ## we grab x from the validation set to pass in below / use alternatively learn. predict_array(?)\n",
    "probs = learn.model(V(x)) ## standard pytorch model, so we can pass in some X\n",
    "preds = to_np(probs.max(2)[1]) ## grab index of highes probability word, so we turn the number into a word\n",
    "\n",
    "## this is just about going through some examples\n",
    "for i in range(180,190):\n",
    "    print(' '.join([fr_itos[o] for o in x[:,i] if o != 1])) ## french\n",
    "    print(' '.join([en_itos[o] for o in y[:,i] if o != 1])) ## correct english\n",
    "    print(' '.join([en_itos[o] for o in preds[:,i] if o!=1])) ## predicted english\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One easy way to bi-directional for classification: take all your tokens, flip order around (?), train new language model, train new classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Seq2SeqRNN_Bidir(nn.Module):\n",
    "    def __init__(self, vecs_enc, itos_enc, em_sz_enc, vecs_dec, itos_dec, em_sz_dec, nh, out_sl, nl=2):\n",
    "        super().__init__()\n",
    "        self.emb_enc = create_emb(vecs_enc, itos_enc, em_sz_enc)\n",
    "        self.nl,self.nh,self.out_sl = nl,nh,out_sl\n",
    "        ## bidirectional=True: next to one encoder left to right, we have a 2nd one right to left\n",
    "        ## hidden state is at the beginning of the sentence for this 2nd RNN\n",
    "        ## tensor with an extra 2 long axis (hidden state in both the same)\n",
    "        ## example: 2 layers with bi-directional --> tensor on length 4\n",
    "        self.gru_enc = nn.GRU(em_sz_enc, nh, num_layers=nl, dropout=0.25, bidirectional=True) ## encoder\n",
    "        ## *2 here because we have a 2nd hidden state with 2nd RNN\n",
    "        self.out_enc = nn.Linear(nh*2, em_sz_dec, bias=False) \n",
    "        self.drop_enc = nn.Dropout(0.05)\n",
    "        self.emb_dec = create_emb(vecs_dec, itos_dec, em_sz_dec)\n",
    "        self.gru_dec = nn.GRU(em_sz_dec, em_sz_dec, num_layers=nl, dropout=0.1)\n",
    "        self.emb_enc_drop = nn.Dropout(0.15)\n",
    "        self.out_drop = nn.Dropout(0.35)\n",
    "        self.out = nn.Linear(em_sz_dec, len(itos_dec))\n",
    "        self.out.weight.data = self.emb_dec.weight.data\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        sl,bs = inp.size()\n",
    "        h = self.initHidden(bs)\n",
    "        emb = self.emb_enc_drop(self.emb_enc(inp))\n",
    "        enc_out, h = self.gru_enc(emb, h)\n",
    "        h = h.view(2,2,bs,-1).permute(0,2,1,3).contiguous().view(2,bs,-1)\n",
    "        h = self.out_enc(self.drop_enc(h))\n",
    "\n",
    "        dec_inp = V(torch.zeros(bs).long())\n",
    "        res = []\n",
    "## Often b-directional for the decoder is not done\n",
    "  ## ??? Why ?1. Considered cheating see 1:20:30 How to turn two separate loops into a final result?\n",
    "  ## ??? range? start training is random, then we would go forever \n",
    "        for i in range(self.out_sl):\n",
    "            emb = self.emb_dec(dec_inp).unsqueeze(0)\n",
    "            outp, h = self.gru_dec(emb, h)\n",
    "            outp = self.out(self.out_drop(outp[0]))\n",
    "            res.append(outp)\n",
    "            dec_inp = V(outp.data.max(1)[1])\n",
    "            ## when we start training everything is random, so this probably is never true\n",
    "            ## a model knows nothing when we start training it\n",
    "            if (dec_inp==1).all(): break\n",
    "        return torch.stack(res)\n",
    "     ## *2 here because we have a 2nd hidden state with 2nd RNN\n",
    "    def initHidden(self, bs): return V(torch.zeros(self.nl*2, bs, self.nh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = Seq2SeqRNN_Bidir(fr_vecd, fr_itos, dim_fr_vec, en_vecd, en_itos, dim_en_vec, nh, enlen_90)\n",
    "learn = RNN_Learner(md, SingleModel(to_gpu(rnn)), opt_fn=opt_fn)\n",
    "learn.crit = seq2seq_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bi Directional improves the result !\n",
    "learn.fit(lr, 1, cycle_len=12, use_clr=(20,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note for bi - directional models one has to consider closely, which layer is going right to left if one has many layers like e.g. google translate. Otherwise one might get performance issues.\n",
    "Generally and for shallow models bi-directional is a good win and does not cause too many performance bottlenecks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('bidir')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teacher Forcing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the model starts learning, it knows 'nothing about nothing'. So, e.g. the 1st translated word in a sentence is more or less random in the beginning. One might say, in the beginning the neural net feeds in an input that is 'stupid' into a model that 'knows nothing' and somehow it is supposed to get better.\n",
    "\n",
    "What if we could 'somehow' inject the 1st 'right' word? self.pr_force.\n",
    "\n",
    "At the start of training we set pr_force very high so that nearly always it gets the actual correct previous (?) word, so it has a useful input.\n",
    "As we train longer, we decrease pr_force so that at the end pr_force is 0 and the neyral net has to learn properly. Now that is fine, because it feeds in sensible input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow or keras (static graph) vs pytorch:\n",
    "- see below line dec_inp = y[i]\n",
    "- key reason to switch fast.ai to pytorch was\n",
    "\n",
    "new tensorflow: https://www.youtube.com/watch?feature=youtu.be&utm_campaign=NLP+News&utm_medium=email&utm_source=Revue+newsletter&v=WTNH0tcscqo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is hook called the stepper, see minute 1:27:00\n",
    "## Idea of the hook is you can reuse and do not have to write from the scratch the training loop\n",
    "## check model.py:\n",
    "    ## fit function: lowest level without learner, just standard pytorch model, model data object\n",
    "    ##               epochs, standard pytorch optimiser, standard pytorch loss function\n",
    "    ## this low level function gets called by upper layers and hence used by us\n",
    "    ## it calls a function called stepper.step\n",
    "    ## stepper.step uses a class called Stepper:\n",
    "        ## - calls the model / zero gradients / loss function / calls backward / clipping if necessary /call optimiser\n",
    "        ## - \n",
    "\n",
    "## Inherit from Stepper and write own version of step (so 1t copy & paste / call. super.step and then add or change )\n",
    "\n",
    "class Seq2SeqStepper(Stepper): ## we inherit from Stepper here\n",
    "    def step(self, xs, y, epoch):\n",
    "## this is the new line\n",
    "        ## replace or_force in the model with sth that gradually decreases linearly with every epoch, then drops to 0\n",
    "        self.m.pr_force = (10-epoch)*0.1 if epoch<10 else 0\n",
    "## instead of below: super().step(xs,y,epoch) \n",
    "        xtra = []\n",
    "        output = self.m(*xs, y)\n",
    "        if isinstance(output,tuple): output,*xtra = output\n",
    "        self.opt.zero_grad()\n",
    "        loss = raw_loss = self.crit(output, y)\n",
    "        if self.reg_fn: loss = self.reg_fn(output, xtra, raw_loss)\n",
    "        loss.backward()\n",
    "        if self.clip:   # Gradient clipping\n",
    "            nn.utils.clip_grad_norm(trainable_params_(self.m), self.clip)\n",
    "        self.opt.step()\n",
    "        return raw_loss.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqRNN_TeacherForcing(nn.Module):\n",
    "    def __init__(self, vecs_enc, itos_enc, em_sz_enc, vecs_dec, itos_dec, em_sz_dec, nh, out_sl, nl=2):\n",
    "        super().__init__()\n",
    "        self.emb_enc = create_emb(vecs_enc, itos_enc, em_sz_enc)\n",
    "        self.nl,self.nh,self.out_sl = nl,nh,out_sl\n",
    "        self.gru_enc = nn.GRU(em_sz_enc, nh, num_layers=nl, dropout=0.25)\n",
    "        self.out_enc = nn.Linear(nh, em_sz_dec, bias=False)\n",
    "        self.emb_dec = create_emb(vecs_dec, itos_dec, em_sz_dec)\n",
    "        self.gru_dec = nn.GRU(em_sz_dec, em_sz_dec, num_layers=nl, dropout=0.1)\n",
    "        self.emb_enc_drop = nn.Dropout(0.15)\n",
    "        self.out_drop = nn.Dropout(0.35)\n",
    "        self.out = nn.Linear(em_sz_dec, len(itos_dec))\n",
    "        self.out.weight.data = self.emb_dec.weight.data\n",
    "        self.pr_force = 1.\n",
    "        \n",
    "    def forward(self, inp, y=None):\n",
    "        sl,bs = inp.size()\n",
    "        h = self.initHidden(bs)\n",
    "        emb = self.emb_enc_drop(self.emb_enc(inp))\n",
    "        enc_out, h = self.gru_enc(emb, h)\n",
    "        h = self.out_enc(h)\n",
    "\n",
    "        dec_inp = V(torch.zeros(bs).long())\n",
    "        res = []\n",
    "        for i in range(self.out_sl):\n",
    "            emb = self.emb_dec(dec_inp).unsqueeze(0)\n",
    "            outp, h = self.gru_dec(emb, h)\n",
    "            outp = self.out(self.out_drop(outp[0]))\n",
    "            res.append(outp)\n",
    "            dec_inp = V(outp.data.max(1)[1])\n",
    "            if (dec_inp==1).all(): break\n",
    "            ##  probability forcing --> see stepepr overwrite above\n",
    "            if (y is not None) and (random.random()<self.pr_force):\n",
    "                ## if some random probability is less than self.pr_force\n",
    "                ## then I replace my decode input with the actual correct thing\n",
    "                if i>=len(y): break ## if we already went too far (longer than target sentence) just stop\n",
    "                ##  pytorch allows\n",
    "                ## static graph like tensorflow does not allow that\n",
    "                dec_inp = y[i]\n",
    "        return torch.stack(res)\n",
    "    \n",
    "    def initHidden(self, bs): return V(torch.zeros(self.nl, bs, self.nh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = Seq2SeqRNN_TeacherForcing(fr_vecd, fr_itos, dim_fr_vec, en_vecd, en_itos, dim_en_vec, nh, enlen_90)\n",
    "learn = RNN_Learner(md, SingleModel(to_gpu(rnn)), opt_fn=opt_fn)\n",
    "learn.crit = seq2seq_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## gives better result than above\n",
    "learn.fit(lr, 1, cycle_len=12, use_clr=(20,10), stepper=Seq2SeqStepper) ## we pass in above stepper overwrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('forcing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attentional Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expectation to summarise one source sentence entirely in one hidden state, such that everything necessary to create a translated sentence is present, is 'asking a lot'.\n",
    "\n",
    "This might be asking too much, so instead of having one hidden state at the end of the sentence, why not have a hidden state after every word? With a bi - directional RNN we would even have two vectors, that we can use.\n",
    "\n",
    "In the example 'He loved pizza', which would need to get translated to 'Er liebte Pizza', to translate 'liebte' we probably want more attention towards the hidden state of 'loved' and less attention to e.g. 'Pizza'.\n",
    "\n",
    "[to do] take 2 sql examples\n",
    "\n",
    "But how do we know, where to place more attention? We will do this via creating a new neural net, that spits out a weight for every index / word of the source sentence.\n",
    "\n",
    "https://distill.pub/2016/augmented-rnns/\n",
    "\n",
    "Note Attention was also used for getting text out of scripts.\n",
    "\n",
    "'Grammar as a foreign language': replace rules based grammar with neural net https://arxiv.org/abs/1412.7449"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_t(*sz): return torch.randn(sz)/math.sqrt(sz[0])\n",
    "## Parameter is like a variable but tells pytorch: I want you to learn the weights for this\n",
    "def rand_p(*sz): return nn.Parameter(rand_t(*sz)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we use teacher forcing, but not bi-directional\n",
    "\n",
    "class Seq2SeqAttnRNN(nn.Module):\n",
    "\n",
    "    ## Until below this, all is identical with the above    \n",
    "    def __init__(self, vecs_enc, itos_enc, em_sz_enc, vecs_dec, itos_dec, em_sz_dec, nh, out_sl, nl=2):\n",
    "        super().__init__()\n",
    "        self.emb_enc = create_emb(vecs_enc, itos_enc, em_sz_enc)\n",
    "        self.nl,self.nh,self.out_sl = nl,nh,out_sl\n",
    "        self.gru_enc = nn.GRU(em_sz_enc, nh, num_layers=nl, dropout=0.25)\n",
    "        self.out_enc = nn.Linear(nh, em_sz_dec, bias=False)\n",
    "        self.emb_dec = create_emb(vecs_dec, itos_dec, em_sz_dec)\n",
    "        self.gru_decgru_dec = nn.GRU(em_sz_dec, em_sz_dec, num_layers=nl, dropout=0.1)\n",
    "        self.emb_enc_drop = nn.Dropout(0.15)\n",
    "        self.out_drop = nn.Dropout(0.35)\n",
    "        self.out = nn.Linear(em_sz_dec, len(itos_dec))\n",
    "        self.out.weight.data = self.emb_dec.weight.data\n",
    "## Until above this, all is identical with the above\n",
    " \n",
    "## An RNN spits out 2 things: enc_out list of state every time step / state last timestep\n",
    "## efore attention, we use state of the last timestep to create the inout state for our decoder\n",
    "## Rather: we would want to use the ones that are most relevant to the ones we are using now\n",
    "## That is it would like to take a weighted average of each time step weighted by whatever would be ‘appropriate’ now\n",
    "## we figure out what is appropriate by training a small neural net\n",
    "## Hence, include in model a minimal possible neural net, which is having 2 layers and 1 non linear activation layer\n",
    "\n",
    "## Note, there is no specific loss function for this inner  neural net. It gets judged \n",
    "## over the SAME MAIN loss function, which is assessing if translations were correct.\n",
    "## So how does this neural net learn? In order to make translation outputs better, \n",
    "## it needs to make weights for the attention better.\n",
    "## In the end, this works via the chain rule and backprop: if you put a little more or less weight here, \n",
    "## overall result will improve\n",
    "    \n",
    "        self.W1 = rand_p(nh, em_sz_dec) ## random matrix, see above definition torch.randn(sz)/math.sqrt(sz[0])\n",
    "        self.l2 = nn.Linear(em_sz_dec, em_sz_dec) ## 1st linear layer\n",
    "        self.l3 = nn.Linear(em_sz_dec+nh, em_sz_dec) ## 2nd linear layer\n",
    "        self.V = rand_p(em_sz_dec)\n",
    "\n",
    "## Until below this (the encoder), all is identical with the above\n",
    "## ret_attn: take note of attentions\n",
    "        def forward(self, inp, y=None, ret_attn=False):\n",
    "        sl,bs = inp.size()\n",
    "        h = self.initHidden(bs)\n",
    "        emb = self.emb_enc_drop(self.emb_enc(inp))\n",
    "        ## enc_out not just last one but whole tensor with all outputs\n",
    "        enc_out, h = self.gru_enc(emb, h)\n",
    "        h = self.out_enc(h)\n",
    "## Until above this (the encoder), all is identical with the above\n",
    "\n",
    "        dec_inp = V(torch.zeros(bs).long())\n",
    "        res,attns = [],[]\n",
    "        w1e = enc_out @ self.W1\n",
    "        for i in range(self.out_sl):\n",
    "## Difference: we create a little neural net with one hidden layer to get a weighted average\n",
    "## we tell this little neural net: do not just take the final state h but take all enc_out\n",
    "            ## we take the last layers hidden state h[-1] and stick that into linear layer l2\n",
    "            ## how do we decide, which words to focus on for translating? We have that information in the hidden state\n",
    "            w2h = self.l2(h[-1])\n",
    "            ## place the result into a non linear activation\n",
    "            ## could we use ReLu? Worth a try\n",
    "            u = F.tanh(w1e + w2h)\n",
    "            ## get the results of that neural net. Softmax ensure all weights we use add up to one\n",
    "            ## we 'hope' one weight stands out, that is so because of the e in it\n",
    "            ## u @ self.V: matrix multiply (no bias?)\n",
    "            a = F.softmax(u @ self.V, 0)\n",
    "            attns.append(a) ## list of the attentions\n",
    "## Difference: we take a weighted average  \n",
    "            ## we use a the activaction results of above neural net to weight the results \n",
    "            ## of encoder outputs enc_out\n",
    "            ## enc_out not just last one but whole tensor with all outputs. gets weighed then by neural net results\n",
    "            Xa = (a.unsqueeze(2) * enc_out).sum(0)\n",
    "            emb = self.emb_dec(dec_inp)\n",
    "            wgt_enc = self.l3(torch.cat([emb, Xa], 1))\n",
    "## Until below this (decoder), all is identical with the above       \n",
    "            outp, h = self.gru_dec(wgt_enc.unsqueeze(0), h)\n",
    "            outp = self.out(self.out_drop(outp[0]))\n",
    "            res.append(outp)\n",
    "            dec_inp = V(outp.data.max(1)[1])\n",
    "            if (dec_inp==1).all(): break\n",
    "            ## we use teacher forcing    \n",
    "            if (y is not None) and (random.random()<self.pr_force):\n",
    "## Until above this (decoder), all is identical with the above\n",
    "                if i>=len(y): break\n",
    "                dec_inp = y[i]\n",
    "\n",
    "        res = torch.stack(res)\n",
    "        ## ret_attn: take note of attentions\n",
    "        if ret_attn: res = res,torch.stack(attns)\n",
    "        return res\n",
    "\n",
    "    def initHidden(self, bs): return V(torch.zeros(self.nl, bs, self.nh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = Seq2SeqAttnRNN(fr_vecd, fr_itos, dim_fr_vec, en_vecd, en_itos, dim_en_vec, nh, enlen_90)\n",
    "learn = RNN_Learner(md, SingleModel(to_gpu(rnn)), opt_fn=opt_fn)\n",
    "learn.crit = seq2seq_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=2e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## another improvement in loss\n",
    "learn.fit(lr, 1, cycle_len=15, use_clr=(20,10), stepper=Seq2SeqStepper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('attn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('attn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Current Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(iter(val_dl))\n",
    "probs,attns = learn.model(V(x),ret_attn=True) ## grab attentions out of the model\n",
    "preds = to_np(probs.max(2)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(180,190):\n",
    "    print(' '.join([fr_itos[o] for o in x[:,i] if o != 1]))\n",
    "    print(' '.join([en_itos[o] for o in y[:,i] if o != 1]))\n",
    "    print(' '.join([en_itos[o] for o in preds[:,i] if o!=1]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = to_np(attns[...,180])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## picture of each timestep at the attention\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 10))\n",
    "for i,ax in enumerate(axes.flat):\n",
    "    ax.plot(attn[i])\n",
    "    \n",
    "## similar info like in these attention graphs: https://distill.pub/2016/augmented-rnns/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqRNN_All(nn.Module):\n",
    "    def __init__(self, vecs_enc, itos_enc, em_sz_enc, vecs_dec, itos_dec, em_sz_dec, nh, out_sl, nl=2):\n",
    "        super().__init__()\n",
    "        self.emb_enc = create_emb(vecs_enc, itos_enc, em_sz_enc)\n",
    "        self.nl,self.nh,self.out_sl = nl,nh,out_sl\n",
    "        self.gru_enc = nn.GRU(em_sz_enc, nh, num_layers=nl, dropout=0.25, bidirectional=True)\n",
    "        self.out_enc = nn.Linear(nh*2, em_sz_dec, bias=False)\n",
    "        self.drop_enc = nn.Dropout(0.25)\n",
    "        self.emb_dec = create_emb(vecs_dec, itos_dec, em_sz_dec)\n",
    "        self.gru_dec = nn.GRU(em_sz_dec, em_sz_dec, num_layers=nl, dropout=0.1)\n",
    "        self.emb_enc_drop = nn.Dropout(0.15)\n",
    "        self.out_drop = nn.Dropout(0.35)\n",
    "        self.out = nn.Linear(em_sz_dec, len(itos_dec))\n",
    "        self.out.weight.data = self.emb_dec.weight.data\n",
    "\n",
    "        self.W1 = rand_p(nh*2, em_sz_dec)\n",
    "        self.l2 = nn.Linear(em_sz_dec, em_sz_dec)\n",
    "        self.l3 = nn.Linear(em_sz_dec+nh*2, em_sz_dec)\n",
    "        self.V = rand_p(em_sz_dec)\n",
    "        \n",
    "def forward(self, inp, y=None):\n",
    "        sl,bs = inp.size()\n",
    "        h = self.initHidden(bs)\n",
    "        emb = self.emb_enc_drop(self.emb_enc(inp))\n",
    "        enc_out, h = self.gru_enc(emb, h)\n",
    "        h = h.view(2,2,bs,-1).permute(0,2,1,3).contiguous().view(2,bs,-1)\n",
    "        h = self.out_enc(self.drop_enc(h))\n",
    "\n",
    "        dec_inp = V(torch.zeros(bs).long())\n",
    "        res,attns = [],[]\n",
    "        w1e = enc_out @ self.W1\n",
    "        for i in range(self.out_sl):\n",
    "            w2h = self.l2(h[-1])\n",
    "            u = F.tanh(w1e + w2h)\n",
    "            a = F.softmax(u @ self.V, 0)\n",
    "            attns.append(a)\n",
    "            Xa = (a.unsqueeze(2) * enc_out).sum(0)\n",
    "            emb = self.emb_dec(dec_inp)\n",
    "            wgt_enc = self.l3(torch.cat([emb, Xa], 1))\n",
    "            \n",
    "            outp, h = self.gru_dec(wgt_enc.unsqueeze(0), h)\n",
    "            outp = self.out(self.out_drop(outp[0]))\n",
    "            res.append(outp)\n",
    "            dec_inp = V(outp.data.max(1)[1])\n",
    "            if (dec_inp==1).all(): break\n",
    "            if (y is not None) and (random.random()<self.pr_force):\n",
    "                if i>=len(y): break\n",
    "                dec_inp = y[i]\n",
    "        return torch.stack(res)\n",
    "\n",
    "    def initHidden(self, bs): return V(torch.zeros(self.nl*2, bs, self.nh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = Seq2SeqRNN_All(fr_vecd, fr_itos, dim_fr_vec, en_vecd, en_itos, dim_en_vec, nh, enlen_90)\n",
    "learn = RNN_Learner(md, SingleModel(to_gpu(rnn)), opt_fn=opt_fn)\n",
    "learn.crit = seq2seq_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(lr, 1, cycle_len=15, use_clr=(20,10), stepper=Seq2SeqStepper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(iter(val_dl))\n",
    "probs = learn.model(V(x))\n",
    "preds = to_np(probs.max(2)[1])\n",
    "\n",
    "for i in range(180,190):\n",
    "    print(' '.join([fr_itos[o] for o in x[:,i] if o != 1]))\n",
    "    print(' '.join([en_itos[o] for o in y[:,i] if o != 1]))\n",
    "    print(' '.join([en_itos[o] for o in preds[:,i] if o!=1]))\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

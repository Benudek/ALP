{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Goal\" data-toc-modified-id=\"Goal-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Goal</a></span></li><li><span><a href=\"#Overview\" data-toc-modified-id=\"Overview-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Overview</a></span></li><li><span><a href=\"#DATA\" data-toc-modified-id=\"DATA-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>DATA</a></span><ul class=\"toc-item\"><li><span><a href=\"#Building-Word-Vectors-and-Pairs-for-SQL\" data-toc-modified-id=\"Building-Word-Vectors-and-Pairs-for-SQL-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Building Word Vectors and Pairs for SQL</a></span><ul class=\"toc-item\"><li><span><a href=\"#Webscraping\" data-toc-modified-id=\"Webscraping-3.1.1\"><span class=\"toc-item-num\">3.1.1&nbsp;&nbsp;</span>Webscraping</a></span><ul class=\"toc-item\"><li><span><a href=\"#SQLITE\" data-toc-modified-id=\"SQLITE-3.1.1.1\"><span class=\"toc-item-num\">3.1.1.1&nbsp;&nbsp;</span>SQLITE</a></span><ul class=\"toc-item\"><li><span><a href=\"#Scrapping-Text-from-the-SQLite-Documentation\" data-toc-modified-id=\"Scrapping-Text-from-the-SQLite-Documentation-3.1.1.1.1\"><span class=\"toc-item-num\">3.1.1.1.1&nbsp;&nbsp;</span>Scrapping Text from the SQLite Documentation</a></span></li><li><span><a href=\"#[LATER]-Scrapping-Text-from-the-Stackoverflow-Questions-with-SQlite\" data-toc-modified-id=\"[LATER]-Scrapping-Text-from-the-Stackoverflow-Questions-with-SQlite-3.1.1.1.2\"><span class=\"toc-item-num\">3.1.1.1.2&nbsp;&nbsp;</span>[LATER] Scrapping Text from the Stackoverflow Questions with SQlite</a></span></li><li><span><a href=\"#[LATER]-Scraping-text-from-github\" data-toc-modified-id=\"[LATER]-Scraping-text-from-github-3.1.1.1.3\"><span class=\"toc-item-num\">3.1.1.1.3&nbsp;&nbsp;</span>[LATER] Scraping text from github</a></span></li><li><span><a href=\"#Merging-and-saving-the-Text\" data-toc-modified-id=\"Merging-and-saving-the-Text-3.1.1.1.4\"><span class=\"toc-item-num\">3.1.1.1.4&nbsp;&nbsp;</span>Merging and saving the Text</a></span></li></ul></li><li><span><a href=\"#Mongo\" data-toc-modified-id=\"Mongo-3.1.1.2\"><span class=\"toc-item-num\">3.1.1.2&nbsp;&nbsp;</span>Mongo</a></span><ul class=\"toc-item\"><li><span><a href=\"#Scrapping-Text-from-the-Documentation\" data-toc-modified-id=\"Scrapping-Text-from-the-Documentation-3.1.1.2.1\"><span class=\"toc-item-num\">3.1.1.2.1&nbsp;&nbsp;</span>Scrapping Text from the Documentation</a></span></li></ul></li></ul></li><li><span><a href=\"#Building-a-Language-Model\" data-toc-modified-id=\"Building-a-Language-Model-3.1.2\"><span class=\"toc-item-num\">3.1.2&nbsp;&nbsp;</span>Building a Language Model</a></span></li><li><span><a href=\"#[DEPRECATED]-Training-a-Word-Vector\" data-toc-modified-id=\"[DEPRECATED]-Training-a-Word-Vector-3.1.3\"><span class=\"toc-item-num\">3.1.3&nbsp;&nbsp;</span>[DEPRECATED] Training a Word Vector</a></span></li><li><span><a href=\"#Training-a-Word-Vector\" data-toc-modified-id=\"Training-a-Word-Vector-3.1.4\"><span class=\"toc-item-num\">3.1.4&nbsp;&nbsp;</span>Training a Word Vector</a></span><ul class=\"toc-item\"><li><span><a href=\"#SQLite-Word-Vector\" data-toc-modified-id=\"SQLite-Word-Vector-3.1.4.1\"><span class=\"toc-item-num\">3.1.4.1&nbsp;&nbsp;</span>SQLite Word Vector</a></span></li><li><span><a href=\"#Mongo-Word-Vector\" data-toc-modified-id=\"Mongo-Word-Vector-3.1.4.2\"><span class=\"toc-item-num\">3.1.4.2&nbsp;&nbsp;</span>Mongo Word Vector</a></span></li></ul></li><li><span><a href=\"#Building-Language-Pairs\" data-toc-modified-id=\"Building-Language-Pairs-3.1.5\"><span class=\"toc-item-num\">3.1.5&nbsp;&nbsp;</span>Building Language Pairs</a></span><ul class=\"toc-item\"><li><span><a href=\"#SQlite\" data-toc-modified-id=\"SQlite-3.1.5.1\"><span class=\"toc-item-num\">3.1.5.1&nbsp;&nbsp;</span>SQlite</a></span><ul class=\"toc-item\"><li><span><a href=\"#SQlite-Inserts\" data-toc-modified-id=\"SQlite-Inserts-3.1.5.1.1\"><span class=\"toc-item-num\">3.1.5.1.1&nbsp;&nbsp;</span>SQlite Inserts</a></span></li><li><span><a href=\"#[LATER]-SQLite-Update\" data-toc-modified-id=\"[LATER]-SQLite-Update-3.1.5.1.2\"><span class=\"toc-item-num\">3.1.5.1.2&nbsp;&nbsp;</span>[LATER] SQLite Update</a></span></li><li><span><a href=\"#[LATER]-SQLite-Delete\" data-toc-modified-id=\"[LATER]-SQLite-Delete-3.1.5.1.3\"><span class=\"toc-item-num\">3.1.5.1.3&nbsp;&nbsp;</span>[LATER] SQLite Delete</a></span></li><li><span><a href=\"#[LATER]-SQLite-Select\" data-toc-modified-id=\"[LATER]-SQLite-Select-3.1.5.1.4\"><span class=\"toc-item-num\">3.1.5.1.4&nbsp;&nbsp;</span>[LATER] SQLite Select</a></span></li></ul></li><li><span><a href=\"#Mongo\" data-toc-modified-id=\"Mongo-3.1.5.2\"><span class=\"toc-item-num\">3.1.5.2&nbsp;&nbsp;</span>Mongo</a></span><ul class=\"toc-item\"><li><span><a href=\"#Mongo-Insert\" data-toc-modified-id=\"Mongo-Insert-3.1.5.2.1\"><span class=\"toc-item-num\">3.1.5.2.1&nbsp;&nbsp;</span>Mongo Insert</a></span></li><li><span><a href=\"#[LATER]-Mongo-Update\" data-toc-modified-id=\"[LATER]-Mongo-Update-3.1.5.2.2\"><span class=\"toc-item-num\">3.1.5.2.2&nbsp;&nbsp;</span>[LATER] Mongo Update</a></span></li><li><span><a href=\"#[LATER]-Mongo-Delete\" data-toc-modified-id=\"[LATER]-Mongo-Delete-3.1.5.2.3\"><span class=\"toc-item-num\">3.1.5.2.3&nbsp;&nbsp;</span>[LATER] Mongo Delete</a></span></li><li><span><a href=\"#[LATER]-Mongo-Select\" data-toc-modified-id=\"[LATER]-Mongo-Select-3.1.5.2.4\"><span class=\"toc-item-num\">3.1.5.2.4&nbsp;&nbsp;</span>[LATER] Mongo Select</a></span></li></ul></li><li><span><a href=\"#Zipping-the-pairs\" data-toc-modified-id=\"Zipping-the-pairs-3.1.5.3\"><span class=\"toc-item-num\">3.1.5.3&nbsp;&nbsp;</span>Zipping the pairs</a></span></li></ul></li></ul></li><li><span><a href=\"#Pre-Processing\" data-toc-modified-id=\"Pre-Processing-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Pre-Processing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Tokenize\" data-toc-modified-id=\"Tokenize-3.2.1\"><span class=\"toc-item-num\">3.2.1&nbsp;&nbsp;</span>Tokenize</a></span></li><li><span><a href=\"#Load-Word-Vectors\" data-toc-modified-id=\"Load-Word-Vectors-3.2.2\"><span class=\"toc-item-num\">3.2.2&nbsp;&nbsp;</span>Load Word Vectors</a></span></li><li><span><a href=\"#Model\" data-toc-modified-id=\"Model-3.2.3\"><span class=\"toc-item-num\">3.2.3&nbsp;&nbsp;</span>Model</a></span></li></ul></li></ul></li><li><span><a href=\"#ARCHITECTURE-&amp;-LOSS-FUNCTION\" data-toc-modified-id=\"ARCHITECTURE-&amp;-LOSS-FUNCTION-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>ARCHITECTURE &amp; LOSS FUNCTION</a></span><ul class=\"toc-item\"><li><span><a href=\"#Initial-Model\" data-toc-modified-id=\"Initial-Model-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Initial Model</a></span></li><li><span><a href=\"#Loss-Function:-categorical-cross-entropy\" data-toc-modified-id=\"Loss-Function:-categorical-cross-entropy-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Loss Function: categorical cross entropy</a></span></li><li><span><a href=\"#Test-Initial-Model\" data-toc-modified-id=\"Test-Initial-Model-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Test Initial Model</a></span></li><li><span><a href=\"#Bidirectional\" data-toc-modified-id=\"Bidirectional-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Bidirectional</a></span></li><li><span><a href=\"#Teacher-Forcing\" data-toc-modified-id=\"Teacher-Forcing-4.5\"><span class=\"toc-item-num\">4.5&nbsp;&nbsp;</span>Teacher Forcing</a></span></li><li><span><a href=\"#Attentional-Model\" data-toc-modified-id=\"Attentional-Model-4.6\"><span class=\"toc-item-num\">4.6&nbsp;&nbsp;</span>Attentional Model</a></span></li><li><span><a href=\"#Test-Current-Model\" data-toc-modified-id=\"Test-Current-Model-4.7\"><span class=\"toc-item-num\">4.7&nbsp;&nbsp;</span>Test Current Model</a></span></li><li><span><a href=\"#Summarised\" data-toc-modified-id=\"Summarised-4.8\"><span class=\"toc-item-num\">4.8&nbsp;&nbsp;</span>Summarised</a></span></li></ul></li><li><span><a href=\"#Final-Test\" data-toc-modified-id=\"Final-Test-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Final Test</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial LanguageTranslation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from fastai.text import *\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook uses common seg2seg translations build to translate natural languages into each other to translate different computer languages with the example of 2 SQL dialects into each other. \n",
    "\n",
    "This book is built upon the API and examples of fast.ai, specifically\n",
    "\n",
    "- https://github.com/fastai/fastai/blob/master/courses/dl2/translate.ipynb\n",
    "- http://course.fast.ai/lessons/lesson11.html\n",
    "\n",
    "Find a more extensive discussion of motivation & usecases here: https://github.com/Benudek/ALP/blob/master/articles/ArticialLanguageTranslation.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "For a Recurrent Neural Network to translate artificial languages into each other, we need 3 things:\n",
    "     \n",
    "    Data: \n",
    "        - pairs of 2 SQL statements, here SQL_MYSQL SQL_MONGODB\n",
    "        - Word Vectors, build from Documentation and Github\n",
    "    \n",
    "    Architecture: \n",
    "        - Recurrent Neural Network with Attention Mechanism \n",
    "        - arbirary length output where the tokens in the inut do not necessarily correspond \n",
    "        to the tokens in the output    \n",
    "        - we pass the SQL_MYSQL through a RNN. Result is one hidden state, 'backbone' / 'encoder'. \n",
    "        This is just a vector (matrix with mini batch)\n",
    "\n",
    "    Loss Function: \n",
    "        - in our case, would state that  SQL_MYSQL X should have generated SQL_MONGODB x, \n",
    "        - we will need a 'yardstick' to see how far off we were and update the weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "While Artificial Languages are less complex than natural languages, there are a number of additional considerations  \n",
    "        \n",
    "Syntactially correct output: \n",
    "    - the output must be syntactically correct formed, one way to verify this could be to send any \n",
    "    output to send to a compiler.\n",
    "    - If there is a syntactic error, the lossu function should 'punish' \n",
    "    that accordingly\n",
    "\n",
    "Handling of values & variable:\n",
    "    - variable values should be immutable! 1 is 1 'hello' is 'hello'. While natural languages know this construction, \n",
    "    in Artificial Languages it is particularly common and important.  \n",
    "    - Anything which is not a keyword of the language is either an immutable variable value or refers to a table or \n",
    "    column of the source or target language. While the former should be treated as immutable the latter should get \n",
    "    translated to the target model\n",
    "    - How a trained RNN treats non keywords, remains to be tested (TO DO)\n",
    "\n",
    "Traing Word Vectors:\n",
    "    - One approach could be to take bits of SQL and train a word vector with it. Since a SQL \n",
    "    language is a very sparse language this approach is not promising (TO DO: Test)\n",
    "    - Instead, we will train word vectors in text that embedds the desired sql statements. We will scrap \n",
    "    documentation of the 2 sql dialects and we will pick github repos with general programming languages like java or \n",
    "    python that contain these sql dialects\n",
    "    - Such word vectors will create embeddings for many more words than we will use for the pure sql translation. \n",
    "    This will eventually not matter, since the input string will only contain words of the source sql dialect and \n",
    "    hence only activate sql words of the target sql\n",
    "\n",
    "Usage of a Language Model:\n",
    "    - Alternatively to a Word Vector, we could train a language Model and use this in translations. This is planned \n",
    "    for a follow up Release\n",
    "    - Usage of a Language Model would be specifically useful for pre-training models. Since all SQL dialects are \n",
    "    known it would be easy to pre-train Models. A specific application would then have a pretrained network for the \n",
    "    syntax of the language and in the later layers add e.g. specifics of source and target model (TO ELABORATE)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Word Vectors and Pairs for SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To Train the Vector, we will get text that embeds the 2 sql dialects, i.e. Documentation of the dialects and github repositories with general programming languages using these sql dialects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Webscraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some useful links:\n",
    "\n",
    "- https://www.crummy.com/software/BeautifulSoup/bs4/doc/ from stackexchange stackoverflow, github, sql docs\n",
    "\n",
    "- https://pygithub.readthedocs.io/en/latest/examples/MainClass.html#search-repositories-by-language\n",
    "\n",
    "- https://realpython.com/python-web-scraping-practical-introduction/\n",
    "\n",
    "- https://pygithub.readthedocs.io/en/latest/examples/Repository.html#get-all-of-the-contents-of-the-repository-recursively\n",
    "\n",
    "- https://docs.scrapy.org/en/latest/intro/overview.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## http://docs.python-requests.org/en/master/\n",
    "##!pip install requests "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "##!pip install BeautifulSoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "from requests.exceptions import RequestException\n",
    "from contextlib import closing\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO: scrap this better ! with e.g. scrapy or recurvice functions going down the page tree from a starting point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SQLITE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Scrapping Text from the SQLite Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO: do for insert, update and so on also"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://www.sqlite.org/lang.html start here and go down the tree\n",
    "## \"https://www.sqlite.org/lang_select.html\"\n",
    "url_sqlitedoc = \"https://www.sqlite.org/lang_insert.html\" \n",
    "html_sqlitedoc = urlopen(url_sqlitedoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup_sqlitedoc = []\n",
    "soup_sqlitedoc.append(BeautifulSoup(html_sqlitedoc, 'lxml'))\n",
    "for link in soup_sqlitedoc[0].findAll('a', attrs={'href': re.compile(\"^http://\")}):\n",
    "    soup_sqlitedoc.append(BeautifulSoup(urlopen(link.get('href')), 'lxml'))\n",
    "##type(soup_sqlitedoc)\n",
    "##for mem in soup_sqlitedoc:\n",
    "##    print(mem.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = open(\"sqlite_all.txt\", \"w\")\n",
    "## adjust this later, merge below\n",
    "##text_file = open(\"sqliteinsert_sqllitedoc.txt\", \"w\")\n",
    "for mem in soup_sqlitedoc:\n",
    "    ##print(mem.text)\n",
    "    text_file.write(mem.text)\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### [LATER] Scrapping Text from the Stackoverflow Questions with SQlite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "url_stackoverflow = \"https://stackoverflow.com/questions/tagged/sqlite\"\n",
    "html_stackoverflow = urlopen(url_stackoverflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "soup_stackoverflow = []\n",
    "soup_stackoverflow.append(BeautifulSoup(html_stackoverflow, 'lxml'))\n",
    "for link in soup_stackoverflow[0].findAll('link', attrs={'href': re.compile(\"^http://\")}):\n",
    "    soup_stackoverflow.append(BeautifulSoup(urlopen(link.get('href')), 'lxml'))\n",
    "##type(soup_sqlitedoc)\n",
    "##for mem in soup_stackoverflow:\n",
    "##print(mem.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text_file = open(\"sqliteselect_stackoverflow.txt\", \"w\")\n",
    "for mem in soup_stackoverflow:\n",
    "    text_file.write(mem.text)\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### [LATER] Scraping text from github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "##!pip install PyGithub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import json\n",
    "from github import Github"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with open('credentials.json') as f:\n",
    "    data = json.load(f)\n",
    "    username = data['username']\n",
    "    password = data['password']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "g = Github(username, password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "##repositories = g.search_repositories(query='language:python')\n",
    "##for repo in repositories:\n",
    "##    print(repo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "textgithub_sqlitedoc = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from github import Github\n",
    "import getpass\n",
    "import base64\n",
    "##repo = g.get_repo(\"PyGithub/PyGithub\")\n",
    "##repo.get_topics()\n",
    "repositories = g.search_repositories(query='topic:sqlite')\n",
    "##repositories. (g.search_repositories(query='label:sqlite'))\n",
    "decoder = json.JSONDecoder()\n",
    "for repo in repositories:\n",
    "    ##print(repo)\n",
    "    contents = repo.get_contents(\"\")\n",
    "    while len(contents) > 1:\n",
    "        file_content = contents.pop(0)\n",
    "        if file_content.type == \"dir\":\n",
    "            contents.extend(repo.get_contents(file_content.path))\n",
    "        ## most files never contain sqlite\n",
    "        if (file_content.name.endswith(\"css\") or file_content.name.endswith(\"png\") or\n",
    "             file_content.name.endswith(\"yml\") or file_content.name.endswith(\"json\")) : continue\n",
    "        else:\n",
    "            ## only if we have those key words contained do we care\n",
    "            ##if (file_content.raw_data.values contains('SELECT')):  \n",
    "            ##    textgithub_sqlitedoc = textgithub_sqlitedoc + file_content.raw_data\n",
    "            ##print(file_content)\n",
    "            ##print(file_content.type)\n",
    "            if(file_content.type == \"file\"):\n",
    "            ##if(file_content.type != \"NoneType\" and file_content.type != \"dir\"):\n",
    "                file_data = base64.b64decode(file_content.content)\n",
    "                ##file_data_string = file_data.decode(\"utf-8\") \n",
    "                ##if(\"Select\" in file_data_string): print(file_data_string)\n",
    "                if(\"SELECT\" in file_content.content): print(file_data); print('#####')        \n",
    "            \n",
    "            ##file_out = open(file_content.name, \"w\")\n",
    "            ##file_out.write(file_data)\n",
    "            ##file_out.close()\n",
    "                   \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##repo = g.get_repo(\"PyGithub/PyGithub\")\n",
    "##repo.get_topics()\n",
    "##repositories = g.search_repositories(query='label:sqlite')\n",
    "##for repo in repositories:\n",
    "##print(repo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text_file = open(\"sqliteselect_github.txt\", \"w\")\n",
    "text_file.write(textgithub_sqlitedoc)\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Merging and saving the Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_sqlitedoc = ''\n",
    "##merge \n",
    "for i in range(len(soup_sqlitedoc)):\n",
    "    text_sqlitedoc = text_sqlitedoc + soup_sqlitedoc[i].get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text_sqlitedoc = ''\n",
    "stopped = 0\n",
    "##merge \n",
    "for i in range(min(len(soup_sqlitedoc), len(soup_stackoverflow))):\n",
    "    text_sqlitedoc = text_sqlitedoc + soup_sqlitedoc[i].get_text() + soup_stackoverflow[i].get_text()\n",
    "    stopped = i\n",
    "##plug the rest    \n",
    "if len(soup_sqlitedoc) > len(soup_stackoverflow):\n",
    "    for i in range(len(soup_sqlitedoc)):\n",
    "        text_sqlitedoc = text_sqlitedoc + soup_sqlitedoc[stopped + i].get_text()\n",
    "if len(soup_sqlitedoc) < len(soup_stackoverflow):\n",
    "    for i in range(len(soup_sqlitedoc)):\n",
    "        text_sqlitedoc = text_sqlitedoc + soup_stackoverflow[stopped + i].get_text()  \n",
    "##TO DO: this text needs lots of cleaning and assessing, where relevant\n",
    "len(text_sqlitedoc)\n",
    "##text_sqlitedoc\n",
    "##now the github stuff\n",
    "text_sqlitedoc = text_sqlitedoc + textgithub_sqlitedoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "##text_sqlitedoc = open(\"sqliteselect_sqllitedoc.txt\", \"w\")\n",
    "text_file = open(\"sqliteselect_all.txt\", \"w\")\n",
    "text_file.write(text_sqlitedoc)\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "##!ls -ltr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "##!more sqliteselect.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mongo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Scrapping Text from the Documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://www.sqlite.org/lang.html start here and go down the tree\n",
    "## \"https://www.sqlite.org/lang_select.html\"\n",
    "url_mongodoc = \"https://docs.mongodb.com/manual/crud/#create-operations\" \n",
    "html_mongodoc = urlopen(url_mongodoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "soup_mongodoc = []\n",
    "soup_mongodoc.append(BeautifulSoup(html_mongodoc, 'lxml'))\n",
    "for link in soup_mongodoc[0].findAll('a', attrs={'href': re.compile(\"^http://\")}):\n",
    "    soup_mongodoc.append(BeautifulSoup(urlopen(link.get('href')), 'lxml'))\n",
    "##type(soup_sqlitedoc)\n",
    "print(len(soup_sqlitedoc))\n",
    "##for mem in soup_sqlitedoc:\n",
    "    ## print(mem.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "##pd.write\n",
    "text_file = open(\"mongosql_all.txt\", \"w\")\n",
    "## adjust this later, merge below\n",
    "##text_file = open(\"sqliteinsert_sqllitedoc.txt\", \"w\")\n",
    "for mem in soup_mongodoc:\n",
    "    ##print(mem.text)\n",
    "    text_file.write(mem.text)\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('mongosql_all.csv', mode='w') as csv_file:\n",
    "    fieldnames = ['sqldialect', 'sentence']\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for mem in soup_mongodoc:\n",
    "        ##print(mem.text)\n",
    "        ##text_file.write(mem.text)\n",
    "        writer.writerow({'sqldialect': 'mongodb_sql doc', 'sentence': mem.text})\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building a Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.fast.ai/text.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "##?pd.read_clipboard\n",
    "##?pd.read_csv\n",
    "##?pd.read_excel\n",
    "##?pd.read_feather\n",
    "##?pd.read_fwf\n",
    "##?pd.read_gbq\n",
    "##?pd.read_hdf\n",
    "##?pd.read_html\n",
    "##?pd.read_json\n",
    "##?pd.read_msgpack\n",
    "##?pd.read_parquet\n",
    "##?pd.read_pickle\n",
    "##?pd.read_sas\n",
    "##?pd.read_sql\n",
    "##?pd.read_sql_query\n",
    "##?pd.read_stata\n",
    "##?pd.read_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [DEPRECATED] Training a Word Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pypi.org/project/word2vec/\n",
    "##!pip install cython\n",
    "##!pip install word2vec\n",
    "import word2vec\n",
    "pick your file\n",
    "word2vec.word2vec('sqlite_all.txt', 'sqlite_all.bin', size=100, verbose=True)\n",
    "word2vec.word2clusters('sqlite_all.txt', 'sqlite_all-clusters.txt', 100, verbose=True)\n",
    "import the word vec\n",
    "model_sqlite = word2vec.load('sqlite_all.bin')\n",
    "and have a look as a numpy array\n",
    "model_sqlite.vocab\n",
    "model_sqlite.vocab.shape\n",
    "model_sqlite.vectors\n",
    "model_sqlite['INSERT'].shape\n",
    "model_sqlite['INSERT'][:10]\n",
    "indexes, metrics = model_sqlite.similar(\"INSERT\")\n",
    "indexes, metrics\n",
    "We can get the words for those indexes\n",
    "model_sqlite.vocab[indexes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training a Word Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the facebook fastText library\n",
    "\n",
    "https://github.com/facebookresearch/fastText#word-representation-learning\n",
    "\n",
    "$ wget https://github.com/facebookresearch/fastText/archive/v0.1.0.zip\n",
    "$ unzip v0.1.0.zip\n",
    "$ cd fastText-0.1.0\n",
    "$ make\n",
    "\n",
    "! pip install git+https://github.com/facebookresearch/fastText.git\n",
    "\n",
    "https://fasttext.cc/docs/en/cheatsheet.html#content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "##! pip install git+https://github.com/facebookresearch/fastText.git\n",
    "##!fasttext skipgram -input sqlite_all.txt -output model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SQLite Word Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sqlite_all.txt is the training file containing UTF-8 encoded text. By default the word vectors will take into account character n-grams from 3 to 6 characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  36\n",
      "Number of labels: 0\n",
      "Progress: 100.0%  words/sec/thread: 819  lr: 0.000000  loss: 0.000000  eta: 0h0m \n"
     ]
    }
   ],
   "source": [
    "FASTTEXTPATH = Path('./fastText-0.1.0')\n",
    "!$FASTTEXTPATH/fasttext skipgram -input sqlite_all.txt -output sqlite_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--  1 jupyter jupyter      5623 Dec  5 14:54 sqlite_all.txt\r\n",
      "-rw-r--r--  1 jupyter jupyter 800029426 Dec  5 14:54 sqlite_all.bin\r\n",
      "-rw-r--r--  1 jupyter jupyter     39036 Dec  5 14:55 sqlite_all.vec\r\n"
     ]
    }
   ],
   "source": [
    "!ls -ltr | grep sqlite_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sqlite_all.vec is a text file containing the word vectors, one per line. model.bin is a binary file containing the parameters of the model along with the dictionary and all hyper parameters. The binary file can be used later to compute word vectors or to restart the optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mongo Word Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  65\n",
      "Number of labels: 0\n",
      "Progress: 100.0%  words/sec/thread: 888  lr: 0.000000  loss: 0.000000  eta: 0h0m \n"
     ]
    }
   ],
   "source": [
    "FASTTEXTPATH = Path('./fastText-0.1.0')\n",
    "!$FASTTEXTPATH/fasttext skipgram -input mongosql_all.txt -output mongosql_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--  1 jupyter jupyter     32939 Dec  5 14:54 mongosql_all.txt\r\n",
      "-rw-r--r--  1 jupyter jupyter     33026 Dec  5 14:54 mongosql_all.csv\r\n",
      "-rw-r--r--  1 jupyter jupyter 800053196 Dec  5 14:55 mongosql_all.bin\r\n",
      "-rw-r--r--  1 jupyter jupyter     61954 Dec  5 14:55 mongosql_all.vec\r\n"
     ]
    }
   ],
   "source": [
    "!ls -ltr | grep mongosql_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building Language Pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SQlite "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "##import numpy as np\n",
    "## import torch\n",
    "import pandas as pd\n",
    "from pandas import Series,DataFrame\n",
    "\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insertChar(mystring, position, chartoinsert ):\n",
    "    longi = len(mystring)\n",
    "    mystring   =  mystring[:position] + chartoinsert + mystring[position:] \n",
    "    return mystring "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "## O Reilly: Python for Data Analysis, chapter 6.4. Interacting with Databases, p.189\n",
    "\n",
    "drop = \"\"\"DROP TABLE Customer;\"\"\"\n",
    "query = \"\"\"\n",
    " CREATE TABLE IF NOT EXISTS Customer (lastname VARCHAR(40), email VARCHAR(40), networth REAL);\"\"\"\n",
    "con = sqlite3.connect('mydata.sqlite')\n",
    "con.execute(drop) ## decomment during 1st run\n",
    "con.execute(query)\n",
    "con.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### SQlite Inserts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "amountinserts = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"INSERT INTO Customer VALUES ('average1','joe.average1@email.com',10000)\",\n",
       " \"INSERT INTO Customer VALUES ('Piet2','piet.yanneke2@email.nl',60000)\",\n",
       " \"INSERT INTO Customer VALUES ('Mustermann3','max.mustermann3@email.de',60000)\",\n",
       " \"INSERT INTO Customer VALUES ('average4','joe.average4@email.com',40000)\",\n",
       " \"INSERT INTO Customer VALUES ('Piet5','piet.yanneke5@email.nl',150000)\",\n",
       " \"INSERT INTO Customer VALUES ('Mustermann6','max.mustermann6@email.de',120000)\",\n",
       " \"INSERT INTO Customer VALUES ('average7','joe.average7@email.com',70000)\",\n",
       " \"INSERT INTO Customer VALUES ('Piet8','piet.yanneke8@email.nl',240000)\",\n",
       " \"INSERT INTO Customer VALUES ('Mustermann9','max.mustermann9@email.de',180000)\",\n",
       " \"INSERT INTO Customer VALUES ('average10','joe.average10@email.com',100000)\",\n",
       " \"INSERT INTO Customer VALUES ('Piet11','piet.yanneke11@email.nl',330000)\",\n",
       " \"INSERT INTO Customer VALUES ('Mustermann12','max.mustermann12@email.de',240000)\",\n",
       " \"INSERT INTO Customer VALUES ('average13','joe.average13@email.com',130000)\",\n",
       " \"INSERT INTO Customer VALUES ('Piet14','piet.yanneke14@email.nl',420000)\",\n",
       " \"INSERT INTO Customer VALUES ('Mustermann15','max.mustermann15@email.de',300000)\",\n",
       " \"INSERT INTO Customer VALUES ('average16','joe.average16@email.com',160000)\",\n",
       " \"INSERT INTO Customer VALUES ('Piet17','piet.yanneke17@email.nl',510000)\",\n",
       " \"INSERT INTO Customer VALUES ('Mustermann18','max.mustermann18@email.de',360000)\",\n",
       " \"INSERT INTO Customer VALUES ('average19','joe.average19@email.com',190000)\",\n",
       " \"INSERT INTO Customer VALUES ('Piet20','piet.yanneke20@email.nl',600000)\",\n",
       " \"INSERT INTO Customer VALUES ('Mustermann21','max.mustermann21@email.de',420000)\",\n",
       " \"INSERT INTO Customer VALUES ('average22','joe.average22@email.com',220000)\",\n",
       " \"INSERT INTO Customer VALUES ('Piet23','piet.yanneke23@email.nl',690000)\",\n",
       " \"INSERT INTO Customer VALUES ('Mustermann24','max.mustermann24@email.de',480000)\",\n",
       " \"INSERT INTO Customer VALUES ('average25','joe.average25@email.com',250000)\",\n",
       " \"INSERT INTO Customer VALUES ('Piet26','piet.yanneke26@email.nl',780000)\",\n",
       " \"INSERT INTO Customer VALUES ('Mustermann27','max.mustermann27@email.de',540000)\",\n",
       " \"INSERT INTO Customer VALUES ('average28','joe.average28@email.com',280000)\",\n",
       " \"INSERT INTO Customer VALUES ('Piet29','piet.yanneke29@email.nl',870000)\"]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data ={0 : ('Mustermann', 'max.mustermann@email.de', 20000),\n",
    "        1 : ('average', 'joe.average@email.com', 10000),\n",
    "        2 : ('Piet', 'piet.yanneke@email.nl', 30000)\n",
    "        }\n",
    "\n",
    "sqlliteinsertlist =[]\n",
    "i=1\n",
    "while i < amountinserts:\n",
    "    sqlliteinsert = \"INSERT INTO Customer VALUES (\"\n",
    "    sqlliteinsert = sqlliteinsert + \"\\'\" + data[i%len(data)][0] + str(i) + \"\\',\"  + \"\\'\" + insertChar(data[i%len(data)][1], data[i%len(data)][1].find('@'),str(i)) + \"\\',\" + str(i * data[i%len(data)][2]) + \")\" \n",
    "    con.execute(sqlliteinsert)\n",
    "    sqlliteinsertlist.append(sqlliteinsert)\n",
    "    i = i + 1\n",
    "\n",
    "sqlliteinsertlist    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('sqlite_insert.csv', mode='w') as csv_file:\n",
    "    fieldnames = ['sqldialect', 'DML Statement']\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for sqlliteinsert in sqlliteinsertlist:\n",
    "        ##print(mem.text)\n",
    "        ##text_file.write(mem.text)\n",
    "        writer.writerow({'sqldialect': 'sqllite_sql', 'DML Statement': sqlliteinsert})  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('average1', 'joe.average1@email.com', 10000.0),\n",
       " ('Piet2', 'piet.yanneke2@email.nl', 60000.0),\n",
       " ('Mustermann3', 'max.mustermann3@email.de', 60000.0),\n",
       " ('average4', 'joe.average4@email.com', 40000.0),\n",
       " ('Piet5', 'piet.yanneke5@email.nl', 150000.0),\n",
       " ('Mustermann6', 'max.mustermann6@email.de', 120000.0),\n",
       " ('average7', 'joe.average7@email.com', 70000.0),\n",
       " ('Piet8', 'piet.yanneke8@email.nl', 240000.0),\n",
       " ('Mustermann9', 'max.mustermann9@email.de', 180000.0),\n",
       " ('average10', 'joe.average10@email.com', 100000.0),\n",
       " ('Piet11', 'piet.yanneke11@email.nl', 330000.0),\n",
       " ('Mustermann12', 'max.mustermann12@email.de', 240000.0),\n",
       " ('average13', 'joe.average13@email.com', 130000.0),\n",
       " ('Piet14', 'piet.yanneke14@email.nl', 420000.0),\n",
       " ('Mustermann15', 'max.mustermann15@email.de', 300000.0),\n",
       " ('average16', 'joe.average16@email.com', 160000.0),\n",
       " ('Piet17', 'piet.yanneke17@email.nl', 510000.0),\n",
       " ('Mustermann18', 'max.mustermann18@email.de', 360000.0),\n",
       " ('average19', 'joe.average19@email.com', 190000.0),\n",
       " ('Piet20', 'piet.yanneke20@email.nl', 600000.0),\n",
       " ('Mustermann21', 'max.mustermann21@email.de', 420000.0),\n",
       " ('average22', 'joe.average22@email.com', 220000.0),\n",
       " ('Piet23', 'piet.yanneke23@email.nl', 690000.0),\n",
       " ('Mustermann24', 'max.mustermann24@email.de', 480000.0),\n",
       " ('average25', 'joe.average25@email.com', 250000.0),\n",
       " ('Piet26', 'piet.yanneke26@email.nl', 780000.0),\n",
       " ('Mustermann27', 'max.mustermann27@email.de', 540000.0),\n",
       " ('average28', 'joe.average28@email.com', 280000.0),\n",
       " ('Piet29', 'piet.yanneke29@email.nl', 870000.0)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cursor = con.execute('select lastname, email, networth from Customer')\n",
    "rows = cursor.fetchall()\n",
    "rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('lastname', None, None, None, None, None, None),\n",
       " ('email', None, None, None, None, None, None),\n",
       " ('networth', None, None, None, None, None, None))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cursor.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lastname</th>\n",
       "      <th>email</th>\n",
       "      <th>networth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>average1</td>\n",
       "      <td>joe.average1@email.com</td>\n",
       "      <td>10000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Piet2</td>\n",
       "      <td>piet.yanneke2@email.nl</td>\n",
       "      <td>60000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mustermann3</td>\n",
       "      <td>max.mustermann3@email.de</td>\n",
       "      <td>60000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>average4</td>\n",
       "      <td>joe.average4@email.com</td>\n",
       "      <td>40000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Piet5</td>\n",
       "      <td>piet.yanneke5@email.nl</td>\n",
       "      <td>150000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Mustermann6</td>\n",
       "      <td>max.mustermann6@email.de</td>\n",
       "      <td>120000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>average7</td>\n",
       "      <td>joe.average7@email.com</td>\n",
       "      <td>70000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Piet8</td>\n",
       "      <td>piet.yanneke8@email.nl</td>\n",
       "      <td>240000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Mustermann9</td>\n",
       "      <td>max.mustermann9@email.de</td>\n",
       "      <td>180000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>average10</td>\n",
       "      <td>joe.average10@email.com</td>\n",
       "      <td>100000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Piet11</td>\n",
       "      <td>piet.yanneke11@email.nl</td>\n",
       "      <td>330000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Mustermann12</td>\n",
       "      <td>max.mustermann12@email.de</td>\n",
       "      <td>240000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>average13</td>\n",
       "      <td>joe.average13@email.com</td>\n",
       "      <td>130000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Piet14</td>\n",
       "      <td>piet.yanneke14@email.nl</td>\n",
       "      <td>420000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Mustermann15</td>\n",
       "      <td>max.mustermann15@email.de</td>\n",
       "      <td>300000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>average16</td>\n",
       "      <td>joe.average16@email.com</td>\n",
       "      <td>160000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Piet17</td>\n",
       "      <td>piet.yanneke17@email.nl</td>\n",
       "      <td>510000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Mustermann18</td>\n",
       "      <td>max.mustermann18@email.de</td>\n",
       "      <td>360000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>average19</td>\n",
       "      <td>joe.average19@email.com</td>\n",
       "      <td>190000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Piet20</td>\n",
       "      <td>piet.yanneke20@email.nl</td>\n",
       "      <td>600000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Mustermann21</td>\n",
       "      <td>max.mustermann21@email.de</td>\n",
       "      <td>420000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>average22</td>\n",
       "      <td>joe.average22@email.com</td>\n",
       "      <td>220000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Piet23</td>\n",
       "      <td>piet.yanneke23@email.nl</td>\n",
       "      <td>690000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Mustermann24</td>\n",
       "      <td>max.mustermann24@email.de</td>\n",
       "      <td>480000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>average25</td>\n",
       "      <td>joe.average25@email.com</td>\n",
       "      <td>250000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Piet26</td>\n",
       "      <td>piet.yanneke26@email.nl</td>\n",
       "      <td>780000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Mustermann27</td>\n",
       "      <td>max.mustermann27@email.de</td>\n",
       "      <td>540000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>average28</td>\n",
       "      <td>joe.average28@email.com</td>\n",
       "      <td>280000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Piet29</td>\n",
       "      <td>piet.yanneke29@email.nl</td>\n",
       "      <td>870000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        lastname                      email  networth\n",
       "0       average1     joe.average1@email.com   10000.0\n",
       "1          Piet2     piet.yanneke2@email.nl   60000.0\n",
       "2    Mustermann3   max.mustermann3@email.de   60000.0\n",
       "3       average4     joe.average4@email.com   40000.0\n",
       "4          Piet5     piet.yanneke5@email.nl  150000.0\n",
       "5    Mustermann6   max.mustermann6@email.de  120000.0\n",
       "6       average7     joe.average7@email.com   70000.0\n",
       "7          Piet8     piet.yanneke8@email.nl  240000.0\n",
       "8    Mustermann9   max.mustermann9@email.de  180000.0\n",
       "9      average10    joe.average10@email.com  100000.0\n",
       "10        Piet11    piet.yanneke11@email.nl  330000.0\n",
       "11  Mustermann12  max.mustermann12@email.de  240000.0\n",
       "12     average13    joe.average13@email.com  130000.0\n",
       "13        Piet14    piet.yanneke14@email.nl  420000.0\n",
       "14  Mustermann15  max.mustermann15@email.de  300000.0\n",
       "15     average16    joe.average16@email.com  160000.0\n",
       "16        Piet17    piet.yanneke17@email.nl  510000.0\n",
       "17  Mustermann18  max.mustermann18@email.de  360000.0\n",
       "18     average19    joe.average19@email.com  190000.0\n",
       "19        Piet20    piet.yanneke20@email.nl  600000.0\n",
       "20  Mustermann21  max.mustermann21@email.de  420000.0\n",
       "21     average22    joe.average22@email.com  220000.0\n",
       "22        Piet23    piet.yanneke23@email.nl  690000.0\n",
       "23  Mustermann24  max.mustermann24@email.de  480000.0\n",
       "24     average25    joe.average25@email.com  250000.0\n",
       "25        Piet26    piet.yanneke26@email.nl  780000.0\n",
       "26  Mustermann27  max.mustermann27@email.de  540000.0\n",
       "27     average28    joe.average28@email.com  280000.0\n",
       "28        Piet29    piet.yanneke29@email.nl  870000.0"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## transfer data to pandas\n",
    "dfsqllitedata = pd.DataFrame(rows, columns=[x[0] for x in cursor.description])\n",
    "dfsqllitedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lastname</th>\n",
       "      <th>email</th>\n",
       "      <th>networth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>average1</td>\n",
       "      <td>joe.average1@email.com</td>\n",
       "      <td>10000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Piet2</td>\n",
       "      <td>piet.yanneke2@email.nl</td>\n",
       "      <td>60000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mustermann3</td>\n",
       "      <td>max.mustermann3@email.de</td>\n",
       "      <td>60000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>average4</td>\n",
       "      <td>joe.average4@email.com</td>\n",
       "      <td>40000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Piet5</td>\n",
       "      <td>piet.yanneke5@email.nl</td>\n",
       "      <td>150000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Mustermann6</td>\n",
       "      <td>max.mustermann6@email.de</td>\n",
       "      <td>120000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>average7</td>\n",
       "      <td>joe.average7@email.com</td>\n",
       "      <td>70000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Piet8</td>\n",
       "      <td>piet.yanneke8@email.nl</td>\n",
       "      <td>240000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Mustermann9</td>\n",
       "      <td>max.mustermann9@email.de</td>\n",
       "      <td>180000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>average10</td>\n",
       "      <td>joe.average10@email.com</td>\n",
       "      <td>100000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Piet11</td>\n",
       "      <td>piet.yanneke11@email.nl</td>\n",
       "      <td>330000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Mustermann12</td>\n",
       "      <td>max.mustermann12@email.de</td>\n",
       "      <td>240000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>average13</td>\n",
       "      <td>joe.average13@email.com</td>\n",
       "      <td>130000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Piet14</td>\n",
       "      <td>piet.yanneke14@email.nl</td>\n",
       "      <td>420000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Mustermann15</td>\n",
       "      <td>max.mustermann15@email.de</td>\n",
       "      <td>300000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>average16</td>\n",
       "      <td>joe.average16@email.com</td>\n",
       "      <td>160000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Piet17</td>\n",
       "      <td>piet.yanneke17@email.nl</td>\n",
       "      <td>510000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Mustermann18</td>\n",
       "      <td>max.mustermann18@email.de</td>\n",
       "      <td>360000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>average19</td>\n",
       "      <td>joe.average19@email.com</td>\n",
       "      <td>190000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Piet20</td>\n",
       "      <td>piet.yanneke20@email.nl</td>\n",
       "      <td>600000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Mustermann21</td>\n",
       "      <td>max.mustermann21@email.de</td>\n",
       "      <td>420000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>average22</td>\n",
       "      <td>joe.average22@email.com</td>\n",
       "      <td>220000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Piet23</td>\n",
       "      <td>piet.yanneke23@email.nl</td>\n",
       "      <td>690000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Mustermann24</td>\n",
       "      <td>max.mustermann24@email.de</td>\n",
       "      <td>480000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>average25</td>\n",
       "      <td>joe.average25@email.com</td>\n",
       "      <td>250000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Piet26</td>\n",
       "      <td>piet.yanneke26@email.nl</td>\n",
       "      <td>780000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Mustermann27</td>\n",
       "      <td>max.mustermann27@email.de</td>\n",
       "      <td>540000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>average28</td>\n",
       "      <td>joe.average28@email.com</td>\n",
       "      <td>280000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Piet29</td>\n",
       "      <td>piet.yanneke29@email.nl</td>\n",
       "      <td>870000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        lastname                      email  networth\n",
       "0       average1     joe.average1@email.com   10000.0\n",
       "1          Piet2     piet.yanneke2@email.nl   60000.0\n",
       "2    Mustermann3   max.mustermann3@email.de   60000.0\n",
       "3       average4     joe.average4@email.com   40000.0\n",
       "4          Piet5     piet.yanneke5@email.nl  150000.0\n",
       "5    Mustermann6   max.mustermann6@email.de  120000.0\n",
       "6       average7     joe.average7@email.com   70000.0\n",
       "7          Piet8     piet.yanneke8@email.nl  240000.0\n",
       "8    Mustermann9   max.mustermann9@email.de  180000.0\n",
       "9      average10    joe.average10@email.com  100000.0\n",
       "10        Piet11    piet.yanneke11@email.nl  330000.0\n",
       "11  Mustermann12  max.mustermann12@email.de  240000.0\n",
       "12     average13    joe.average13@email.com  130000.0\n",
       "13        Piet14    piet.yanneke14@email.nl  420000.0\n",
       "14  Mustermann15  max.mustermann15@email.de  300000.0\n",
       "15     average16    joe.average16@email.com  160000.0\n",
       "16        Piet17    piet.yanneke17@email.nl  510000.0\n",
       "17  Mustermann18  max.mustermann18@email.de  360000.0\n",
       "18     average19    joe.average19@email.com  190000.0\n",
       "19        Piet20    piet.yanneke20@email.nl  600000.0\n",
       "20  Mustermann21  max.mustermann21@email.de  420000.0\n",
       "21     average22    joe.average22@email.com  220000.0\n",
       "22        Piet23    piet.yanneke23@email.nl  690000.0\n",
       "23  Mustermann24  max.mustermann24@email.de  480000.0\n",
       "24     average25    joe.average25@email.com  250000.0\n",
       "25        Piet26    piet.yanneke26@email.nl  780000.0\n",
       "26  Mustermann27  max.mustermann27@email.de  540000.0\n",
       "27     average28    joe.average28@email.com  280000.0\n",
       "28        Piet29    piet.yanneke29@email.nl  870000.0"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## TO DO transfer sqllite statement to pandas\n",
    "dfsqllitestatements = pd.DataFrame(rows, columns=[x[0] for x in cursor.description])\n",
    "dfsqllitestatements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### [LATER] SQLite Update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  [LATER] SQLite Delete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  [LATER] SQLite Select"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mongo "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.w3schools.com/python/python_mongodb_getstarted.asp\n",
    "https://docs.mongodb.com/manual/tutorial/install-mongodb-on-os-x/#install-mongodb-community-edition\n",
    "\n",
    "See: https://docs.brew.sh/Homebrew-and-Python\n",
    "==> mongodb\n",
    "To have launchd start mongodb now and restart at login:\n",
    "  brew services start mongodb\n",
    "Or, if you don't want/need a background service you can just run:\n",
    "  mongod --config /usr/local/etc/mongod.conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "##!pip install pymongo\n",
    "##!pip install brew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "\n",
    "## !brew services start mongodb\n",
    "## !brew services stop mongodb\n",
    "## !brew services restart mongodb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "## uname -a\n",
    "## https://docs.mongodb.com/manual/tutorial/install-mongodb-on-debian/\n",
    "\n",
    "##!sudo service mongod start\n",
    "## !tail /var/log/mongodb/mongod.log | grep -m1 \"waiting for connections on port\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo service mongod restart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check\n",
    "!tail /var/log/mongodb/mongod.log | grep -m1 \"waiting for connections on port\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "##!sudo service mongod stop\n",
    "##!rm /var/log/mongodb/mongod.log ## wont work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MongoDB waits until you have created a collection (table), \n",
    "## with at least one document (record) before it actually creates the database (and collection).\n",
    "\n",
    "myclient = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "\n",
    "mydb = myclient[\"mydatabase\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['admin', 'config', 'local', 'mydatabase']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dblist = myclient.list_database_names()\n",
    "dblist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The database exists.\n"
     ]
    }
   ],
   "source": [
    "if \"mydatabase\" in dblist:\n",
    "  print(\"The database exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['admin', 'config', 'local', 'mydatabase']\n"
     ]
    }
   ],
   "source": [
    "print(myclient.list_database_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "mycol = mydb[\"Clients\"]\n",
    "mycol.drop()\n",
    "mycol = mydb[\"Clients\"]\n",
    "print(mydb.list_collection_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "collist = mydb.list_collection_names()\n",
    "if \"Clients\" in collist:\n",
    "  print(\"The collection exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Mongo Insert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'average1', 'emailid': 'joe.average1@email.com'},\n",
       " {'name': 'Piet2', 'emailid': 'piet.yanneke2@email.nl'},\n",
       " {'name': 'Mustermann3', 'emailid': 'max.mustermann3@email.de'},\n",
       " {'name': 'average4', 'emailid': 'joe.average4@email.com'},\n",
       " {'name': 'Piet5', 'emailid': 'piet.yanneke5@email.nl'},\n",
       " {'name': 'Mustermann6', 'emailid': 'max.mustermann6@email.de'},\n",
       " {'name': 'average7', 'emailid': 'joe.average7@email.com'},\n",
       " {'name': 'Piet8', 'emailid': 'piet.yanneke8@email.nl'},\n",
       " {'name': 'Mustermann9', 'emailid': 'max.mustermann9@email.de'},\n",
       " {'name': 'average10', 'emailid': 'joe.average10@email.com'},\n",
       " {'name': 'Piet11', 'emailid': 'piet.yanneke11@email.nl'},\n",
       " {'name': 'Mustermann12', 'emailid': 'max.mustermann12@email.de'},\n",
       " {'name': 'average13', 'emailid': 'joe.average13@email.com'},\n",
       " {'name': 'Piet14', 'emailid': 'piet.yanneke14@email.nl'},\n",
       " {'name': 'Mustermann15', 'emailid': 'max.mustermann15@email.de'},\n",
       " {'name': 'average16', 'emailid': 'joe.average16@email.com'},\n",
       " {'name': 'Piet17', 'emailid': 'piet.yanneke17@email.nl'},\n",
       " {'name': 'Mustermann18', 'emailid': 'max.mustermann18@email.de'},\n",
       " {'name': 'average19', 'emailid': 'joe.average19@email.com'},\n",
       " {'name': 'Piet20', 'emailid': 'piet.yanneke20@email.nl'},\n",
       " {'name': 'Mustermann21', 'emailid': 'max.mustermann21@email.de'},\n",
       " {'name': 'average22', 'emailid': 'joe.average22@email.com'},\n",
       " {'name': 'Piet23', 'emailid': 'piet.yanneke23@email.nl'},\n",
       " {'name': 'Mustermann24', 'emailid': 'max.mustermann24@email.de'},\n",
       " {'name': 'average25', 'emailid': 'joe.average25@email.com'},\n",
       " {'name': 'Piet26', 'emailid': 'piet.yanneke26@email.nl'},\n",
       " {'name': 'Mustermann27', 'emailid': 'max.mustermann27@email.de'},\n",
       " {'name': 'average28', 'emailid': 'joe.average28@email.com'},\n",
       " {'name': 'Piet29', 'emailid': 'piet.yanneke29@email.nl'}]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##data ={0 : ('Mustermann', 'max.mustermann@email.de', 20000),\n",
    "##        1 : ('average', 'joe.average@email.com', 10000),\n",
    "##        2 : ('Piet', 'piet.yanneke@email.nl', 30000)\n",
    "##        }\n",
    "\n",
    "## Collection Clients, with slightly different column names and one less\n",
    "mylist = [{ \"name\": \"Mustermann\", \"emailid\": \"max.mustermann@email.de\"}\n",
    "            ,{ \"name\": \"average\", \"emailid\": \"joe.average@email.com\"}\n",
    "            , {\"name\": \"Piet\", \"emailid\": \"piet.yanneke@email.nl\"}\n",
    "         ]\n",
    "\n",
    "## d = data['response']['globalstats']['heist_success']['history']\n",
    "## result_dict = dict((i[\"date\"],i[\"total\"]) for i in d)\n",
    "\n",
    "## mongoinsertlist ={}\n",
    "mongoinsertlist =[]\n",
    "i=1\n",
    "mongoinserttest = mylist[i%len(mylist)][\"name\"]\n",
    "mongoinserttest = []\n",
    "## mongoinsertlist = \"[\"\n",
    "while i < amountinserts:\n",
    "    mongoinsertnamekey = \"name\"\n",
    "    mongoinsertnamevalue =  mylist[i%len(mylist)][\"name\"] + str(i)  ## '\\\"'  \n",
    "    \n",
    "    mongoinsertemailidkey = \"emailid\"\n",
    "    mongoinsertemaildvalue =  mylist[i%len(mylist)][\"emailid\"] ## '\\\"'  ## insertChar(data[i%len(data)][1], data[i%len(data)][1].find('@'),str(i)) + \"\\',\" + str(i * data[i%len(data)][2]) + \")\"\n",
    "    mongoinsertemaildvalue = insertChar(mongoinsertemaildvalue, mongoinsertemaildvalue.find('@'),str(i))\n",
    "    \n",
    "    client = { mongoinsertnamekey : mongoinsertnamevalue , mongoinsertemailidkey : mongoinsertemaildvalue }\n",
    "    ##mycol.insert_one(mongoinsert)\n",
    "    mongoinsertlist.append(client)\n",
    "    i = i + 1\n",
    "    \n",
    "mongoinsertlist\n",
    "##mylist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('mongodb_insert.csv', mode='w') as csv_file:\n",
    "    fieldnames = ['sqldialect', 'DML Statement']\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for mongoinsert in mongoinsertlist:\n",
    "        ##print(mem.text)\n",
    "        ##text_file.write(mem.text)\n",
    "        writer.writerow({'sqldialect': 'mongodb_sql', 'DML Statement': mongoinsert})  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "##insertChar??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "## mongoinsert1 = { \"name\": \"Mustermann\", \"emailid\": \"max.mustermann@email.de\"}\n",
    "## mongoinsert2 = { \"name\": \"average\", \"emailid\": \"joe.average@email.com\"}\n",
    "## mongoinsert3 = {\"name\": \"Piet\", \"emailid\": \"piet.yanneke@email.nl\"}\n",
    "\n",
    "## x1 = mycol.insert_one(mongoinsert1)\n",
    "## x2 = mycol.insert_one(mongoinsert2)\n",
    "## x3 = mycol.insert_one(mongoinsert3)\n",
    "\n",
    "## print(x1.inserted_id)\n",
    "## print(x2.inserted_id)\n",
    "## print(x3.inserted_id)\n",
    "\n",
    "mycol.drop()\n",
    "x = mycol.insert_many(mongoinsertlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO DO: doesnt work yet, but no problem. We want to zip the sql statements not the data\n",
    "## transfer data to pandas\n",
    "##dfmongodata = pd.DataFrame(rowsmongo)\n",
    "##dfmongodata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  [LATER] Mongo Update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  [LATER] Mongo Delete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  [LATER] Mongo Select"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Zipping the pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target format, same as for natural languages. Only the dot might not mark a sentence end, even though we could just add it.\n",
    "\n",
    "http://www.manythings.org/anki/\n",
    "\n",
    "    I'm ill.\tIk ben ziek.\n",
    "    I'm sad.\tIk ben bedroefd.\n",
    "    It's me!\tIk ben het.\n",
    "    It's me.\tIk ben het.\n",
    "    Join us.\tKom met ons mee.\n",
    "    Join us.\tDoe maar mee.\n",
    "    Join us.\tKom maar meedoen.\n",
    "    Join us.\tSluit je aan.\n",
    "    Join us.\tSluit je bij ons aan.\n",
    "    Keep it.\tHoud het.\n",
    "    See you!\tTot weerziens!\n",
    "    Shut up!\tZwijg!\n",
    "    Stop it.\tHou daar toch mee op.\n",
    "    \n",
    "We do this with Panda .... we take the respective Insert, and Select statemeents from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sqlite</th>\n",
       "      <th>mongosql</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INSERT INTO Customer VALUES ('average1','joe.a...</td>\n",
       "      <td>{'name': 'average1', 'emailid': 'joe.average1@...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>INSERT INTO Customer VALUES ('Piet2','piet.yan...</td>\n",
       "      <td>{'name': 'Piet2', 'emailid': 'piet.yanneke2@em...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INSERT INTO Customer VALUES ('Mustermann3','ma...</td>\n",
       "      <td>{'name': 'Mustermann3', 'emailid': 'max.muster...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INSERT INTO Customer VALUES ('average4','joe.a...</td>\n",
       "      <td>{'name': 'average4', 'emailid': 'joe.average4@...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>INSERT INTO Customer VALUES ('Piet5','piet.yan...</td>\n",
       "      <td>{'name': 'Piet5', 'emailid': 'piet.yanneke5@em...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>INSERT INTO Customer VALUES ('Mustermann6','ma...</td>\n",
       "      <td>{'name': 'Mustermann6', 'emailid': 'max.muster...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>INSERT INTO Customer VALUES ('average7','joe.a...</td>\n",
       "      <td>{'name': 'average7', 'emailid': 'joe.average7@...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>INSERT INTO Customer VALUES ('Piet8','piet.yan...</td>\n",
       "      <td>{'name': 'Piet8', 'emailid': 'piet.yanneke8@em...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>INSERT INTO Customer VALUES ('Mustermann9','ma...</td>\n",
       "      <td>{'name': 'Mustermann9', 'emailid': 'max.muster...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>INSERT INTO Customer VALUES ('average10','joe....</td>\n",
       "      <td>{'name': 'average10', 'emailid': 'joe.average1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>INSERT INTO Customer VALUES ('Piet11','piet.ya...</td>\n",
       "      <td>{'name': 'Piet11', 'emailid': 'piet.yanneke11@...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>INSERT INTO Customer VALUES ('Mustermann12','m...</td>\n",
       "      <td>{'name': 'Mustermann12', 'emailid': 'max.muste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>INSERT INTO Customer VALUES ('average13','joe....</td>\n",
       "      <td>{'name': 'average13', 'emailid': 'joe.average1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>INSERT INTO Customer VALUES ('Piet14','piet.ya...</td>\n",
       "      <td>{'name': 'Piet14', 'emailid': 'piet.yanneke14@...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>INSERT INTO Customer VALUES ('Mustermann15','m...</td>\n",
       "      <td>{'name': 'Mustermann15', 'emailid': 'max.muste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>INSERT INTO Customer VALUES ('average16','joe....</td>\n",
       "      <td>{'name': 'average16', 'emailid': 'joe.average1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>INSERT INTO Customer VALUES ('Piet17','piet.ya...</td>\n",
       "      <td>{'name': 'Piet17', 'emailid': 'piet.yanneke17@...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>INSERT INTO Customer VALUES ('Mustermann18','m...</td>\n",
       "      <td>{'name': 'Mustermann18', 'emailid': 'max.muste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>INSERT INTO Customer VALUES ('average19','joe....</td>\n",
       "      <td>{'name': 'average19', 'emailid': 'joe.average1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>INSERT INTO Customer VALUES ('Piet20','piet.ya...</td>\n",
       "      <td>{'name': 'Piet20', 'emailid': 'piet.yanneke20@...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>INSERT INTO Customer VALUES ('Mustermann21','m...</td>\n",
       "      <td>{'name': 'Mustermann21', 'emailid': 'max.muste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>INSERT INTO Customer VALUES ('average22','joe....</td>\n",
       "      <td>{'name': 'average22', 'emailid': 'joe.average2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>INSERT INTO Customer VALUES ('Piet23','piet.ya...</td>\n",
       "      <td>{'name': 'Piet23', 'emailid': 'piet.yanneke23@...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>INSERT INTO Customer VALUES ('Mustermann24','m...</td>\n",
       "      <td>{'name': 'Mustermann24', 'emailid': 'max.muste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>INSERT INTO Customer VALUES ('average25','joe....</td>\n",
       "      <td>{'name': 'average25', 'emailid': 'joe.average2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>INSERT INTO Customer VALUES ('Piet26','piet.ya...</td>\n",
       "      <td>{'name': 'Piet26', 'emailid': 'piet.yanneke26@...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>INSERT INTO Customer VALUES ('Mustermann27','m...</td>\n",
       "      <td>{'name': 'Mustermann27', 'emailid': 'max.muste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>INSERT INTO Customer VALUES ('average28','joe....</td>\n",
       "      <td>{'name': 'average28', 'emailid': 'joe.average2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>INSERT INTO Customer VALUES ('Piet29','piet.ya...</td>\n",
       "      <td>{'name': 'Piet29', 'emailid': 'piet.yanneke29@...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sqlite  \\\n",
       "0   INSERT INTO Customer VALUES ('average1','joe.a...   \n",
       "1   INSERT INTO Customer VALUES ('Piet2','piet.yan...   \n",
       "2   INSERT INTO Customer VALUES ('Mustermann3','ma...   \n",
       "3   INSERT INTO Customer VALUES ('average4','joe.a...   \n",
       "4   INSERT INTO Customer VALUES ('Piet5','piet.yan...   \n",
       "5   INSERT INTO Customer VALUES ('Mustermann6','ma...   \n",
       "6   INSERT INTO Customer VALUES ('average7','joe.a...   \n",
       "7   INSERT INTO Customer VALUES ('Piet8','piet.yan...   \n",
       "8   INSERT INTO Customer VALUES ('Mustermann9','ma...   \n",
       "9   INSERT INTO Customer VALUES ('average10','joe....   \n",
       "10  INSERT INTO Customer VALUES ('Piet11','piet.ya...   \n",
       "11  INSERT INTO Customer VALUES ('Mustermann12','m...   \n",
       "12  INSERT INTO Customer VALUES ('average13','joe....   \n",
       "13  INSERT INTO Customer VALUES ('Piet14','piet.ya...   \n",
       "14  INSERT INTO Customer VALUES ('Mustermann15','m...   \n",
       "15  INSERT INTO Customer VALUES ('average16','joe....   \n",
       "16  INSERT INTO Customer VALUES ('Piet17','piet.ya...   \n",
       "17  INSERT INTO Customer VALUES ('Mustermann18','m...   \n",
       "18  INSERT INTO Customer VALUES ('average19','joe....   \n",
       "19  INSERT INTO Customer VALUES ('Piet20','piet.ya...   \n",
       "20  INSERT INTO Customer VALUES ('Mustermann21','m...   \n",
       "21  INSERT INTO Customer VALUES ('average22','joe....   \n",
       "22  INSERT INTO Customer VALUES ('Piet23','piet.ya...   \n",
       "23  INSERT INTO Customer VALUES ('Mustermann24','m...   \n",
       "24  INSERT INTO Customer VALUES ('average25','joe....   \n",
       "25  INSERT INTO Customer VALUES ('Piet26','piet.ya...   \n",
       "26  INSERT INTO Customer VALUES ('Mustermann27','m...   \n",
       "27  INSERT INTO Customer VALUES ('average28','joe....   \n",
       "28  INSERT INTO Customer VALUES ('Piet29','piet.ya...   \n",
       "\n",
       "                                             mongosql  \n",
       "0   {'name': 'average1', 'emailid': 'joe.average1@...  \n",
       "1   {'name': 'Piet2', 'emailid': 'piet.yanneke2@em...  \n",
       "2   {'name': 'Mustermann3', 'emailid': 'max.muster...  \n",
       "3   {'name': 'average4', 'emailid': 'joe.average4@...  \n",
       "4   {'name': 'Piet5', 'emailid': 'piet.yanneke5@em...  \n",
       "5   {'name': 'Mustermann6', 'emailid': 'max.muster...  \n",
       "6   {'name': 'average7', 'emailid': 'joe.average7@...  \n",
       "7   {'name': 'Piet8', 'emailid': 'piet.yanneke8@em...  \n",
       "8   {'name': 'Mustermann9', 'emailid': 'max.muster...  \n",
       "9   {'name': 'average10', 'emailid': 'joe.average1...  \n",
       "10  {'name': 'Piet11', 'emailid': 'piet.yanneke11@...  \n",
       "11  {'name': 'Mustermann12', 'emailid': 'max.muste...  \n",
       "12  {'name': 'average13', 'emailid': 'joe.average1...  \n",
       "13  {'name': 'Piet14', 'emailid': 'piet.yanneke14@...  \n",
       "14  {'name': 'Mustermann15', 'emailid': 'max.muste...  \n",
       "15  {'name': 'average16', 'emailid': 'joe.average1...  \n",
       "16  {'name': 'Piet17', 'emailid': 'piet.yanneke17@...  \n",
       "17  {'name': 'Mustermann18', 'emailid': 'max.muste...  \n",
       "18  {'name': 'average19', 'emailid': 'joe.average1...  \n",
       "19  {'name': 'Piet20', 'emailid': 'piet.yanneke20@...  \n",
       "20  {'name': 'Mustermann21', 'emailid': 'max.muste...  \n",
       "21  {'name': 'average22', 'emailid': 'joe.average2...  \n",
       "22  {'name': 'Piet23', 'emailid': 'piet.yanneke23@...  \n",
       "23  {'name': 'Mustermann24', 'emailid': 'max.muste...  \n",
       "24  {'name': 'average25', 'emailid': 'joe.average2...  \n",
       "25  {'name': 'Piet26', 'emailid': 'piet.yanneke26@...  \n",
       "26  {'name': 'Mustermann27', 'emailid': 'max.muste...  \n",
       "27  {'name': 'average28', 'emailid': 'joe.average2...  \n",
       "28  {'name': 'Piet29', 'emailid': 'piet.yanneke29@...  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## zip a dataframe of \n",
    "\n",
    "\n",
    "##mongoinsertlist = [mongoinsert1, mongoinsert1, mongoinsert3]\n",
    "## sqlliteinsert1 mongoinsert1\n",
    "## sqlliteinsert2 mongoinsert2\n",
    "## sqlliteinsert3 mongoinsert3\n",
    "\n",
    "## sqlliteinsert1 : \"INSERT INTO Customer VALUES ('Mustermann', 'max.mustermann@email.de', 20000)\"\n",
    "## mongoinsert1 : {'name': 'Mustermann', 'emailid': 'max.mustermann@email.de', '_id': ObjectId('5bd5aad637b8896cc56ea929')}\n",
    "\n",
    "sql_tuples = list(zip(sqlliteinsertlist,mongoinsertlist ))\n",
    "pd.DataFrame(sql_tuples, columns=['sqlite','mongosql'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... whats next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check this, what does it and useful for our kinds of text ?\n",
    "## spacy? no, do we need this here?\n",
    "from fastai.text import *\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "## adjust this to 2 sql languages\n",
    "PATH = Path('data')\n",
    "##PATH.mkdir(exist_ok=True)\n",
    "PATH = PATH/'translatesql'\n",
    "PATH.mkdir(exist_ok=True)\n",
    "TMP_PATH = PATH/'tmp'\n",
    "TMP_PATH.mkdir(exist_ok=True)\n",
    "fname='sqls'\n",
    "sqlite_fname = PATH/f'{fname}.sqlite'\n",
    "mongosql_fname = PATH/f'{fname}.mongosql'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/translatesql\n"
     ]
    }
   ],
   "source": [
    "##?Path\n",
    "print(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(\"INSERT INTO Customer VALUES ('average1','joe.average1@email.com',10000)\",\n",
       "   {'name': 'average1',\n",
       "    'emailid': 'joe.average1@email.com',\n",
       "    '_id': ObjectId('5c07e6d7769981236b533368')}),\n",
       "  (\"INSERT INTO Customer VALUES ('Piet2','piet.yanneke2@email.nl',60000)\",\n",
       "   {'name': 'Piet2',\n",
       "    'emailid': 'piet.yanneke2@email.nl',\n",
       "    '_id': ObjectId('5c07e6d7769981236b533369')}),\n",
       "  (\"INSERT INTO Customer VALUES ('Mustermann3','max.mustermann3@email.de',60000)\",\n",
       "   {'name': 'Mustermann3',\n",
       "    'emailid': 'max.mustermann3@email.de',\n",
       "    '_id': ObjectId('5c07e6d7769981236b53336a')}),\n",
       "  (\"INSERT INTO Customer VALUES ('average4','joe.average4@email.com',40000)\",\n",
       "   {'name': 'average4',\n",
       "    'emailid': 'joe.average4@email.com',\n",
       "    '_id': ObjectId('5c07e6d7769981236b53336b')}),\n",
       "  (\"INSERT INTO Customer VALUES ('Piet5','piet.yanneke5@email.nl',150000)\",\n",
       "   {'name': 'Piet5',\n",
       "    'emailid': 'piet.yanneke5@email.nl',\n",
       "    '_id': ObjectId('5c07e6d7769981236b53336c')})],\n",
       " 29)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## just some examples\n",
    "##qs[:5], len(qs)\n",
    "sql_tuples[:5], len(sql_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'name': 'average1',\n",
       "  'emailid': 'joe.average1@email.com',\n",
       "  '_id': ObjectId('5c07e6d7769981236b533368')},\n",
       " {'name': 'Piet2',\n",
       "  'emailid': 'piet.yanneke2@email.nl',\n",
       "  '_id': ObjectId('5c07e6d7769981236b533369')},\n",
       " {'name': 'Mustermann3',\n",
       "  'emailid': 'max.mustermann3@email.de',\n",
       "  '_id': ObjectId('5c07e6d7769981236b53336a')},\n",
       " {'name': 'average4',\n",
       "  'emailid': 'joe.average4@email.com',\n",
       "  '_id': ObjectId('5c07e6d7769981236b53336b')},\n",
       " {'name': 'Piet5',\n",
       "  'emailid': 'piet.yanneke5@email.nl',\n",
       "  '_id': ObjectId('5c07e6d7769981236b53336c')},\n",
       " {'name': 'Mustermann6',\n",
       "  'emailid': 'max.mustermann6@email.de',\n",
       "  '_id': ObjectId('5c07e6d7769981236b53336d')},\n",
       " {'name': 'average7',\n",
       "  'emailid': 'joe.average7@email.com',\n",
       "  '_id': ObjectId('5c07e6d7769981236b53336e')},\n",
       " {'name': 'Piet8',\n",
       "  'emailid': 'piet.yanneke8@email.nl',\n",
       "  '_id': ObjectId('5c07e6d7769981236b53336f')},\n",
       " {'name': 'Mustermann9',\n",
       "  'emailid': 'max.mustermann9@email.de',\n",
       "  '_id': ObjectId('5c07e6d7769981236b533370')},\n",
       " {'name': 'average10',\n",
       "  'emailid': 'joe.average10@email.com',\n",
       "  '_id': ObjectId('5c07e6d7769981236b533371')},\n",
       " {'name': 'Piet11',\n",
       "  'emailid': 'piet.yanneke11@email.nl',\n",
       "  '_id': ObjectId('5c07e6d7769981236b533372')},\n",
       " {'name': 'Mustermann12',\n",
       "  'emailid': 'max.mustermann12@email.de',\n",
       "  '_id': ObjectId('5c07e6d7769981236b533373')},\n",
       " {'name': 'average13',\n",
       "  'emailid': 'joe.average13@email.com',\n",
       "  '_id': ObjectId('5c07e6d7769981236b533374')},\n",
       " {'name': 'Piet14',\n",
       "  'emailid': 'piet.yanneke14@email.nl',\n",
       "  '_id': ObjectId('5c07e6d7769981236b533375')},\n",
       " {'name': 'Mustermann15',\n",
       "  'emailid': 'max.mustermann15@email.de',\n",
       "  '_id': ObjectId('5c07e6d7769981236b533376')},\n",
       " {'name': 'average16',\n",
       "  'emailid': 'joe.average16@email.com',\n",
       "  '_id': ObjectId('5c07e6d7769981236b533377')},\n",
       " {'name': 'Piet17',\n",
       "  'emailid': 'piet.yanneke17@email.nl',\n",
       "  '_id': ObjectId('5c07e6d7769981236b533378')},\n",
       " {'name': 'Mustermann18',\n",
       "  'emailid': 'max.mustermann18@email.de',\n",
       "  '_id': ObjectId('5c07e6d7769981236b533379')},\n",
       " {'name': 'average19',\n",
       "  'emailid': 'joe.average19@email.com',\n",
       "  '_id': ObjectId('5c07e6d7769981236b53337a')},\n",
       " {'name': 'Piet20',\n",
       "  'emailid': 'piet.yanneke20@email.nl',\n",
       "  '_id': ObjectId('5c07e6d7769981236b53337b')},\n",
       " {'name': 'Mustermann21',\n",
       "  'emailid': 'max.mustermann21@email.de',\n",
       "  '_id': ObjectId('5c07e6d7769981236b53337c')},\n",
       " {'name': 'average22',\n",
       "  'emailid': 'joe.average22@email.com',\n",
       "  '_id': ObjectId('5c07e6d7769981236b53337d')},\n",
       " {'name': 'Piet23',\n",
       "  'emailid': 'piet.yanneke23@email.nl',\n",
       "  '_id': ObjectId('5c07e6d7769981236b53337e')},\n",
       " {'name': 'Mustermann24',\n",
       "  'emailid': 'max.mustermann24@email.de',\n",
       "  '_id': ObjectId('5c07e6d7769981236b53337f')},\n",
       " {'name': 'average25',\n",
       "  'emailid': 'joe.average25@email.com',\n",
       "  '_id': ObjectId('5c07e6d7769981236b533380')},\n",
       " {'name': 'Piet26',\n",
       "  'emailid': 'piet.yanneke26@email.nl',\n",
       "  '_id': ObjectId('5c07e6d7769981236b533381')},\n",
       " {'name': 'Mustermann27',\n",
       "  'emailid': 'max.mustermann27@email.de',\n",
       "  '_id': ObjectId('5c07e6d7769981236b533382')},\n",
       " {'name': 'average28',\n",
       "  'emailid': 'joe.average28@email.com',\n",
       "  '_id': ObjectId('5c07e6d7769981236b533383')},\n",
       " {'name': 'Piet29',\n",
       "  'emailid': 'piet.yanneke29@email.nl',\n",
       "  '_id': ObjectId('5c07e6d7769981236b533384')})"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlite_sql_tuples,mongosql_sql_tuples = zip(*sql_tuples)\n",
    "##sqlite_sql_tuples\n",
    "mongosql_sql_tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "##?Tokenizer\n",
    "##https://docs.fast.ai/text.transform.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"xxup insert xxup into xxmaj customer xxup values ( ' average1','joe.average1@email.com',1 xxrep 4 0 ) xxup insert xxup into xxmaj customer xxup values ( ' xxmaj piet2','piet.yanneke2@email.nl',6 xxrep 4 0 ) xxup insert xxup into xxmaj customer xxup values ( ' xxmaj mustermann3','max.mustermann3@email.de',6 xxrep 4 0 ) xxup insert xxup into xxmaj customer xxup values ( ' average4','joe.average4@email.com',4 xxrep 4 0 ) xxup insert xxup into xxmaj customer xxup values ( ' xxmaj piet5','piet.yanneke5@email.nl',15 xxrep 4 0 ) xxup insert xxup into xxmaj customer xxup values ( ' xxmaj mustermann6','max.mustermann6@email.de',12 xxrep 4 0 ) xxup insert xxup into xxmaj customer xxup values ( ' average7','joe.average7@email.com',7 xxrep 4 0 ) xxup insert xxup into xxmaj customer xxup values ( ' xxmaj piet8','piet.yanneke8@email.nl',24 xxrep 4 0 ) xxup insert xxup into xxmaj customer xxup values ( ' xxmaj mustermann9','max.mustermann9@email.de',18 xxrep 4 0 ) xxup insert xxup into xxmaj customer xxup values ( ' average10','joe.average10@email.com',1 xxrep 5 0 ) xxup insert xxup into xxmaj customer xxup values ( ' xxmaj piet11','piet.yanneke11@email.nl',33 xxrep 4 0 ) xxup insert xxup into xxmaj customer xxup values ( ' xxmaj mustermann12','max.mustermann12@email.de',24 xxrep 4 0 ) xxup insert xxup into xxmaj customer xxup values ( ' average13','joe.average13@email.com',13 xxrep 4 0 ) xxup insert xxup into xxmaj customer xxup values ( ' xxmaj piet14','piet.yanneke14@email.nl',42 xxrep 4 0 ) xxup insert xxup into xxmaj customer xxup values ( ' xxmaj mustermann15','max.mustermann15@email.de',3 xxrep 5 0 ) xxup insert xxup into xxmaj customer xxup values ( ' average16','joe.average16@email.com',16 xxrep 4 0 ) xxup insert xxup into xxmaj customer xxup values ( ' xxmaj piet17','piet.yanneke17@email.nl',51 xxrep 4 0 ) xxup insert xxup into xxmaj customer xxup values ( ' xxmaj mustermann18','max.mustermann18@email.de',36 xxrep 4 0 ) xxup insert xxup into xxmaj customer xxup values ( ' average19','joe.average19@email.com',19 xxrep 4 0 ) xxup insert xxup into xxmaj customer xxup values ( ' xxmaj piet20','piet.yanneke20@email.nl',6 xxrep 5 0 ) xxup insert xxup into xxmaj customer xxup values ( ' xxmaj mustermann21','max.mustermann21@email.de',42 xxrep 4 0 ) xxup insert xxup into xxmaj customer xxup values ( ' average22','joe.average22@email.com',22 xxrep 4 0 ) xxup insert xxup into xxmaj customer xxup values ( ' xxmaj piet23','piet.yanneke23@email.nl',69 xxrep 4 0 ) xxup insert xxup into xxmaj customer xxup values ( ' xxmaj mustermann24','max.mustermann24@email.de',48 xxrep 4 0 ) xxup insert xxup into xxmaj customer xxup values ( ' average25','joe.average25@email.com',25 xxrep 4 0 ) xxup insert xxup into xxmaj customer xxup values ( ' xxmaj piet26','piet.yanneke26@email.nl',78 xxrep 4 0 ) xxup insert xxup into xxmaj customer xxup values ( ' xxmaj mustermann27','max.mustermann27@email.de',54 xxrep 4 0 ) xxup insert xxup into xxmaj customer xxup values ( ' average28','joe.average28@email.com',28 xxrep 4 0 ) xxup insert xxup into xxmaj customer xxup values ( ' xxmaj piet29','piet.yanneke29@email.nl',87 xxrep 4 0 )\""
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlite_sql_text = ''.join(sqlite_sql_tuples)\n",
    "tokenizer = Tokenizer()\n",
    "tok = SpacyTokenizer('en')\n",
    "sqlite_sql_tok = tokenizer.process_text(sqlite_sql_text, tok)\n",
    "' '.join(sqlite_sql_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{ ' name ' : ' average1 ' , ' emailid ' : ' joe.average1@email.com ' , ' _ i d ' : objectid('5c07e6d7769981236b533368')},{'name ' : ' xxmaj piet2 ' , ' emailid ' : ' piet.yanneke2@email.nl ' , ' _ i d ' : objectid('5c07e6d7769981236b533369')},{'name ' : ' xxmaj mustermann3 ' , ' emailid ' : ' max.mustermann3@email.de ' , ' _ i d ' : objectid('5c07e6d7769981236b53336a')},{'name ' : ' average4 ' , ' emailid ' : ' joe.average4@email.com ' , ' _ i d ' : objectid('5c07e6d7769981236b53336b')},{'name ' : ' xxmaj piet5 ' , ' emailid ' : ' piet.yanneke5@email.nl ' , ' _ i d ' : objectid('5c07e6d7769981236b53336c')},{'name ' : ' xxmaj mustermann6 ' , ' emailid ' : ' max.mustermann6@email.de ' , ' _ i d ' : objectid('5c07e6d7769981236b53336d')},{'name ' : ' average7 ' , ' emailid ' : ' joe.average7@email.com ' , ' _ i d ' : objectid('5c07e6d7769981236b53336e')},{'name ' : ' xxmaj piet8 ' , ' emailid ' : ' piet.yanneke8@email.nl ' , ' _ i d ' : objectid('5c07e6d7769981236b53336f')},{'name ' : ' xxmaj mustermann9 ' , ' emailid ' : ' max.mustermann9@email.de ' , ' _ i d ' : objectid('5c07e6d7769981236b533370')},{'name ' : ' average10 ' , ' emailid ' : ' joe.average10@email.com ' , ' _ i d ' : objectid('5c07e6d7769981236b533371')},{'name ' : ' xxmaj piet11 ' , ' emailid ' : ' piet.yanneke11@email.nl ' , ' _ i d ' : objectid('5c07e6d7769981236b533372')},{'name ' : ' xxmaj mustermann12 ' , ' emailid ' : ' max.mustermann12@email.de ' , ' _ i d ' : objectid('5c07e6d7769981236b533373')},{'name ' : ' average13 ' , ' emailid ' : ' joe.average13@email.com ' , ' _ i d ' : objectid('5c07e6d7769981236b533374')},{'name ' : ' xxmaj piet14 ' , ' emailid ' : ' piet.yanneke14@email.nl ' , ' _ i d ' : objectid('5c07e6d7769981236b533375')},{'name ' : ' xxmaj mustermann15 ' , ' emailid ' : ' max.mustermann15@email.de ' , ' _ i d ' : objectid('5c07e6d7769981236b533376')},{'name ' : ' average16 ' , ' emailid ' : ' joe.average16@email.com ' , ' _ i d ' : objectid('5c07e6d7769981236b533377')},{'name ' : ' xxmaj piet17 ' , ' emailid ' : ' piet.yanneke17@email.nl ' , ' _ i d ' : objectid('5c07e6d7769981236b533378')},{'name ' : ' xxmaj mustermann18 ' , ' emailid ' : ' max.mustermann18@email.de ' , ' _ i d ' : objectid('5c07e6d7769981236b533379')},{'name ' : ' average19 ' , ' emailid ' : ' joe.average19@email.com ' , ' _ i d ' : objectid('5c07e6d7769981236b53337a')},{'name ' : ' xxmaj piet20 ' , ' emailid ' : ' piet.yanneke20@email.nl ' , ' _ i d ' : objectid('5c07e6d7769981236b53337b')},{'name ' : ' xxmaj mustermann21 ' , ' emailid ' : ' max.mustermann21@email.de ' , ' _ i d ' : objectid('5c07e6d7769981236b53337c')},{'name ' : ' average22 ' , ' emailid ' : ' joe.average22@email.com ' , ' _ i d ' : objectid('5c07e6d7769981236b53337d')},{'name ' : ' xxmaj piet23 ' , ' emailid ' : ' piet.yanneke23@email.nl ' , ' _ i d ' : objectid('5c07e6d7769981236b53337e')},{'name ' : ' xxmaj mustermann24 ' , ' emailid ' : ' max.mustermann24@email.de ' , ' _ i d ' : objectid('5c07e6d7769981236b53337f')},{'name ' : ' average25 ' , ' emailid ' : ' joe.average25@email.com ' , ' _ i d ' : objectid('5c07e6d7769981236b533380')},{'name ' : ' xxmaj piet26 ' , ' emailid ' : ' piet.yanneke26@email.nl ' , ' _ i d ' : objectid('5c07e6d7769981236b533381')},{'name ' : ' xxmaj mustermann27 ' , ' emailid ' : ' max.mustermann27@email.de ' , ' _ i d ' : objectid('5c07e6d7769981236b533382')},{'name ' : ' average28 ' , ' emailid ' : ' joe.average28@email.com ' , ' _ i d ' : objectid('5c07e6d7769981236b533383')},{'name ' : ' xxmaj piet29 ' , ' emailid ' : ' piet.yanneke29@email.nl ' , ' _ i d ' : objectid('5c07e6d7769981236b533384 ' ) }\""
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mongosql_sql_text = ','.join(str(v) for v in mongosql_sql_tuples)\n",
    "tokenizer = Tokenizer()\n",
    "mongosql_sql_tok = tokenizer.process_text(mongosql_sql_text, tok)\n",
    "tok = SpacyTokenizer('en')\n",
    "' '.join(mongosql_sql_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('xxup', '{')"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlite_sql_tok[0], mongosql_sql_tok[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8.0, 11.700000000000045)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "np.percentile([len(o) for o in sqlite_sql_tok], 90), np.percentile([len(o) for o in mongosql_sql_tok], 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_sqlite_sql_tok = np.array([len(o)<10 for o in sqlite_sql_tok])\n",
    "keep_mongosql_sql_tok = np.array([len(o)<10 for o in mongosql_sql_tok])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlite_sql_tok = np.array(sqlite_sql_tok)[keep_sqlite_sql_tok]\n",
    "mongosql_sql_tok = np.array(mongosql_sql_tok)[keep_mongosql_sql_tok]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(sqlite_sql_tok, (PATH/'sqlite_sql_tok.pkl').open('wb'))\n",
    "pickle.dump(mongosql_sql_tok, (PATH/'mongosql_sql_tok.pkl').open('wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlite_sql_tok = pickle.load((PATH/'sqlite_sql_tok.pkl').open('rb'))\n",
    "mongosql_sql_tok = pickle.load((PATH/'mongosql_sql_tok.pkl').open('rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- this might need to change, we might need to dictionaries to identify variables that are immutable\n",
    "- Also, we always have the same list of keywords, therefore we might do all this differnt (how)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "## can we use this instead: https://docs.fast.ai/text.transform.html\n",
    "import collections\n",
    "def toks2ids(tok,pre):\n",
    "    freq = collections.Counter(p for o in tok for p in o)\n",
    "    itos = [o for o,c in freq.most_common(40000)]\n",
    "    itos.insert(0, '_bos_')\n",
    "    itos.insert(1, '_pad_')\n",
    "    itos.insert(2, '_eos_')\n",
    "    itos.insert(3, '_unk')\n",
    "    stoi = collections.defaultdict(lambda: 3, {v:k for k,v in enumerate(itos)})\n",
    "    ids = np.array([([stoi[o] for o in p] + [2]) for p in tok])\n",
    "    np.save(TMP_PATH/f'{pre}_ids.npy', ids)\n",
    "    pickle.dump(itos, open(TMP_PATH/f'{pre}_itos.pkl', 'wb'))\n",
    "    return ids,itos,stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "## mapping from tokens to IDs\n",
    "sqlite_sql_ids,sqlite_sql_itos,sqlite_sql_stoi = toks2ids(sqlite_sql_tok,'sqlite')\n",
    "mongosql_sql_ids,mongosql_sql_itos,mongosql_sql_stoi = toks2ids(mongosql_sql_tok,'mongosql')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ids(pre):\n",
    "    ids = np.load(TMP_PATH/f'{pre}_ids.npy')\n",
    "    itos = pickle.load(open(TMP_PATH/f'{pre}_itos.pkl', 'rb'))\n",
    "    stoi = collections.defaultdict(lambda: 3, {v:k for k,v in enumerate(itos)})\n",
    "    return ids,itos,stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlite_sql_ids,sqlite_sql_itos,sqlite_sql_stoi = load_ids('sqlite')\n",
    "mongosql_sql_ids,mongosql_sql_itos,mongosql_sql_stoi = load_ids('mongosql')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['{', '_eos_'], 26, 35)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## just a test here\n",
    "[mongosql_sql_itos[o] for o in mongosql_sql_ids[0]], len(sqlite_sql_itos), len(mongosql_sql_itos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "##! pip install git+https://github.com/facebookresearch/fastText.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastText as ft\n",
    "from fastai import *\n",
    "from fastai.text import * "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the 2 wordvectors trained above:\n",
    "\n",
    "- sqlite_all.bin\n",
    "- mongosql_all.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "##!ls -ltr\n",
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "## en_vecs = ft.load_model(str((PATH/'wiki.en.bin')))\n",
    "## sqlite_vecs = word2vec.load('sqlite_all.bin')\n",
    "sqlite_vecs = ft.load_model('sqlite_all.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "##fr_vecs = ft.load_model(str((PATH/'wiki.fr.bin')))\n",
    "mongo_vecs = ft.load_model('mongosql_all.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vecs(lang, ft_vecs):\n",
    "    vecd = {w:ft_vecs.get_word_vector(w) for w in ft_vecs.get_words()}\n",
    "    pickle.dump(vecd, open(PATH/f'wiki.{lang}.pkl','wb'))\n",
    "    return vecd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlite_vecd = get_vecs('sqlite', sqlite_vecs)\n",
    "mongo_vecd = get_vecs('mongo', mongo_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'</s>': array([-0.008276,  0.007853, -0.006241, -0.002111, ...,  0.006212,  0.009624, -0.00549 , -0.001924], dtype=float32),\n",
       " 'the': array([ 0.000619,  0.001022,  0.001314, -0.003401, ...,  0.000433,  0.002386,  0.000284, -0.002677], dtype=float32),\n",
       " 'of': array([ 0.00032 , -0.00444 , -0.001661, -0.002171, ..., -0.002661,  0.003059,  0.002194,  0.002763], dtype=float32),\n",
       " 'show': array([ 0.000838,  0.002659, -0.000447, -0.000708, ..., -0.001445,  0.001981, -0.001099,  0.002244], dtype=float32),\n",
       " 'is': array([ 0.00182 ,  0.000833, -0.001047,  0.003871, ...,  0.002191, -0.000655, -0.001566, -0.006427], dtype=float32),\n",
       " 'INSERT': array([-0.001485, -0.000643, -0.000369,  0.000643, ...,  0.000708,  0.000466,  0.000768, -0.001456], dtype=float32),\n",
       " '=': array([ 0.002737, -0.002874,  0.007813, -0.005409, ..., -0.002047, -0.005387,  0.008574, -0.006415], dtype=float32),\n",
       " 'The': array([ 0.000279, -0.001448, -0.00508 , -0.004454, ...,  0.001678, -0.00059 ,  0.004474, -0.002585], dtype=float32),\n",
       " 'in': array([ 0.005901, -0.002701,  0.000167,  0.000378, ...,  0.003404, -0.000124,  0.001456,  0.002296], dtype=float32),\n",
       " 'a': array([-0.005697, -0.00117 ,  0.004455,  0.001111, ...,  0.000231,  0.000413, -0.004478, -0.005715], dtype=float32),\n",
       " 'an': array([ 0.000955,  0.002065, -0.00456 ,  0.004155, ...,  0.001131, -0.001326, -0.00417 , -0.00067 ], dtype=float32),\n",
       " 'for': array([ 0.000415,  0.002296, -0.001973,  0.003645, ...,  0.001313,  0.000827,  0.000312, -0.002812], dtype=float32),\n",
       " 'number': array([ 9.393624e-05, -8.094935e-04, -3.818368e-04, -1.812309e-03, ..., -9.242982e-04,  1.383181e-03, -1.482138e-03,\n",
       "        -9.554219e-04], dtype=float32),\n",
       " '}': array([ 0.003901,  0.002449,  0.00275 ,  0.008443, ..., -0.004474, -0.005745,  0.008918, -0.00587 ], dtype=float32),\n",
       " 'SELECT': array([ 1.566191e-03, -1.201644e-04, -1.944344e-03,  1.351590e-03, ..., -2.984187e-04,  2.486646e-03, -1.562109e-03,\n",
       "        -6.745569e-05], dtype=float32),\n",
       " 'with': array([-0.00036 ,  0.001733, -0.000837, -0.00067 , ...,  0.001392, -0.001074, -0.001098, -0.000726], dtype=float32),\n",
       " 'statement': array([ 0.000256,  0.000893, -0.000106, -0.000866, ..., -0.001319,  0.000517,  0.001327,  0.000246], dtype=float32),\n",
       " 'if': array([ 0.000896,  0.00086 ,  0.001783, -0.002576, ..., -0.001891, -0.002881,  0.000741, -0.000859], dtype=float32),\n",
       " 'as': array([ 0.002981,  0.005047, -0.002653, -0.001051, ...,  0.001855,  0.000597, -0.001403,  0.000846], dtype=float32),\n",
       " 'be': array([ 0.00247 ,  0.000976,  0.000656,  0.002208, ..., -0.003603,  0.001937,  0.001089,  0.005512], dtype=float32),\n",
       " 'not': array([-0.000426, -0.003516, -0.001035, -0.001617, ...,  0.002737,  0.00046 , -0.00505 , -0.00086 ], dtype=float32),\n",
       " 'columns': array([ 1.024167e-03, -3.900270e-05, -5.785622e-04,  1.985846e-03, ..., -3.721933e-04, -2.034670e-03,  4.010921e-04,\n",
       "        -1.259569e-03], dtype=float32),\n",
       " 'table': array([-2.610983e-04, -1.057748e-03,  3.177377e-04,  1.075467e-05, ...,  2.555580e-05, -3.999424e-05,  2.050710e-04,\n",
       "        -1.932251e-03], dtype=float32),\n",
       " 'new': array([-0.002268,  0.002358, -0.00067 ,  0.001807, ...,  0.00036 ,  0.003808,  0.001847,  0.001963], dtype=float32),\n",
       " 'each': array([ 9.689922e-04,  4.812693e-04,  7.636924e-04,  3.420481e-04, ...,  4.781326e-06, -8.552221e-04,  7.438849e-04,\n",
       "        -5.360541e-04], dtype=float32),\n",
       " 'must': array([ 0.00121 , -0.001667, -0.001312,  0.000229, ...,  0.00136 , -0.002574,  0.001812,  0.001094], dtype=float32),\n",
       " 'and': array([-0.003615,  0.003376,  0.00085 ,  0.000429, ..., -0.000646, -0.002089, -0.002489, -0.00077 ], dtype=float32),\n",
       " 'row': array([ 0.001803, -0.001779, -0.002712, -0.003786, ..., -0.001229,  0.00613 ,  0.000549, -0.002436], dtype=float32),\n",
       " 'column': array([-8.806957e-05,  5.659750e-05, -5.680007e-04,  1.498421e-03, ..., -4.817741e-04, -3.186943e-03, -7.325421e-04,\n",
       "         1.313813e-04], dtype=float32),\n",
       " 'that': array([ 0.001183, -0.000906, -0.000459,  0.001007, ..., -0.001136,  0.001898, -0.001302, -0.000594], dtype=float32),\n",
       " 'or': array([ 0.002573,  0.000919, -0.001716,  0.000108, ...,  0.000127,  0.001466,  0.002219, -0.000549], dtype=float32),\n",
       " 'list': array([-0.002778, -0.000243,  0.001463,  0.000324, ..., -0.000902,  0.000519, -0.003883,  0.003017], dtype=float32),\n",
       " 'into': array([ 0.001124, -0.000188,  0.00018 ,  0.002076, ..., -0.000921, -0.000808,  0.000827,  0.000271], dtype=float32),\n",
       " 'statements': array([-6.480209e-05, -1.779908e-04,  7.508915e-04, -3.474082e-04, ..., -1.192700e-03, -1.417912e-04,  1.171962e-03,\n",
       "        -8.508328e-04], dtype=float32),\n",
       " 'DEFAULT': array([ 0.001263, -0.000403, -0.000683, -0.002063, ..., -0.002457, -0.001845, -0.000589, -0.001579], dtype=float32),\n",
       " 'Search': array([ 0.001547,  0.003133, -0.000743,  0.00012 , ..., -0.002156, -0.000421,  0.001515,  0.000362], dtype=float32)}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlite_vecd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'</s>': array([ 0.026572,  0.008227,  0.0087  , -0.016861, ..., -0.006737,  0.002499, -0.019761, -0.017849], dtype=float32),\n",
       " 'a': array([ 0.305529,  0.006245,  0.135235, -0.1302  , ..., -0.109298, -0.059339, -0.126099, -0.138492], dtype=float32),\n",
       " 'MongoDB': array([ 0.323463,  0.006455,  0.137797, -0.137952, ..., -0.11607 , -0.066065, -0.13162 , -0.146963], dtype=float32),\n",
       " 'to': array([ 0.35738 ,  0.00932 ,  0.154755, -0.149509, ..., -0.132644, -0.075474, -0.142769, -0.162828], dtype=float32),\n",
       " 'Replica': array([ 0.288852,  0.001284,  0.122842, -0.122143, ..., -0.107079, -0.061758, -0.120462, -0.132373], dtype=float32),\n",
       " 'Set': array([ 0.287953,  0.002465,  0.125282, -0.12323 , ..., -0.111241, -0.055618, -0.116624, -0.132725], dtype=float32),\n",
       " 'for': array([ 0.229385,  0.001481,  0.101941, -0.097624, ..., -0.088496, -0.046647, -0.093528, -0.110318], dtype=float32),\n",
       " 'in': array([ 0.152427, -0.001483,  0.065589, -0.061596, ..., -0.061227, -0.032165, -0.064983, -0.066387], dtype=float32),\n",
       " 'and': array([ 0.407465,  0.00228 ,  0.171844, -0.16869 , ..., -0.151661, -0.081227, -0.164439, -0.180363], dtype=float32),\n",
       " 'Sharded': array([ 0.348371,  0.004407,  0.150586, -0.146226, ..., -0.130183, -0.072054, -0.144872, -0.160623], dtype=float32),\n",
       " 'the': array([ 0.097308, -0.003197,  0.043396, -0.037789, ..., -0.032341, -0.018074, -0.039918, -0.03956 ], dtype=float32),\n",
       " 'with': array([ 0.280516,  0.00355 ,  0.116617, -0.122412, ..., -0.103391, -0.056701, -0.116488, -0.130742], dtype=float32),\n",
       " 'on': array([ 0.152004,  0.00185 ,  0.05957 , -0.064214, ..., -0.053832, -0.03471 , -0.06347 , -0.072358], dtype=float32),\n",
       " 'Cluster': array([ 0.259259,  0.001843,  0.112628, -0.110692, ..., -0.095147, -0.05449 , -0.104251, -0.116819], dtype=float32),\n",
       " 'of': array([ 0.135835, -0.001438,  0.062583, -0.054535, ..., -0.052333, -0.027988, -0.053225, -0.059478], dtype=float32),\n",
       " '>=': array([ 0.103942,  0.00091 ,  0.040732, -0.046167, ..., -0.042766, -0.022958, -0.041665, -0.043843], dtype=float32),\n",
       " '0)': array([ 0.09179 ,  0.003746,  0.041403, -0.039356, ..., -0.034289, -0.015613, -0.038713, -0.045596], dtype=float32),\n",
       " 'Query': array([ 0.170904, -0.000268,  0.073836, -0.073244, ..., -0.064525, -0.033801, -0.0722  , -0.078559], dtype=float32),\n",
       " '=': array([ 0.007537,  0.000643, -0.004484, -0.008804, ..., -0.000807, -0.003149, -0.000394,  0.002589], dtype=float32),\n",
       " 'operations': array([ 0.093289,  0.00109 ,  0.040951, -0.039679, ..., -0.034951, -0.01951 , -0.037665, -0.043769], dtype=float32),\n",
       " 'Notes': array([ 0.162623,  0.001423,  0.072899, -0.068874, ..., -0.06134 , -0.03397 , -0.065383, -0.074068], dtype=float32),\n",
       " 'from': array([ 0.184507,  0.003292,  0.077522, -0.076349, ..., -0.067018, -0.037413, -0.07801 , -0.08308 ], dtype=float32),\n",
       " 'Data': array([ 0.142711,  0.002035,  0.058683, -0.059504, ..., -0.051709, -0.030519, -0.055471, -0.066696], dtype=float32),\n",
       " 'documents': array([ 0.023375,  0.000341,  0.009608, -0.010174, ..., -0.008434, -0.004049, -0.009497, -0.011072], dtype=float32),\n",
       " 'an': array([ 0.252679,  0.001011,  0.102951, -0.100833, ..., -0.097897, -0.051738, -0.104894, -0.114273], dtype=float32),\n",
       " '{': array([ 0.041809, -0.000313,  0.019527, -0.015527, ..., -0.015342, -0.009273, -0.01358 , -0.017996], dtype=float32),\n",
       " 'or': array([ 0.189588, -0.002036,  0.082369, -0.079719, ..., -0.068846, -0.038502, -0.076137, -0.083984], dtype=float32),\n",
       " 'Standalone': array([ 0.136797,  0.001137,  0.056697, -0.056069, ..., -0.050861, -0.028256, -0.055053, -0.064229], dtype=float32),\n",
       " 'gptadslots[1]=': array([ 0.01219 ,  0.000829,  0.005343, -0.004456, ..., -0.003808, -0.002271, -0.004503, -0.005311], dtype=float32),\n",
       " '}': array([ 0.037399,  0.00703 ,  0.016202, -0.025851, ..., -0.015068, -0.014867, -0.01053 , -0.020272], dtype=float32),\n",
       " '||': array([ 1.414831e-01,  6.940449e-05,  6.386539e-02, -6.090232e-02, ..., -5.594619e-02, -3.174747e-02, -5.404009e-02,\n",
       "        -6.709255e-02], dtype=float32),\n",
       " 'Changes': array([ 0.330344,  0.00503 ,  0.143908, -0.138392, ..., -0.12077 , -0.068312, -0.134915, -0.151999], dtype=float32),\n",
       " '3.2': array([ 0.039502,  0.001908,  0.011817, -0.014399, ..., -0.015052, -0.009017, -0.015747, -0.023599], dtype=float32),\n",
       " 'else': array([ 0.043936,  0.001613,  0.019333, -0.020129, ..., -0.017432, -0.01183 , -0.016627, -0.018448], dtype=float32),\n",
       " 'if': array([ 0.035431, -0.003472,  0.01155 , -0.014108, ..., -0.014792, -0.011583, -0.020825, -0.013718], dtype=float32),\n",
       " 'New': array([ 0.035923, -0.0012  ,  0.011989, -0.011807, ..., -0.014022, -0.00174 , -0.018365, -0.016414], dtype=float32),\n",
       " 'Update': array([ 0.143852,  0.002717,  0.061276, -0.062769, ..., -0.048996, -0.030493, -0.059844, -0.062009], dtype=float32),\n",
       " 'Access': array([ 0.16398 ,  0.001208,  0.07055 , -0.068895, ..., -0.058905, -0.031634, -0.066648, -0.074455], dtype=float32),\n",
       " 'mongo': array([ 0.214146,  0.002819,  0.090702, -0.091094, ..., -0.079092, -0.040654, -0.086815, -0.098467], dtype=float32),\n",
       " \"[[160,600],[243,202],[293,244]],'mongodb-docs-1').addService(googletag.pubads());\": array([ 0.017775,  0.000453,  0.007616, -0.008164, ..., -0.006887, -0.003806, -0.008241, -0.00864 ], dtype=float32),\n",
       " 'version': array([ 0.033267, -0.001165,  0.014526, -0.013464, ..., -0.011004, -0.006088, -0.012869, -0.01552 ], dtype=float32),\n",
       " 'collection.': array([ 0.033002,  0.000458,  0.013588, -0.015786, ..., -0.013186, -0.008927, -0.013799, -0.015934], dtype=float32),\n",
       " 'Tree': array([ 0.199598, -0.001828,  0.087018, -0.08791 , ..., -0.071511, -0.039493, -0.083606, -0.096091], dtype=float32),\n",
       " 'Keyfile': array([ 0.175083,  0.002386,  0.075977, -0.075201, ..., -0.066393, -0.037018, -0.072767, -0.080299], dtype=float32),\n",
       " 'var': array([ 0.040231, -0.005509,  0.022228, -0.020827, ..., -0.014569, -0.007082, -0.01719 , -0.021127], dtype=float32),\n",
       " 'Operations': array([ 0.104618,  0.001532,  0.045082, -0.044556, ..., -0.040191, -0.022312, -0.042373, -0.049054], dtype=float32),\n",
       " 'single': array([ 0.036971,  0.000126,  0.014851, -0.016099, ..., -0.014681, -0.006039, -0.017309, -0.015959], dtype=float32),\n",
       " 'MongoDB,': array([ 0.230171,  0.00522 ,  0.099123, -0.097671, ..., -0.083673, -0.047265, -0.093022, -0.103463], dtype=float32),\n",
       " 'Concern': array([ 0.214157,  0.004735,  0.092669, -0.091363, ..., -0.078297, -0.042722, -0.086424, -0.098588], dtype=float32),\n",
       " 'Pipeline': array([ 0.094365,  0.001485,  0.039657, -0.041601, ..., -0.03588 , -0.018056, -0.03944 , -0.041947], dtype=float32),\n",
       " 'Write': array([ 0.12894 ,  0.001363,  0.05431 , -0.053001, ..., -0.050624, -0.027045, -0.055466, -0.059416], dtype=float32),\n",
       " 'Authentication': array([ 0.200081,  0.002005,  0.086109, -0.087109, ..., -0.074805, -0.039609, -0.084425, -0.093055], dtype=float32),\n",
       " 'ChangelogCompatibility': array([ 0.170962,  0.00402 ,  0.073366, -0.072521, ..., -0.0628  , -0.035348, -0.06991 , -0.078201], dtype=float32),\n",
       " '3.4': array([ 0.192487,  0.003997,  0.084226, -0.08216 , ..., -0.074655, -0.041379, -0.07769 , -0.090522], dtype=float32),\n",
       " '4.0': array([ 0.175526,  0.006317,  0.07439 , -0.073502, ..., -0.060112, -0.035582, -0.071479, -0.082187], dtype=float32),\n",
       " 'by': array([ 0.172199,  0.006287,  0.073747, -0.068814, ..., -0.062434, -0.032783, -0.069373, -0.078428], dtype=float32),\n",
       " 'Up': array([ 0.223808,  0.006978,  0.093281, -0.095017, ..., -0.082663, -0.044186, -0.088084, -0.097082], dtype=float32),\n",
       " 'For': array([ 0.057219, -0.003761,  0.026332, -0.024683, ..., -0.022112, -0.01393 , -0.026219, -0.024366], dtype=float32),\n",
       " 'Structures': array([ 0.232919,  0.001467,  0.098147, -0.101792, ..., -0.086643, -0.048744, -0.093156, -0.104952], dtype=float32),\n",
       " 'CRUD': array([ 0.096853,  0.001809,  0.043075, -0.041971, ..., -0.035962, -0.019704, -0.038585, -0.045093], dtype=float32),\n",
       " 'that': array([ 0.046535,  0.002541,  0.02069 , -0.021193, ..., -0.015336, -0.011071, -0.018667, -0.021096], dtype=float32),\n",
       " 'Operations': array([ 0.091549,  0.001405,  0.039118, -0.039085, ..., -0.034109, -0.019148, -0.037045, -0.042773], dtype=float32),\n",
       " 'Read': array([ 0.110927,  0.003742,  0.047467, -0.050187, ..., -0.041266, -0.025191, -0.047218, -0.051794], dtype=float32),\n",
       " 'Shard': array([ 0.313533,  0.004789,  0.134377, -0.131065, ..., -0.113942, -0.060743, -0.129948, -0.14379 ], dtype=float32),\n",
       " 'provides': array([ 0.045306,  0.00204 ,  0.019529, -0.0212  , ..., -0.014799, -0.010455, -0.018863, -0.021114], dtype=float32)}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mongo_vecd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -ltr | grep pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fast.ai downloads pre-trained vectors: we do not have that !\n",
    "## en_vecd = pickle.load(open(PATH/'wiki.en.pkl','rb'))\n",
    "## fr_vecd = pickle.load(open(PATH/'wiki.fr.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_words = sqlite_vecs.get_words(include_freq=True)\n",
    "ft_word_dict = {k:v for k,v in zip(*ft_words)}\n",
    "ft_words = sorted(ft_word_dict.keys(), key=lambda x: ft_word_dict[x])\n",
    "\n",
    "len(ft_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_words = mongo_vecs.get_words(include_freq=True)\n",
    "ft_word_dict = {k:v for k,v in zip(*ft_words)}\n",
    "ft_words = sorted(ft_word_dict.keys(), key=lambda x: ft_word_dict[x])\n",
    "\n",
    "len(ft_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "##dim_sqlite_vec = len(sqlite_vecd[','])\n",
    "##dim_sqlite_vec\n",
    "dim_sqlite_vec = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[TO DO]: gives a key error - why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##dim_mongo_vec = len(mongo_vecd[','])\n",
    "##dim_mongo_vec\n",
    "dim_mongo_vec=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-2.626178e-05, 0.002534231)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlite_vecs = np.stack(list(sqlite_vecd.values()))\n",
    "## mean and standard deviation\n",
    "sqlite_vecs.mean(),sqlite_vecs.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 8)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## purely for processing speed up, we make sure the length of the longest sequence gets truncated\n",
    "sqlitelen_90 = int(np.percentile([len(o) for o in sqlite_sql_ids], 99))\n",
    "mongolen_90 = int(np.percentile([len(o) for o in mongosql_sql_ids], 97))\n",
    "sqlitelen_90,mongolen_90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlite_sql_ids_tr = np.array([o[:sqlitelen_90] for o in sqlite_sql_ids])\n",
    "mongosql_sql_ids_tr = np.array([o[:mongolen_90] for o in mongosql_sql_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "## training set\n",
    "## length & index for pytorch\n",
    "## convention: v variables, t tensors, a arrays\n",
    "class Seq2SeqDataset(Dataset):\n",
    "    def __init__(self, x, y): self.x,self.y = x,y\n",
    "    ## anything that is not yet a numpy array gets turned into it    \n",
    "    def __getitem__(self, idx): return A(self.x[idx], self.y[idx]) \n",
    "    def __len__(self): return len(self.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "567"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mongosql_sql_ids_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "425"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sqlite_sql_ids_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(372, 53)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## easy way to get training & validation set\n",
    "np.random.seed(42)\n",
    "trn_keep_sqlite = np.random.rand(len(sqlite_sql_ids_tr))>0.1 ## randomlist of bools to index into the set\n",
    "sqlite_trn = sqlite_sql_ids_tr[trn_keep_sqlite]\n",
    "sqlite_val = sqlite_sql_ids_tr[~trn_keep_sqlite]\n",
    "len(sqlite_trn),len(sqlite_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(497, 70)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## easy way to get training & validation set\n",
    "np.random.seed(42)\n",
    "trn_keep_mongo = np.random.rand(len(mongosql_sql_ids_tr))>0.1 ## randomlist of bools to index into the set\n",
    "mongo_trn = mongosql_sql_ids_tr[trn_keep_mongo]\n",
    "mongo_val = mongosql_sql_ids_tr[~trn_keep_mongo]\n",
    "len(mongo_trn),len(mongo_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO (CHECK): In the original notebook, the dimensions are equal\n",
    "\n",
    "https://github.com/fastai/fastai/blob/master/courses/dl2/translate.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for english to french, just switch around. This is the training & validation set here\n",
    "trn_ds = Seq2SeqDataset(mongo_trn,sqlite_trn)\n",
    "val_ds = Seq2SeqDataset(mongo_val,sqlite_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=3 ## as long as we work on a pseudo data set ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  validation set: sort by length, training set: randomize the order of things, so similar things about similar spot\n",
    "trn_samp = SortishSampler(sqlite_trn, key=lambda x: len(sqlite_trn[x]), bs=bs)\n",
    "val_samp = SortSampler(sqlite_val, key=lambda x: len(sqlite_val[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "##?DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "##doc(DataLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "##doc(TextLMDataBunch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "## minute 45\n",
    "## why do we need to transpose the oriantation?\n",
    "## we did pre-work, no augmentation (?)\n",
    "## padding index\n",
    "## for classifier padding at the start, here pre-padding = false for encider\n",
    "trn_dl = DataLoader(trn_ds, bs, num_workers=1, sampler=trn_samp) ## uses fast.ai behind the scenes\n",
    "val_dl = DataLoader(val_ds, int(bs*1.6), num_workers=1, sampler=val_samp)## uses fast.ai behind the scenes\n",
    "\n",
    "## says: I ahve a training & validation set (optional test set): into one object with a path to store temporary stuff\n",
    "## md = ModelData(PATH, trn_dl, val_dl)\n",
    "md = TextLMDataBunch(train_dl=trn_dl, valid_dl=val_dl , path='.')\n",
    "## after that you can create a learner and call fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trn_dl)\n",
    "##trn_dl\n",
    "\n",
    "##it = iter(trn_dl)\n",
    "##its = [next(it) for i in range(5)]\n",
    "##[(len(x),len(y)) for x,y in its]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO (CHECK): the iter doesnt work - why ?\n",
    "https://github.com/fastai/fastai/blob/master/courses/dl2/translate.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARCHITECTURE & LOSS FUNCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic idea of the Neural Network Translation is as follows:\n",
    "\n",
    "- We start with a sequence of tokens with arbitrary length, this is the statement to be translated\n",
    "- With the help of a Recurrent Neural Network, we inject this statement into a encoder (backbone) to turn the statement into a representation [of embedings of the word vector ?]\n",
    "- This RNN outputs the final hidden state, a vector per sentence\n",
    "- Noew we place this output into a Decoder RNN: this decoder goes through one word by word [and finds a corresponding (?what?) word]\n",
    "- The algorithm continues, until 'it thinks' then sentence is finished and gives the result back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "## number of rows = vocabulary size, each word has a vector\n",
    "## how big? fast text says size 300 !\n",
    "def create_emb(vecs, itos, em_sz):\n",
    "    ## random embeddings: if we find it in fast test we replace with that finding\n",
    "    emb = nn.Embedding(len(itos), em_sz, padding_idx=1)\n",
    "    wgts = emb.weight.data ## pytorch weight attribute is a variable. Vars have data attributes, that is a tensor\n",
    "    miss = []\n",
    "    ## with weight tensor we can now go through our vocabulary\n",
    "    for i,w in enumerate(itos):\n",
    "        ## Hacky: the 3 is about aligning standard deviations\n",
    "        try: wgts[i] = torch.from_numpy(vecs[w]*3) ## replace random weights with pre-trained vectors. \n",
    "        except: miss.append(w) ## what is not isn fast text we keep track of it\n",
    "    print(len(miss),miss[5:10])\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO: reset function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check enc vs dec here for encoder and decoder. emb embedding\n",
    "## out is output\n",
    "## GRU is similar to LSTM\n",
    "\n",
    "# Remember, we pass in an index. \n",
    "class Seq2SeqRNN(nn.Module):\n",
    "    def __init__(self, vecs_enc, itos_enc, em_sz_enc, vecs_dec, itos_dec, em_sz_dec, nh, out_sl, nl=2):\n",
    "        super().__init__()\n",
    "        self.nl,self.nh,self.out_sl = nl,nh,out_sl\n",
    "        ## create encode embedding\n",
    "        self.emb_enc = create_emb(vecs_enc, itos_enc, em_sz_enc)\n",
    "        ## add dropout\n",
    "        self.emb_enc_drop = nn.Dropout(0.15)\n",
    "        ## create the RNN: em_sz_enc = size of embedding, nh = our choice (56 for now), \n",
    "        ## num_layers: how many layers do we want, some dropout inside the RNN\n",
    "        self.gru_enc = nn.GRU(em_sz_enc, nh, num_layers=nl, dropout=0.25) ## standard pytorch, you could use LSTM too\n",
    "        ## some output to fit the decoder, so lets use a linear layer\n",
    "        self.out_enc = nn.Linear(nh, em_sz_dec, bias=False) ## nh: number of hidden into the decoder embedding size\n",
    "        \n",
    "        self.emb_dec = create_emb(vecs_dec, itos_dec, em_sz_dec)\n",
    "        self.gru_dec = nn.GRU(em_sz_dec, em_sz_dec, num_layers=nl, dropout=0.1) ## or take LSTM\n",
    "        self.out_drop = nn.Dropout(0.35)\n",
    "        self.out = nn.Linear(em_sz_dec, len(itos_dec))\n",
    "        self.out.weight.data = self.emb_dec.weight.data\n",
    "    ## forward pass\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        #####################\n",
    "        ## the most simple RNN: takes our inout and spits out a hidden vector that hopefull \n",
    "        ## will learn to contain all of the\n",
    "        ## about what the sentence says and how it says it\n",
    "        ## Only then we can hope it gives us a translation\n",
    "        sl,bs = inp.size()\n",
    "        ## initialise our hidden state to some zeros, vector of 0\n",
    "        h = self.initHidden(bs)\n",
    "        ## inout through embedding, dropout\n",
    "        emb = self.emb_enc_drop(self.emb_enc(inp))\n",
    "        ## pass 0 hidden state into our RMM\n",
    "        ## gives back final hidden state\n",
    "        ## AN RNN spits out 2 things: enc_out list of state every time step / state last timestep\n",
    "        enc_out, h = self.gru_enc(emb, h)\n",
    "        ## pass through linear layer\n",
    "        ## here we use state of the last timestep to create the input ( a vector) state for our decoder\n",
    "        h = self.out_enc(h)\n",
    "        ####################\n",
    "        \n",
    "        ##### ADDITIONAL to simple RNN\n",
    "        \n",
    "        ## dec_inp: represents the previous word that we translated\n",
    "        ## so tell me word 4, then you need the word 4 in a sentence. \n",
    "        ## therefore we feed that in into each time step\n",
    "        ## see function toks2ids, above\n",
    "            ## itos.insert(0, '_bos_') : this is the 1st token, beginning of stream\n",
    "            ## itos.insert(1, '_pad_')\n",
    "            ## itos.insert(2, '_eos_')\n",
    "            ## itos.insert(3, '_unk')\n",
    "        \n",
    "        dec_inp = V(torch.zeros(bs).long()) ## _bos_ for 1st run\n",
    "        res = []\n",
    "        ## for loop does the same as 4 steps inside pytorch, see above\n",
    "        for i in range(self.out_sl): ## output sequence length (see constructor) \n",
    "            ##= length of largest english sentence, because we translate into english (for this corpus)\n",
    "            \n",
    "            ## Normally: RNN gru_dec works in a whole sequence at a time, but we have a for loop\n",
    "            ## add leading unit access to the start unsqueeze. \n",
    "            ##So, we do not use the RNN really and could rewrite the thing with a linear layer\n",
    "            ## take input dec_inp and fit in the embedding emb_dec unsqueeze says: \n",
    "            ##treat this as a eequence of length 1\n",
    "            \n",
    "            ## 1. put through embedding\n",
    "            ## 1st run: what is the vector for beginning of stream token is\n",
    "            emb = self.emb_dec(dec_inp).unsqueeze(0) \n",
    "            \n",
    "            ## 2. put through RNN\n",
    "            ## 1st run: h is whatever came out of encoder. This figures out what the 1st word ir\n",
    "            outp, h = self.gru_dec(emb, h) \n",
    "            \n",
    "            ## 3. put through dropout 4. put through linear layer\n",
    "            ## 1st run 3: dropout\n",
    "            ## 1st run 4: linear layer in order to convert that into correct size for our decoder embedding matrix\n",
    "            outp = self.out(self.out_drop(outp[0]))\n",
    "            ## append that output to a list\n",
    "            \n",
    "            ## 5. Addpend to list of translated words\n",
    "            ## outp: is a tensor whose length is equual to the no. words in english vocabulary an\\\n",
    "            ## contains the probability that that word is the word\n",
    "            ## pdb.settrace\n",
    "            res.append(outp) \n",
    "            ## stack up list to a tensor and return it\n",
    "            \n",
    "            ## 6. takes the highes probability: check tensor for highes probability and give that index\n",
    "            \n",
    "            dec_inp = V(outp.data.max(1)[1]) ##1 is word index of largest things\n",
    "            ## dec_inp 1  is padding, so we are finished. Or largest sentence length\n",
    "            if (dec_inp==1).all(): break\n",
    "        ## we stack up these vector probabilities into a tensor, so we can feed this to a loss function\n",
    "        return torch.stack(res)\n",
    "    \n",
    "    def initHidden(self, bs): return V(torch.zeros(self.nl, bs, self.nh))\n",
    "    ##def reset(self): ##return ##V(torch.zeros(self.nl, bs, self.nh))\n",
    "    def reset(self): torch.zeros(self.nl, self.nh)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "##?Seq2SeqRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "##doc(Seq2SeqRNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function: categorical cross entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO: Explain in plain english why this should be the loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss Function is categorical cross entropy loss:\n",
    "- list of probabilities for each of our classes (class all words in our english vocab)\n",
    "- target: correct class, correct word at this location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "## tweak no.2\n",
    "def seq2seq_loss(input, target):\n",
    "    sl,bs = target.size()\n",
    "    sl_in,bs_in,nc = input.size()\n",
    "    ## tweak no.1 : we might have stopped early , so sequence length could be smaller than target.\n",
    "    ##    so we add padding\n",
    "    ##  pytroch padding: \n",
    "    ##      rank 3 tensor (sequence length x batch size x no. words of vocab)\n",
    "    ##      6 tuple required: each pair padding before padding after 1:08:30\n",
    "    ## 1st dim. & 2nd dim no padding 3. dim no padding left as much as required on the right\n",
    "    if sl>sl_in: input = F.pad(input, (0,0,0,0,0,sl-sl_in))\n",
    "    input = input[:sl]\n",
    "    ## tweak 2cross entropy loss expects rank 2 tensor, we have 3 , so flatten out: -1 in view\n",
    "    return F.cross_entropy(input.view(-1,nc), target.view(-1))#, ignore_index=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[TO DO] Understand, where opt_fn will go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_fn = partial(optim.Adam, betas=(0.8, 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "nh,nl = 256,2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "##?RNNLearner\n",
    "##doc(RNNLearner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[TO DO] for dim_mongo_vec and dim_sqlite_vec we took pseudo values! see to do above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35 ['i', ':', 'a', 'e', ',']\n",
      "26 ['u', 'p', 'e', 's', 'r']\n"
     ]
    }
   ],
   "source": [
    "## this gives some stuff that misses in that word vector (not relevant for us, just: these would be variables)\n",
    "## standard pytorch\n",
    "rnn = Seq2SeqRNN(mongo_vecd, mongosql_sql_itos, dim_mongo_vec, sqlite_vecd, sqlite_sql_itos, dim_sqlite_vec, nh, sqlitelen_90)\n",
    "## put to the GPU\n",
    "## SingleModel turns pytorch model into fast.ai Model min 1:09:40:\n",
    "## how to handle learnng rate groups (fast.ai concept)\n",
    "## we call RNN_Learner (not just Learner): RNN_Learner has cross entropy as default criteria\n",
    "## check save & load_encoder --> not really required\n",
    "\n",
    "## https://github.com/fastai/fastai/blob/master/fastai/text/learner.py\n",
    "learn = RNNLearner(data=md, model=rnn)\n",
    "## now we give our learner that loss function\n",
    "learn.crit = seq2seq_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "Traceback (most recent call last):\n  File \"/opt/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 138, in _worker_loop\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"/opt/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 138, in <listcomp>\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"<ipython-input-113-b4cd46b10a9f>\", line 7, in __getitem__\n    def __getitem__(self, idx): return A(self.x[idx], self.y[idx])\nNameError: name 'A' is not defined\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-146-f4f4a613c37b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m##torch.cuda.empty_cache()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m##learn.fit_one_cycle(cyc_len=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_find\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/fastai/train.py\u001b[0m in \u001b[0;36mlr_find\u001b[0;34m(learn, start_lr, end_lr, num_it, stop_div, **kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLRFinder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_it\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_div\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_it\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mto_fp16\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mLearner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_scale\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_master\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0mLearner\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, epochs, lr, wd, callbacks)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_fns\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         fit(epochs, self.model, self.loss_func, opt=self.opt, data=self.data, metrics=self.metrics,\n\u001b[0;32m--> 163\u001b[0;31m             callbacks=self.callbacks+callbacks)\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(epochs, model, loss_func, opt, data, callbacks, metrics)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mexception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(epochs, model, loss_func, opt, data, callbacks, metrics)\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpbar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m                 \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/fastprogress/fastprogress.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_update\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/fastai/basic_data.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;34m\"Process and returns items from `DataLoader`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mskip_size1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Batch size cannot be one if skip_size1 is set to True\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_listy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mskip_size1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    635\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreorder_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 637\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_next_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0mnext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__next__\u001b[0m  \u001b[0;31m# Python 2 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_next_batch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    656\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_put_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 658\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    659\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: Traceback (most recent call last):\n  File \"/opt/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 138, in _worker_loop\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"/opt/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 138, in <listcomp>\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"<ipython-input-113-b4cd46b10a9f>\", line 7, in __getitem__\n    def __getitem__(self, idx): return A(self.x[idx], self.y[idx])\nNameError: name 'A' is not defined\n"
     ]
    }
   ],
   "source": [
    "##torch.cuda.empty_cache()\n",
    "##learn.fit_one_cycle(cyc_len=1)\n",
    "learn.lr_find()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=3e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "        \t/* Turns off some styling */\n",
       "        \tprogress {\n",
       "\n",
       "            \t/* gets rid of default border in Firefox and Opera. */\n",
       "            \tborder: none;\n",
       "\n",
       "            \t/* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "            \tbackground-size: auto;\n",
       "            }\n",
       "\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='12', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/12 00:00<00:00]\n",
       "    </div>\n",
       "    \n",
       "<table style='width:300px; margin-bottom:10px'>\n",
       "  <tr>\n",
       "    <th>epoch</th>\n",
       "    <th>train_loss</th>\n",
       "    <th>valid_loss</th>\n",
       "    <th>accuracy</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "\n",
       "  </tr>\n",
       "</table>\n",
       "\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "        \t/* Turns off some styling */\n",
       "        \tprogress {\n",
       "\n",
       "            \t/* gets rid of default border in Firefox and Opera. */\n",
       "            \tborder: none;\n",
       "\n",
       "            \t/* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "            \tbackground-size: auto;\n",
       "            }\n",
       "\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='progress-bar-interrupted' max='124', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      Interrupted\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "Traceback (most recent call last):\n  File \"/opt/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 138, in _worker_loop\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"/opt/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 138, in <listcomp>\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"<ipython-input-113-b4cd46b10a9f>\", line 7, in __getitem__\n    def __getitem__(self, idx): return A(self.x[idx], self.y[idx])\nNameError: name 'A' is not defined\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-151-c227d66a1ad7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_one_cycle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcyc_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m##learn.fit(lr, 1, cyc_len=12, use_clr=(20,10))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/fastai/train.py\u001b[0m in \u001b[0;36mfit_one_cycle\u001b[0;34m(learn, cyc_len, max_lr, moms, div_factor, pct_start, wd, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m     19\u001b[0m     callbacks.append(OneCycleScheduler(learn, max_lr, moms=moms, div_factor=div_factor,\n\u001b[1;32m     20\u001b[0m                                         pct_start=pct_start, **kwargs))\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcyc_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlr_find\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mLearner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_lr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_lr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_it\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_div\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, epochs, lr, wd, callbacks)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_fns\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         fit(epochs, self.model, self.loss_func, opt=self.opt, data=self.data, metrics=self.metrics,\n\u001b[0;32m--> 163\u001b[0;31m             callbacks=self.callbacks+callbacks)\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(epochs, model, loss_func, opt, data, callbacks, metrics)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mexception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(epochs, model, loss_func, opt, data, callbacks, metrics)\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpbar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m                 \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/fastprogress/fastprogress.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_update\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/fastai/basic_data.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;34m\"Process and returns items from `DataLoader`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mskip_size1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Batch size cannot be one if skip_size1 is set to True\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_listy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mskip_size1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    635\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreorder_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 637\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_next_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0mnext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__next__\u001b[0m  \u001b[0;31m# Python 2 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_next_batch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    656\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_put_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 658\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    659\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: Traceback (most recent call last):\n  File \"/opt/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 138, in _worker_loop\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"/opt/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 138, in <listcomp>\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"<ipython-input-113-b4cd46b10a9f>\", line 7, in __getitem__\n    def __getitem__(self, idx): return A(self.x[idx], self.y[idx])\nNameError: name 'A' is not defined\n"
     ]
    }
   ],
   "source": [
    "learn.fit_one_cycle(cyc_len=12)\n",
    "##learn.fit(lr, 1, cyc_len=12, use_clr=(20,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('initial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('initial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Initial Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CONTINUE 1:11:10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(iter(val_dl)) ## we grab x from the validation set to pass in below / use alternatively learn. predict_array(?)\n",
    "probs = learn.model(V(x)) ## standard pytorch model, so we can pass in some X\n",
    "preds = to_np(probs.max(2)[1]) ## grab index of highes probability word, so we turn the number into a word\n",
    "\n",
    "## this is just about going through some examples\n",
    "for i in range(180,190):\n",
    "    print(' '.join([fr_itos[o] for o in x[:,i] if o != 1])) ## french\n",
    "    print(' '.join([en_itos[o] for o in y[:,i] if o != 1])) ## correct english\n",
    "    print(' '.join([en_itos[o] for o in preds[:,i] if o!=1])) ## predicted english\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One easy way to bi-directional for classification: take all your tokens, flip order around (?), train new language model, train new classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Seq2SeqRNN_Bidir(nn.Module):\n",
    "    def __init__(self, vecs_enc, itos_enc, em_sz_enc, vecs_dec, itos_dec, em_sz_dec, nh, out_sl, nl=2):\n",
    "        super().__init__()\n",
    "        self.emb_enc = create_emb(vecs_enc, itos_enc, em_sz_enc)\n",
    "        self.nl,self.nh,self.out_sl = nl,nh,out_sl\n",
    "        ## bidirectional=True: next to one encoder left to right, we have a 2nd one right to left\n",
    "        ## hidden state is at the beginning of the sentence for this 2nd RNN\n",
    "        ## tensor with an extra 2 long axis (hidden state in both the same)\n",
    "        ## example: 2 layers with bi-directional --> tensor on length 4\n",
    "        self.gru_enc = nn.GRU(em_sz_enc, nh, num_layers=nl, dropout=0.25, bidirectional=True) ## encoder\n",
    "        ## *2 here because we have a 2nd hidden state with 2nd RNN\n",
    "        self.out_enc = nn.Linear(nh*2, em_sz_dec, bias=False) \n",
    "        self.drop_enc = nn.Dropout(0.05)\n",
    "        self.emb_dec = create_emb(vecs_dec, itos_dec, em_sz_dec)\n",
    "        self.gru_dec = nn.GRU(em_sz_dec, em_sz_dec, num_layers=nl, dropout=0.1)\n",
    "        self.emb_enc_drop = nn.Dropout(0.15)\n",
    "        self.out_drop = nn.Dropout(0.35)\n",
    "        self.out = nn.Linear(em_sz_dec, len(itos_dec))\n",
    "        self.out.weight.data = self.emb_dec.weight.data\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        sl,bs = inp.size()\n",
    "        h = self.initHidden(bs)\n",
    "        emb = self.emb_enc_drop(self.emb_enc(inp))\n",
    "        enc_out, h = self.gru_enc(emb, h)\n",
    "        h = h.view(2,2,bs,-1).permute(0,2,1,3).contiguous().view(2,bs,-1)\n",
    "        h = self.out_enc(self.drop_enc(h))\n",
    "\n",
    "        dec_inp = V(torch.zeros(bs).long())\n",
    "        res = []\n",
    "## Often b-directional for the decoder is not done\n",
    "  ## ??? Why ?1. Considered cheating see 1:20:30 How to turn two separate loops into a final result?\n",
    "  ## ??? range? start training is random, then we would go forever \n",
    "        for i in range(self.out_sl):\n",
    "            emb = self.emb_dec(dec_inp).unsqueeze(0)\n",
    "            outp, h = self.gru_dec(emb, h)\n",
    "            outp = self.out(self.out_drop(outp[0]))\n",
    "            res.append(outp)\n",
    "            dec_inp = V(outp.data.max(1)[1])\n",
    "            ## when we start training everything is random, so this probably is never true\n",
    "            ## a model knows nothing when we start training it\n",
    "            if (dec_inp==1).all(): break\n",
    "        return torch.stack(res)\n",
    "     ## *2 here because we have a 2nd hidden state with 2nd RNN\n",
    "    def initHidden(self, bs): return V(torch.zeros(self.nl*2, bs, self.nh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = Seq2SeqRNN_Bidir(fr_vecd, fr_itos, dim_fr_vec, en_vecd, en_itos, dim_en_vec, nh, enlen_90)\n",
    "learn = RNN_Learner(md, SingleModel(to_gpu(rnn)), opt_fn=opt_fn)\n",
    "learn.crit = seq2seq_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bi Directional improves the result !\n",
    "learn.fit(lr, 1, cycle_len=12, use_clr=(20,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note for bi - directional models one has to consider closely, which layer is going right to left if one has many layers like e.g. google translate. Otherwise one might get performance issues.\n",
    "Generally and for shallow models bi-directional is a good win and does not cause too many performance bottlenecks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('bidir')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teacher Forcing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the model starts learning, it knows 'nothing about nothing'. So, e.g. the 1st translated word in a sentence is more or less random in the beginning. One might say, in the beginning the neural net feeds in an input that is 'stupid' into a model that 'knows nothing' and somehow it is supposed to get better.\n",
    "\n",
    "What if we could 'somehow' inject the 1st 'right' word? self.pr_force.\n",
    "\n",
    "At the start of training we set pr_force very high so that nearly always it gets the actual correct previous (?) word, so it has a useful input.\n",
    "As we train longer, we decrease pr_force so that at the end pr_force is 0 and the neyral net has to learn properly. Now that is fine, because it feeds in sensible input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow or keras (static graph) vs pytorch:\n",
    "- see below line dec_inp = y[i]\n",
    "- key reason to switch fast.ai to pytorch was\n",
    "\n",
    "new tensorflow: https://www.youtube.com/watch?feature=youtu.be&utm_campaign=NLP+News&utm_medium=email&utm_source=Revue+newsletter&v=WTNH0tcscqo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is hook called the stepper, see minute 1:27:00\n",
    "## Idea of the hook is you can reuse and do not have to write from the scratch the training loop\n",
    "## check model.py:\n",
    "    ## fit function: lowest level without learner, just standard pytorch model, model data object\n",
    "    ##               epochs, standard pytorch optimiser, standard pytorch loss function\n",
    "    ## this low level function gets called by upper layers and hence used by us\n",
    "    ## it calls a function called stepper.step\n",
    "    ## stepper.step uses a class called Stepper:\n",
    "        ## - calls the model / zero gradients / loss function / calls backward / clipping if necessary /call optimiser\n",
    "        ## - \n",
    "\n",
    "## Inherit from Stepper and write own version of step (so 1t copy & paste / call. super.step and then add or change )\n",
    "\n",
    "class Seq2SeqStepper(Stepper): ## we inherit from Stepper here\n",
    "    def step(self, xs, y, epoch):\n",
    "## this is the new line\n",
    "        ## replace or_force in the model with sth that gradually decreases linearly with every epoch, then drops to 0\n",
    "        self.m.pr_force = (10-epoch)*0.1 if epoch<10 else 0\n",
    "## instead of below: super().step(xs,y,epoch) \n",
    "        xtra = []\n",
    "        output = self.m(*xs, y)\n",
    "        if isinstance(output,tuple): output,*xtra = output\n",
    "        self.opt.zero_grad()\n",
    "        loss = raw_loss = self.crit(output, y)\n",
    "        if self.reg_fn: loss = self.reg_fn(output, xtra, raw_loss)\n",
    "        loss.backward()\n",
    "        if self.clip:   # Gradient clipping\n",
    "            nn.utils.clip_grad_norm(trainable_params_(self.m), self.clip)\n",
    "        self.opt.step()\n",
    "        return raw_loss.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqRNN_TeacherForcing(nn.Module):\n",
    "    def __init__(self, vecs_enc, itos_enc, em_sz_enc, vecs_dec, itos_dec, em_sz_dec, nh, out_sl, nl=2):\n",
    "        super().__init__()\n",
    "        self.emb_enc = create_emb(vecs_enc, itos_enc, em_sz_enc)\n",
    "        self.nl,self.nh,self.out_sl = nl,nh,out_sl\n",
    "        self.gru_enc = nn.GRU(em_sz_enc, nh, num_layers=nl, dropout=0.25)\n",
    "        self.out_enc = nn.Linear(nh, em_sz_dec, bias=False)\n",
    "        self.emb_dec = create_emb(vecs_dec, itos_dec, em_sz_dec)\n",
    "        self.gru_dec = nn.GRU(em_sz_dec, em_sz_dec, num_layers=nl, dropout=0.1)\n",
    "        self.emb_enc_drop = nn.Dropout(0.15)\n",
    "        self.out_drop = nn.Dropout(0.35)\n",
    "        self.out = nn.Linear(em_sz_dec, len(itos_dec))\n",
    "        self.out.weight.data = self.emb_dec.weight.data\n",
    "        self.pr_force = 1.\n",
    "        \n",
    "    def forward(self, inp, y=None):\n",
    "        sl,bs = inp.size()\n",
    "        h = self.initHidden(bs)\n",
    "        emb = self.emb_enc_drop(self.emb_enc(inp))\n",
    "        enc_out, h = self.gru_enc(emb, h)\n",
    "        h = self.out_enc(h)\n",
    "\n",
    "        dec_inp = V(torch.zeros(bs).long())\n",
    "        res = []\n",
    "        for i in range(self.out_sl):\n",
    "            emb = self.emb_dec(dec_inp).unsqueeze(0)\n",
    "            outp, h = self.gru_dec(emb, h)\n",
    "            outp = self.out(self.out_drop(outp[0]))\n",
    "            res.append(outp)\n",
    "            dec_inp = V(outp.data.max(1)[1])\n",
    "            if (dec_inp==1).all(): break\n",
    "            ##  probability forcing --> see stepepr overwrite above\n",
    "            if (y is not None) and (random.random()<self.pr_force):\n",
    "                ## if some random probability is less than self.pr_force\n",
    "                ## then I replace my decode input with the actual correct thing\n",
    "                if i>=len(y): break ## if we already went too far (longer than target sentence) just stop\n",
    "                ##  pytorch allows\n",
    "                ## static graph like tensorflow does not allow that\n",
    "                dec_inp = y[i]\n",
    "        return torch.stack(res)\n",
    "    \n",
    "    def initHidden(self, bs): return V(torch.zeros(self.nl, bs, self.nh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = Seq2SeqRNN_TeacherForcing(fr_vecd, fr_itos, dim_fr_vec, en_vecd, en_itos, dim_en_vec, nh, enlen_90)\n",
    "learn = RNN_Learner(md, SingleModel(to_gpu(rnn)), opt_fn=opt_fn)\n",
    "learn.crit = seq2seq_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## gives better result than above\n",
    "learn.fit(lr, 1, cycle_len=12, use_clr=(20,10), stepper=Seq2SeqStepper) ## we pass in above stepper overwrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('forcing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attentional Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expectation to summarise one source sentence entirely in one hidden state, such that everything necessary to create a translated sentence is present, is 'asking a lot'.\n",
    "\n",
    "This might be asking too much, so instead of having one hidden state at the end of the sentence, why not have a hidden state after every word? With a bi - directional RNN we would even have two vectors, that we can use.\n",
    "\n",
    "In the example 'He loved pizza', which would need to get translated to 'Er liebte Pizza', to translate 'liebte' we probably want more attention towards the hidden state of 'loved' and less attention to e.g. 'Pizza'.\n",
    "\n",
    "[to do] take 2 sql examples\n",
    "\n",
    "But how do we know, where to place more attention? We will do this via creating a new neural net, that spits out a weight for every index / word of the source sentence.\n",
    "\n",
    "https://distill.pub/2016/augmented-rnns/\n",
    "\n",
    "Note Attention was also used for getting text out of scripts.\n",
    "\n",
    "'Grammar as a foreign language': replace rules based grammar with neural net https://arxiv.org/abs/1412.7449"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_t(*sz): return torch.randn(sz)/math.sqrt(sz[0])\n",
    "## Parameter is like a variable but tells pytorch: I want you to learn the weights for this\n",
    "def rand_p(*sz): return nn.Parameter(rand_t(*sz)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we use teacher forcing, but not bi-directional\n",
    "\n",
    "class Seq2SeqAttnRNN(nn.Module):\n",
    "\n",
    "    ## Until below this, all is identical with the above    \n",
    "    def __init__(self, vecs_enc, itos_enc, em_sz_enc, vecs_dec, itos_dec, em_sz_dec, nh, out_sl, nl=2):\n",
    "        super().__init__()\n",
    "        self.emb_enc = create_emb(vecs_enc, itos_enc, em_sz_enc)\n",
    "        self.nl,self.nh,self.out_sl = nl,nh,out_sl\n",
    "        self.gru_enc = nn.GRU(em_sz_enc, nh, num_layers=nl, dropout=0.25)\n",
    "        self.out_enc = nn.Linear(nh, em_sz_dec, bias=False)\n",
    "        self.emb_dec = create_emb(vecs_dec, itos_dec, em_sz_dec)\n",
    "        self.gru_decgru_dec = nn.GRU(em_sz_dec, em_sz_dec, num_layers=nl, dropout=0.1)\n",
    "        self.emb_enc_drop = nn.Dropout(0.15)\n",
    "        self.out_drop = nn.Dropout(0.35)\n",
    "        self.out = nn.Linear(em_sz_dec, len(itos_dec))\n",
    "        self.out.weight.data = self.emb_dec.weight.data\n",
    "## Until above this, all is identical with the above\n",
    " \n",
    "## An RNN spits out 2 things: enc_out list of state every time step / state last timestep\n",
    "## efore attention, we use state of the last timestep to create the inout state for our decoder\n",
    "## Rather: we would want to use the ones that are most relevant to the ones we are using now\n",
    "## That is it would like to take a weighted average of each time step weighted by whatever would be appropriate now\n",
    "## we figure out what is appropriate by training a small neural net\n",
    "## Hence, include in model a minimal possible neural net, which is having 2 layers and 1 non linear activation layer\n",
    "\n",
    "## Note, there is no specific loss function for this inner  neural net. It gets judged \n",
    "## over the SAME MAIN loss function, which is assessing if translations were correct.\n",
    "## So how does this neural net learn? In order to make translation outputs better, \n",
    "## it needs to make weights for the attention better.\n",
    "## In the end, this works via the chain rule and backprop: if you put a little more or less weight here, \n",
    "## overall result will improve\n",
    "    \n",
    "        self.W1 = rand_p(nh, em_sz_dec) ## random matrix, see above definition torch.randn(sz)/math.sqrt(sz[0])\n",
    "        self.l2 = nn.Linear(em_sz_dec, em_sz_dec) ## 1st linear layer\n",
    "        self.l3 = nn.Linear(em_sz_dec+nh, em_sz_dec) ## 2nd linear layer\n",
    "        self.V = rand_p(em_sz_dec)\n",
    "\n",
    "## Until below this (the encoder), all is identical with the above\n",
    "## ret_attn: take note of attentions\n",
    "        def forward(self, inp, y=None, ret_attn=False):\n",
    "        sl,bs = inp.size()\n",
    "        h = self.initHidden(bs)\n",
    "        emb = self.emb_enc_drop(self.emb_enc(inp))\n",
    "        ## enc_out not just last one but whole tensor with all outputs\n",
    "        enc_out, h = self.gru_enc(emb, h)\n",
    "        h = self.out_enc(h)\n",
    "## Until above this (the encoder), all is identical with the above\n",
    "\n",
    "        dec_inp = V(torch.zeros(bs).long())\n",
    "        res,attns = [],[]\n",
    "        w1e = enc_out @ self.W1\n",
    "        for i in range(self.out_sl):\n",
    "## Difference: we create a little neural net with one hidden layer to get a weighted average\n",
    "## we tell this little neural net: do not just take the final state h but take all enc_out\n",
    "            ## we take the last layers hidden state h[-1] and stick that into linear layer l2\n",
    "            ## how do we decide, which words to focus on for translating? We have that information in the hidden state\n",
    "            w2h = self.l2(h[-1])\n",
    "            ## place the result into a non linear activation\n",
    "            ## could we use ReLu? Worth a try\n",
    "            u = F.tanh(w1e + w2h)\n",
    "            ## get the results of that neural net. Softmax ensure all weights we use add up to one\n",
    "            ## we 'hope' one weight stands out, that is so because of the e in it\n",
    "            ## u @ self.V: matrix multiply (no bias?)\n",
    "            a = F.softmax(u @ self.V, 0)\n",
    "            attns.append(a) ## list of the attentions\n",
    "## Difference: we take a weighted average  \n",
    "            ## we use a the activaction results of above neural net to weight the results \n",
    "            ## of encoder outputs enc_out\n",
    "            ## enc_out not just last one but whole tensor with all outputs. gets weighed then by neural net results\n",
    "            Xa = (a.unsqueeze(2) * enc_out).sum(0)\n",
    "            emb = self.emb_dec(dec_inp)\n",
    "            wgt_enc = self.l3(torch.cat([emb, Xa], 1))\n",
    "## Until below this (decoder), all is identical with the above       \n",
    "            outp, h = self.gru_dec(wgt_enc.unsqueeze(0), h)\n",
    "            outp = self.out(self.out_drop(outp[0]))\n",
    "            res.append(outp)\n",
    "            dec_inp = V(outp.data.max(1)[1])\n",
    "            if (dec_inp==1).all(): break\n",
    "            ## we use teacher forcing    \n",
    "            if (y is not None) and (random.random()<self.pr_force):\n",
    "## Until above this (decoder), all is identical with the above\n",
    "                if i>=len(y): break\n",
    "                dec_inp = y[i]\n",
    "\n",
    "        res = torch.stack(res)\n",
    "        ## ret_attn: take note of attentions\n",
    "        if ret_attn: res = res,torch.stack(attns)\n",
    "        return res\n",
    "\n",
    "    def initHidden(self, bs): return V(torch.zeros(self.nl, bs, self.nh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = Seq2SeqAttnRNN(fr_vecd, fr_itos, dim_fr_vec, en_vecd, en_itos, dim_en_vec, nh, enlen_90)\n",
    "learn = RNN_Learner(md, SingleModel(to_gpu(rnn)), opt_fn=opt_fn)\n",
    "learn.crit = seq2seq_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=2e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## another improvement in loss\n",
    "learn.fit(lr, 1, cycle_len=15, use_clr=(20,10), stepper=Seq2SeqStepper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('attn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('attn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Current Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(iter(val_dl))\n",
    "probs,attns = learn.model(V(x),ret_attn=True) ## grab attentions out of the model\n",
    "preds = to_np(probs.max(2)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(180,190):\n",
    "    print(' '.join([fr_itos[o] for o in x[:,i] if o != 1]))\n",
    "    print(' '.join([en_itos[o] for o in y[:,i] if o != 1]))\n",
    "    print(' '.join([en_itos[o] for o in preds[:,i] if o!=1]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = to_np(attns[...,180])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## picture of each timestep at the attention\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 10))\n",
    "for i,ax in enumerate(axes.flat):\n",
    "    ax.plot(attn[i])\n",
    "    \n",
    "## similar info like in these attention graphs: https://distill.pub/2016/augmented-rnns/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqRNN_All(nn.Module):\n",
    "    def __init__(self, vecs_enc, itos_enc, em_sz_enc, vecs_dec, itos_dec, em_sz_dec, nh, out_sl, nl=2):\n",
    "        super().__init__()\n",
    "        self.emb_enc = create_emb(vecs_enc, itos_enc, em_sz_enc)\n",
    "        self.nl,self.nh,self.out_sl = nl,nh,out_sl\n",
    "        self.gru_enc = nn.GRU(em_sz_enc, nh, num_layers=nl, dropout=0.25, bidirectional=True)\n",
    "        self.out_enc = nn.Linear(nh*2, em_sz_dec, bias=False)\n",
    "        self.drop_enc = nn.Dropout(0.25)\n",
    "        self.emb_dec = create_emb(vecs_dec, itos_dec, em_sz_dec)\n",
    "        self.gru_dec = nn.GRU(em_sz_dec, em_sz_dec, num_layers=nl, dropout=0.1)\n",
    "        self.emb_enc_drop = nn.Dropout(0.15)\n",
    "        self.out_drop = nn.Dropout(0.35)\n",
    "        self.out = nn.Linear(em_sz_dec, len(itos_dec))\n",
    "        self.out.weight.data = self.emb_dec.weight.data\n",
    "\n",
    "        self.W1 = rand_p(nh*2, em_sz_dec)\n",
    "        self.l2 = nn.Linear(em_sz_dec, em_sz_dec)\n",
    "        self.l3 = nn.Linear(em_sz_dec+nh*2, em_sz_dec)\n",
    "        self.V = rand_p(em_sz_dec)\n",
    "        \n",
    "def forward(self, inp, y=None):\n",
    "        sl,bs = inp.size()\n",
    "        h = self.initHidden(bs)\n",
    "        emb = self.emb_enc_drop(self.emb_enc(inp))\n",
    "        enc_out, h = self.gru_enc(emb, h)\n",
    "        h = h.view(2,2,bs,-1).permute(0,2,1,3).contiguous().view(2,bs,-1)\n",
    "        h = self.out_enc(self.drop_enc(h))\n",
    "\n",
    "        dec_inp = V(torch.zeros(bs).long())\n",
    "        res,attns = [],[]\n",
    "        w1e = enc_out @ self.W1\n",
    "        for i in range(self.out_sl):\n",
    "            w2h = self.l2(h[-1])\n",
    "            u = F.tanh(w1e + w2h)\n",
    "            a = F.softmax(u @ self.V, 0)\n",
    "            attns.append(a)\n",
    "            Xa = (a.unsqueeze(2) * enc_out).sum(0)\n",
    "            emb = self.emb_dec(dec_inp)\n",
    "            wgt_enc = self.l3(torch.cat([emb, Xa], 1))\n",
    "            \n",
    "            outp, h = self.gru_dec(wgt_enc.unsqueeze(0), h)\n",
    "            outp = self.out(self.out_drop(outp[0]))\n",
    "            res.append(outp)\n",
    "            dec_inp = V(outp.data.max(1)[1])\n",
    "            if (dec_inp==1).all(): break\n",
    "            if (y is not None) and (random.random()<self.pr_force):\n",
    "                if i>=len(y): break\n",
    "                dec_inp = y[i]\n",
    "        return torch.stack(res)\n",
    "\n",
    "    def initHidden(self, bs): return V(torch.zeros(self.nl*2, bs, self.nh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = Seq2SeqRNN_All(fr_vecd, fr_itos, dim_fr_vec, en_vecd, en_itos, dim_en_vec, nh, enlen_90)\n",
    "learn = RNN_Learner(md, SingleModel(to_gpu(rnn)), opt_fn=opt_fn)\n",
    "learn.crit = seq2seq_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(lr, 1, cycle_len=15, use_clr=(20,10), stepper=Seq2SeqStepper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(iter(val_dl))\n",
    "probs = learn.model(V(x))\n",
    "preds = to_np(probs.max(2)[1])\n",
    "\n",
    "for i in range(180,190):\n",
    "    print(' '.join([fr_itos[o] for o in x[:,i] if o != 1]))\n",
    "    print(' '.join([en_itos[o] for o in y[:,i] if o != 1]))\n",
    "    print(' '.join([en_itos[o] for o in preds[:,i] if o!=1]))\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
